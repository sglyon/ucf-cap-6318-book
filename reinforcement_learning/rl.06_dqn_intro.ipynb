{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61031815-c76b-450d-a565-29cca3a26b3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; Intro to Deep Q Networks\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow, keras\n",
    "- Reinforcement Learning -- Q learning with continuous state spaces\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Be able to use tensorflow to use a neural network to approximate $Q(s, a)$ for continuous spaces $\\mathcal{S}$\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 9-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234ccd5-e651-477c-acc0-fb695e97e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import keras\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4c042-217f-43e4-8856-df55a9e4b042",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Review: CartPole\n",
    "\n",
    "- We previously studied the cart pole problem:\n",
    "    - Pole fastened to a cart, but can freely rotate\n",
    "    - Pole starts vertical, but with some angular velocity\n",
    "    - Goal: move cart left and right to keep pole vertical\n",
    "    - $\\mathcal{S} = \\{\\text{ cart position}, \\text{cart velocity}, \\text{ pole angle}, \\text{ pole angular velocity} \\} \\subset \\mathbb{R}^4$\n",
    "    - $\\mathcal{A}(s) = \\{\\text{ left, right }\\}\\; \\forall s$\n",
    "- Need $Q$ to generalize between observations from continuous $\\mathcal{S}$\n",
    "- Used complete polynomial to represent $Q$ and obtained about 120/200 tiem steps (random 30/120)\n",
    "- Need more flexible method for approximating $Q$..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a2b5f-34da-49a9-b99d-6a6f7e3e86a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Objective: DQN\n",
    "\n",
    "- The objective for this lecture will be to use a MLP for approximating $Q$\n",
    "- A few key concepts:\n",
    "    - Will represent $Q(s): \\mathcal{S} -> \\mathbb{R}^{|\\mathcal{A}|}$\n",
    "    - To support mini-batch training (and other reasons we'll learn about in another lecture 😉) we will use *experience replay*\n",
    "- Experience replay:\n",
    "    - Store $(s, a, r, s')$ transitions in a memory bank of fixed size\n",
    "    - As new transitions are added, \"forget\" oldest transitions if memory full\n",
    "    - When training, sample randomly from current memory bank to form batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abdd70-321f-4276-a634-936e78923671",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## DQN\n",
    "\n",
    "- Below we implement a Deep Q Network -- or a Q learning agent that uses a deep neural network for representing Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9da94bee-8b2a-4faf-b87d-f79d27884c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            environment,\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "            loss=keras.losses.mean_squared_error,\n",
    "            hidden_layer_sizes: List[int] = [24, 24],\n",
    "            batch_size: int = 64,\n",
    "            memory_size: int = 5_000,\n",
    "            epsilon: float=0.9,\n",
    "            beta: float=0.5\n",
    "        ):\n",
    "        # check that environment is what we think it is\n",
    "        self.env = environment\n",
    "        self.Ns = self.env.observation_space.shape[0]\n",
    "\n",
    "        assert isinstance(self.env.action_space, gym.spaces.Discrete)\n",
    "        self.Na = self.env.action_space.n\n",
    "        self.A_s = np.arange(self.Na)\n",
    "\n",
    "        # set up Q function\n",
    "        self.Q = keras.Sequential(\n",
    "            [keras.layers.InputLayer((self.Ns,))] +\n",
    "            [keras.layers.Dense(n, activation=\"relu\") for n in hidden_layer_sizes] +\n",
    "            [keras.layers.Dense(\n",
    "                self.Na, activation=\"linear\",\n",
    "                kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "                    minval=-0.03, maxval=0.03\n",
    "                ),\n",
    "                bias_initializer=tf.keras.initializers.Constant(-0.2)\n",
    "            )]\n",
    "        )\n",
    "        self.Q.compile(loss=loss, optimizer=optimizer)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "\n",
    "        # set up memory\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # store hyper parameters\n",
    "        self.epsilon = epsilon\n",
    "        self.beta = beta\n",
    "\n",
    "    def get_greedy(self, s):\n",
    "        assert s.shape[0] == 1\n",
    "        Q_s = self.Q.predict(s, verbose=0)[0]\n",
    "        max_val = max(Q_s)\n",
    "        return random.choice(self.A_s[Q_s == max_val])\n",
    "\n",
    "    def remember(self, s, a, r, sp, done):\n",
    "        self.memory.append((s, a, r, sp, done))\n",
    "\n",
    "    def act(self, s):\n",
    "        if random.random() > self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return self.get_greedy(s)\n",
    "\n",
    "    def learn_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            # not enough memory yet...\n",
    "            return\n",
    "\n",
    "        # sample a batch\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        sarsd = list(zip(*batch))\n",
    "\n",
    "        # reconstruct s, a, r, s' arrays\n",
    "        s = keras.ops.concatenate(sarsd[0], axis=0)\n",
    "        a = np.row_stack(sarsd[1])[:, 0]\n",
    "        r = np.row_stack(sarsd[2])[:, 0]\n",
    "        sp = np.concatenate(sarsd[3])\n",
    "\n",
    "        # compute temporal difference target using greedy policy\n",
    "        td_target = r + self.beta * tf.reduce_max(self.Q.predict(sp, verbose=0), axis=1)\n",
    "\n",
    "        # apply one hot encoding for easy application of `a` below\n",
    "        a_hot = keras.ops.one_hot(a, self.Na)\n",
    "\n",
    "        # compute the loss between current Q(s, a) and the targets\n",
    "        with tf.GradientTape() as tape:\n",
    "            Q_s = self.Q(s)\n",
    "            Q_sa = tf.reduce_sum(Q_s * a_hot, axis=1)\n",
    "            l = self.loss(td_target, Q_sa)\n",
    "\n",
    "        # backprop -- compute and then allow optimizer to apply gradients\n",
    "        grads = tape.gradient(l, self.Q.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3de3c-6367-4085-b8ae-ea74824179eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Setup\n",
    "\n",
    "- Below we set random seeds, create env, optimizer, and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "065ce782-2f36-4cdd-a880-f9d97d8c8c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.01, )\n",
    "agent = DQN(env, optimizer, epsilon=0.9, beta=0.8, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0c144b4-15a0-4fd1-a4da-9be022a788dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m50\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">770</span> (3.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m770\u001b[0m (3.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.Q.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48255d04-c8ef-4fb0-96f6-61bc8596d31e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Training DQN\n",
    "\n",
    "- Below we have a routine for training our DQN\n",
    "- Notice that we need to make sure that the `s` array is [1, Ns] before handing to tensorflow\n",
    "    - Tensorflow expects first dimension to be for batch size and subsequent dimensions to be for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9672a52e-479f-4ac5-8203-b190896c940b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dqn, N_episodes=100):\n",
    "    for episode in range(N_episodes):\n",
    "        s_, _ = dqn.env.reset()\n",
    "        s = s_[None, :]\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            step += 1\n",
    "            a = dqn.act(s)\n",
    "            sp, r, done, _truncated, _ = env.step(a)\n",
    "            sp = sp[None, :]\n",
    "            r = r if (not done or step >= 200) else -r  # penalize learner when fails\n",
    "\n",
    "            dqn.remember(s, a, r, sp, done)\n",
    "            if done:\n",
    "                # if episode % 100 == 0:\n",
    "                print(f\"episode: {episode}, steps: {step}\")\n",
    "                break\n",
    "\n",
    "            # step forward in time\n",
    "            s = sp\n",
    "\n",
    "            # learn!\n",
    "            dqn.learn_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a392d65e-712f-498f-bb59-a9d15b5cddc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, steps: 9\n",
      "episode: 1, steps: 9\n",
      "episode: 2, steps: 10\n",
      "episode: 3, steps: 10\n",
      "episode: 4, steps: 9\n",
      "episode: 5, steps: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7d/x0yzvh3172zdzj7wsxtnhhh00000gn/T/ipykernel_81861/2317294709.py:70: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n",
      "  a = np.row_stack(sarsd[1])[:, 0]\n",
      "/var/folders/7d/x0yzvh3172zdzj7wsxtnhhh00000gn/T/ipykernel_81861/2317294709.py:71: DeprecationWarning: `row_stack` alias is deprecated. Use `np.vstack` directly.\n",
      "  r = np.row_stack(sarsd[2])[:, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 6, steps: 8\n",
      "episode: 7, steps: 10\n",
      "episode: 8, steps: 11\n",
      "episode: 9, steps: 10\n",
      "episode: 10, steps: 9\n",
      "episode: 11, steps: 11\n",
      "episode: 12, steps: 26\n",
      "episode: 13, steps: 18\n",
      "episode: 14, steps: 13\n",
      "episode: 15, steps: 12\n",
      "episode: 16, steps: 18\n",
      "episode: 17, steps: 40\n",
      "episode: 18, steps: 24\n",
      "episode: 19, steps: 20\n",
      "episode: 20, steps: 59\n",
      "episode: 21, steps: 41\n",
      "episode: 22, steps: 56\n",
      "episode: 23, steps: 51\n",
      "episode: 24, steps: 61\n",
      "episode: 25, steps: 97\n",
      "episode: 26, steps: 107\n",
      "episode: 27, steps: 85\n",
      "episode: 28, steps: 86\n",
      "episode: 29, steps: 67\n",
      "episode: 30, steps: 99\n",
      "episode: 31, steps: 115\n",
      "episode: 32, steps: 121\n",
      "episode: 33, steps: 102\n",
      "episode: 34, steps: 89\n",
      "episode: 35, steps: 86\n",
      "episode: 36, steps: 73\n",
      "episode: 37, steps: 112\n",
      "episode: 38, steps: 122\n",
      "episode: 39, steps: 73\n",
      "episode: 40, steps: 129\n",
      "episode: 41, steps: 121\n",
      "episode: 42, steps: 83\n",
      "episode: 43, steps: 121\n",
      "episode: 44, steps: 121\n",
      "episode: 45, steps: 107\n",
      "episode: 46, steps: 110\n",
      "episode: 47, steps: 126\n",
      "episode: 48, steps: 115\n",
      "episode: 49, steps: 132\n"
     ]
    }
   ],
   "source": [
    "train(agent, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b510e-4aac-4cb7-8faa-111bb4b232e7",
   "metadata": {},
   "source": [
    "- After only 50 episodes our agent is regularly achieving the full 200 steps\n",
    "    - We chose $\\epsilon=0.9$, so we are forcing the agent to make random decisions 10% of the time\n",
    "    - If we evaluate in greedy mode, we would expect to see perfect scores more often\n",
    "- The added flexibility and generalization power we get from the MLP (relative to complete polynomial) is sufficient to succesfully complete this task!\n",
    "- We'll learn more about DQN and its recent exciting applications in another lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebbaf5-1a1b-49fd-af3f-bb2d2aafd8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "cap-6318",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
