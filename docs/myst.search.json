{"version":"1","records":[{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem"},"type":"lvl1","url":"/rl-01","position":0},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem"},"content":"Prerequisites\n\nLinear Algebra\n\nStatistics and Probability\n\nDynamic Programming\n\nOutcomes\n\nUnderstand the core structure of the reinforcement learning project\n\nSee how “Robot vaccums” may be an example of RL techiniques\n\nKnow the multi-armed bandit problem and see apply the epsilon greedy algorithm to it\n\nReferences\n\nBarto & Sutton book (online by authors \n\nhere) chapters 1-3\n\n","type":"content","url":"/rl-01","position":1},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl2":"Introduction – Meet Roomba"},"type":"lvl2","url":"/rl-01#introduction-meet-roomba","position":2},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl2":"Introduction – Meet Roomba"},"content":"\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"OljPRVMfXW4\", width=800, height=600)\n\n","type":"content","url":"/rl-01#introduction-meet-roomba","position":3},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"My Roomba: first clean","lvl2":"Introduction – Meet Roomba"},"type":"lvl3","url":"/rl-01#my-roomba-first-clean","position":4},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"My Roomba: first clean","lvl2":"Introduction – Meet Roomba"},"content":"Notified me it would take extra long to create a map\n\nSpent about 4 hours moving a few feet, bumping into something, then\nmoving a few more feet\n\n","type":"content","url":"/rl-01#my-roomba-first-clean","position":5},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"One Year Later","lvl2":"Introduction – Meet Roomba"},"type":"lvl3","url":"/rl-01#one-year-later","position":6},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"One Year Later","lvl2":"Introduction – Meet Roomba"},"content":"Now Roomba runs in about 2 1/2 hours\n\nRarely bumps in to anything\n\nCleans automatically on a schedule\n\nCan clean individual rooms on command\n\nLearns a new path when we re-arrange furniture\n\n","type":"content","url":"/rl-01#one-year-later","position":7},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"How might Roomba work?","lvl2":"Introduction – Meet Roomba"},"type":"lvl3","url":"/rl-01#how-might-roomba-work","position":8},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"How might Roomba work?","lvl2":"Introduction – Meet Roomba"},"content":"Roomba first had to explore my house\n\nTo do this, it acted by moving to various locations\n\nIn response to each movement Roomba received a signal about its state\n\nIs it near an object (via cameras)\n\nDid it just bump into something (via sensors)\n\nAfter receiving signal, Roomba chose to act again\n\nThen, Roomba can exploit the knowledge it gained to achieve it’s goal\n\nThe goal for Roomba is to clean my house as effectively and\nefficiently as possible\n\n","type":"content","url":"/rl-01#how-might-roomba-work","position":9},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl2":"What is Reinforcement Learning?"},"type":"lvl2","url":"/rl-01#what-is-reinforcement-learning","position":10},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl2":"What is Reinforcement Learning?"},"content":"Reinforcement Learning (RL) is a branch of Computer Science (Machine Learning)\nthat aims to find algorithms that can learn by doing\n\nTo understand RL, we need to understand:\n\nThe RL problem/setting - today\n\nRL algorithms - another day\n\n","type":"content","url":"/rl-01#what-is-reinforcement-learning","position":11},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"The Reinforcement Learning Problem","lvl2":"What is Reinforcement Learning?"},"type":"lvl3","url":"/rl-01#the-reinforcement-learning-problem","position":12},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"The Reinforcement Learning Problem","lvl2":"What is Reinforcement Learning?"},"content":"Time is discrete and indexed by t\n\nAt each time step (period) the RL agent observes the state of the\nenvironment: S_t \\in \\mathcal{S}\n\nThe agent must choose an action A_t \\in \\mathcal{A}(S_t)\n\nAt the start of period t+1, the agent gets a reward R_{t+1} \\in\n       \\mathbb{R}\n\nReward depends, perhaps stochastically, on S_t and A_t\n\n","type":"content","url":"/rl-01#the-reinforcement-learning-problem","position":13},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Th RL Problem: Objective","lvl2":"What is Reinforcement Learning?"},"type":"lvl3","url":"/rl-01#th-rl-problem-objective","position":14},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Th RL Problem: Objective","lvl2":"What is Reinforcement Learning?"},"content":"The RL agent and its environment give rise to the following sequence of\nevents: S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots\n\nThe goal of the RL agent is to learn to behave optimally by interacting\nwith its environment\n\nObserve S, choose A, observe R... repeat!\n\nTry to maximize all the R_t\n\nMore formally: \\max_{\\{A_t\\}_t} \\sum_{t=0}^{T} \\beta^t R_t\n\n","type":"content","url":"/rl-01#th-rl-problem-objective","position":15},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"The RL Problem, Illustrated","lvl2":"What is Reinforcement Learning?"},"type":"lvl3","url":"/rl-01#the-rl-problem-illustrated","position":16},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"The RL Problem, Illustrated","lvl2":"What is Reinforcement Learning?"},"content":"The setting for RL can be understood by diagram below (From Barto &\nSutton book)\n\n","type":"content","url":"/rl-01#the-rl-problem-illustrated","position":17},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Roomba as RL Problem?","lvl2":"What is Reinforcement Learning?"},"type":"lvl3","url":"/rl-01#roomba-as-rl-problem","position":18},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Roomba as RL Problem?","lvl2":"What is Reinforcement Learning?"},"content":"We can think of Roomba as an RL agent\n\nEach moment in time it receives signals from its sensors (S_t)\n\nIt chooses whether to turn, slow down, speed up, etc. (A_t)\n\nReward R_{t+1} might be:\n\nCollision status\n\nPercentage of job complete\n\nNote: Roomba was almost certainly programmed more deliberately than this,\nbut it is fun to think about!\n\n","type":"content","url":"/rl-01#roomba-as-rl-problem","position":19},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Challenges in RL","lvl2":"What is Reinforcement Learning?"},"type":"lvl3","url":"/rl-01#challenges-in-rl","position":20},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Challenges in RL","lvl2":"What is Reinforcement Learning?"},"content":"Solutions to the RL problem share some common challenges and obstacles\n\nExploration vs exploitation: how to both gather information and behave\noptimally?\n\nCurse of dimensionality: \\mathcal{S} and \\mathcal{A} can be very\nlarge (example: self driving cars)\n\nTime delays: perhaps actions in t don’t impact rewards until t+N\n(example: exercise)\n\nWe’ll explore the exploration/exploitation tradeoff today\n\n","type":"content","url":"/rl-01#challenges-in-rl","position":21},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl2":"Multi-Armed Bandits"},"type":"lvl2","url":"/rl-01#multi-armed-bandits","position":22},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl2":"Multi-Armed Bandits"},"content":"\n\n","type":"content","url":"/rl-01#multi-armed-bandits","position":23},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"What is a Bandit?","lvl2":"Multi-Armed Bandits"},"type":"lvl3","url":"/rl-01#what-is-a-bandit","position":24},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"What is a Bandit?","lvl2":"Multi-Armed Bandits"},"content":"To build intuition for the properties of RL problems and algorithms, we\nwill study a classic problem called the Multi-armed bandit\n\nA bandit is like a slot machine at a Casino\n\nYou pull the arm and a non-deterministic reward is paid out\n\nA multi-armed bandit is like a row of slot machines\n\nEach machine or “bandit” has a fixed, but distinct and unknown\ndistribution of payoffs\n\n","type":"content","url":"/rl-01#what-is-a-bandit","position":25},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"K-Armed Bandit Problem","lvl2":"Multi-Armed Bandits"},"type":"lvl3","url":"/rl-01#k-armed-bandit-problem","position":26},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"K-Armed Bandit Problem","lvl2":"Multi-Armed Bandits"},"content":"The variation of the multi-armed bandit problem we will study is as follows\n\nThere will be k \\in [2, 10] bandits\n\nFor each bandit i = 1, \\dots, k we will draw \\mu_i \\sim U[-3,3]\n\nIf in time step t the agent selects bandit i, we have R_{t+1}\\sim\n       N(\\mu_i, 1)\n\nThe agent is allowed to know this structure (returns are normally\ndistributed with an i specific mean)\n\n","type":"content","url":"/rl-01#k-armed-bandit-problem","position":27},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Values","lvl2":"Multi-Armed Bandits"},"type":"lvl3","url":"/rl-01#values","position":28},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Values","lvl2":"Multi-Armed Bandits"},"content":"Let a \\in \\mathcal{A} represent an arbitrary action\n\nLet q_{*}: \\mathcal{A} \\Longrightarrow \\mathbb{R}, such that\nq_{*}(a) represents the the value of an RL agent of choosing a\n\nIt must be that q_{*}(a) \\equiv E[R_{t+1} | A_t = a]\n\nThe agent does not know the value of this expectation, but can approximate it\n\nLet Q_t: \\mathcal{S} \\times \\mathcal{A} \\Longrightarrow \\mathbb{R} represent the\nagent’s approximation for the value function at time t: Q_t(S_t, a)\n         \\equiv E_t[q_{*}(a)] = E_t[R_{t+1} + Q_{t+1}(S_{t+1}, A_{t+1})| A_t =\n         a]\n\n","type":"content","url":"/rl-01#values","position":29},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Policies","lvl2":"Multi-Armed Bandits"},"type":"lvl3","url":"/rl-01#policies","position":30},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Policies","lvl2":"Multi-Armed Bandits"},"content":"In RL a policy is a rule that dictates behavior\n\nMathematically, a policy is a function: \\pi_t: \\mathcal{S} \\times\n       \\mathbb{R} \\Longrightarrow \\mathcal{A}\n\nPolicies can be determinsitic: for each S, R they always map to the\nsame A\n\nPolicies can be stochastic: for each S, R they map to a probability\ndistribution over \\mathcal{A}\n\nPolicies can be greedy: select the action that maximizes E_t[R_{t+1} | S_t, A_t]\n\n","type":"content","url":"/rl-01#policies","position":31},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"\\epsilon -Greedy Policies","lvl2":"Multi-Armed Bandits"},"type":"lvl3","url":"/rl-01#id-epsilon-greedy-policies","position":32},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"\\epsilon -Greedy Policies","lvl2":"Multi-Armed Bandits"},"content":"When playing a greedy policy, the RL agent chooses to forgo exploration\nof the environment and exploit knowledge previously obtained\n\nThis can be optimal, but only when the agent knows the best action\n\nWhen there is uncertainty about payoffs, it pays to explore, sometimes\n\nOne way to handle the explore-exploit tradeoff is to use an\n\\epsilon -greedy policy\n\nWith probability 1-\\epsilon select a random action, with equal weights\n\nWith probability \\epsilon select the action you believe is best\n\n","type":"content","url":"/rl-01#id-epsilon-greedy-policies","position":33},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Application to Bandit Problem","lvl2":"Multi-Armed Bandits"},"type":"lvl3","url":"/rl-01#application-to-bandit-problem","position":34},{"hierarchy":{"lvl1":"Reinforcement learning, I - the problem","lvl3":"Application to Bandit Problem","lvl2":"Multi-Armed Bandits"},"content":"See interactive example \n\nhere","type":"content","url":"/rl-01#application-to-bandit-problem","position":35},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning"},"type":"lvl1","url":"/rl-02-tdlearning","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning"},"content":"Prerequisites\n\nLinear Algebra\n\nStatistics and Probability\n\nDynamic Programming\n\nReinforcement Learning Introduction\n\nOutcomes\n\nUnderstand the meaning of the Q(s, a) function\n\nUnderstand the concept of a temporal difference\n\nApply temporal differences to form an RL algorithm (Sarsa)\n\nReferences\n\nBarto & Sutton book (online by authors \n\nhere) chapters 4-6\n\nStokey and Lucas (1989) Chapter 4\n\n","type":"content","url":"/rl-02-tdlearning","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl2":"Reminder: Dynamic programming"},"type":"lvl2","url":"/rl-02-tdlearning#reminder-dynamic-programming","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl2":"Reminder: Dynamic programming"},"content":"Let’s begin by recalling what we know about dynamic programming\n\nRecall the cake eating problem:\n\nTime is discrete\n\n\\beta is discount factor\n\nSize of cake is \\bar{x}\n\nConsumption of cake in period t is c_t\n\nUtility function u: \\mathbb{R} \\rightarrow \\mathbb{R} maps from consumption today into happiness\n\n","type":"content","url":"/rl-02-tdlearning#reminder-dynamic-programming","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Sequential Problem","lvl2":"Reminder: Dynamic programming"},"type":"lvl3","url":"/rl-02-tdlearning#sequential-problem","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Sequential Problem","lvl2":"Reminder: Dynamic programming"},"content":"Objective\n\\begin{aligned}\n\\max_{c_t} &\\sum_{t=0}^{\\infty} \\beta^t u(c_t) \\\\\n\\text{subject to } \\quad & \\sum_{t=0}^{\\infty} c_t \\le \\bar{x} \\\\\n& c_t \\ge 0 \\quad \\forall t\n\\end{aligned}\n\nNeed to solve for infinite sequence \\{c_t\\}_t\n\nOr...\n\n","type":"content","url":"/rl-02-tdlearning#sequential-problem","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Value Function","lvl2":"Reminder: Dynamic programming"},"type":"lvl3","url":"/rl-02-tdlearning#value-function","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Value Function","lvl2":"Reminder: Dynamic programming"},"content":"We can set up a value function v(\\bar{x}) \\equiv \\sum_{t=0}^{\\infty} \\beta^t u(c_t)\n\nv(\\bar{x}) is the total value the consumer places on having a cake of size \\bar{x}\n\nDecompose v into two steps: first period + later periods v(\\bar{x}) = \\underbrace{u(c_0)}_{\\text{flow utility}} + \\underbrace{\\beta \\sum_{t=1}^{\\infty} \\beta^{t-1} u(c_t)}_{\\text{continuation utility}}\n\n","type":"content","url":"/rl-02-tdlearning#value-function","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Recursive Formulation","lvl2":"Reminder: Dynamic programming"},"type":"lvl3","url":"/rl-02-tdlearning#recursive-formulation","position":8},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Recursive Formulation","lvl2":"Reminder: Dynamic programming"},"content":"Note: continuation utility depends on x_{t+1} = x_t - c_t\n\nUse this observation to write v: \\mathbb{R}^+ \\rightarrow \\mathbb{R} recursively:\n\\begin{aligned}\nv(x_t) &= \\max_{0 \\leq c_t  \\leq x} \\underbrace{u(c_t)}_{\\text{flow utility}} + \\underbrace{\\beta v(x_t - c_t)}_{\\text{continuation value}}\n\\end{aligned}\n\nThis is known as the Bellman Equation\n\n","type":"content","url":"/rl-02-tdlearning#recursive-formulation","position":9},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Solution to Recursive Problem","lvl2":"Reminder: Dynamic programming"},"type":"lvl3","url":"/rl-02-tdlearning#solution-to-recursive-problem","position":10},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Solution to Recursive Problem","lvl2":"Reminder: Dynamic programming"},"content":"A solution to the dynamic program consists of two functions:\n\nValue function v^*(x): \\mathbb{R}^+ \\rightarrow \\mathbb{R} -- value of beginning period with x cake remaining\n\nPolicy function c^*(x): \\mathbb{R}^+ \\rightarrow [0, x] -- optimal level of consumption with x cake remaining\n\nUnder certain regularity conditions (which we assume), the recursive problem (and its solution) is equivalent to the sequential problem we started with\n\n","type":"content","url":"/rl-02-tdlearning#solution-to-recursive-problem","position":11},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Connection to RL","lvl2":"Reminder: Dynamic programming"},"type":"lvl3","url":"/rl-02-tdlearning#connection-to-rl","position":12},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Connection to RL","lvl2":"Reminder: Dynamic programming"},"content":"How does this connect to RL?\n\n(S, A, R) pattern for RL is very closely related to recursive formulation of dynamic programming\n\nS_t \\Longrightarrow x_t\n\nA_t \\Longrightarrow c_t\n\nR_t \\Longrightarrow u(c_t)\n\nExpressing v(x) = \\text{flow utility} + \\text{ continuation value} is like repeating (S, A, R) sequence many times\n\nBaseline algorithm for solving DP problem (VFI) is quite similar to how basic RL algorithms work\n\nStart with guess for value, make decision, update guess, repeat...\n\n","type":"content","url":"/rl-02-tdlearning#connection-to-rl","position":13},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl2":"TD-Learning"},"type":"lvl2","url":"/rl-02-tdlearning#td-learning","position":14},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl2":"TD-Learning"},"content":"\n\n","type":"content","url":"/rl-02-tdlearning#td-learning","position":15},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Baseline Assumptions","lvl2":"TD-Learning"},"type":"lvl3","url":"/rl-02-tdlearning#baseline-assumptions","position":16},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Baseline Assumptions","lvl2":"TD-Learning"},"content":"Let state space \\mathcal{S} and action space \\mathcal{A} be discrete\n\nLet S_t \\in \\mathcal{S}  represent state at time t\n\nLet A_t \\in \\mathcal{A}(S_t) represent action at time t\n\nLet R_{t+1} \\in \\mathcal{R} \\subseteq \\mathbb{R} represent reward at time t+1\n\nLet state transitions satisfy the Markov property such that\n\\begin{aligned}\n& p(s', r | s, a) = \\text{Prob}(S_{t+1}=s', R_{t+1}=r | S_{t} = s, A_{t} = 1) \\\\\n\\text{ where } \\quad & \\sum_{s' \\in \\mathcal{S}} \\sum_{R \\in \\mathcal{R}} p(s', r | s, a) = 1 \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(S) \\\\\n\\text{ and } \\quad & p(s', r | s, a) \\ge = 0 \\quad \\forall s, s' \\in \\mathcal{S}, a \\in \\mathcal{A}(s), r \\in \\mathcal{R}\n\\end{aligned}\n\nNote Markov means that probability for S_{t+1}, R_{t+1} only depends on S_t, A_t and not and S_i, A_i, R_i where i < t\n\n","type":"content","url":"/rl-02-tdlearning#baseline-assumptions","position":17},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"State Value Function","lvl2":"TD-Learning"},"type":"lvl3","url":"/rl-02-tdlearning#state-value-function","position":18},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"State Value Function","lvl2":"TD-Learning"},"content":"Let v^*(s) be the optimal value of being in state s (called state value function)\n\nWe write this as:\n\\begin{aligned}\nv^*(s) &= \\max_{a \\in \\mathcal{A}(s)} E \\left[R_{t+1} + \\beta v^*(S_{t+1}) | S_t=s, A_t=a \\right] \\\\\n&= \\max_{a \\in \\mathcal{A}(s)} \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta v^*(s') \\right]\n\\end{aligned}\n\nShould be familiar from our dynamic programming studies\n\nNote expectation around the flow utility term R_{t+1}, leaving room for that reward to be stochastic\n\n","type":"content","url":"/rl-02-tdlearning#state-value-function","position":19},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Action Value Function","lvl2":"TD-Learning"},"type":"lvl3","url":"/rl-02-tdlearning#action-value-function","position":20},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Action Value Function","lvl2":"TD-Learning"},"content":"We can also write an action value function\n\nLet q^*(s, a) be the optimal value of being in state s and choosing action a:\n\\begin{aligned}\nq^*(s, a) &= E \\left[R_{t+1} + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(S_{t+1}, a') | S_t=s, A_t=a \\right] \\\\\n&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(s', a') \\right]\n\\end{aligned}\n\nNotice max operator is now inside the expectation and applied to future decision a'\n\nThe function q^*(s, a) is more general than v^*(s): v^*(s) = \\max_{a \\in \\mathcal{A}(s)} q^*(s, a)\n\n","type":"content","url":"/rl-02-tdlearning#action-value-function","position":21},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Acting with q^*","lvl2":"TD-Learning"},"type":"lvl3","url":"/rl-02-tdlearning#acting-with-q","position":22},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Acting with q^*","lvl2":"TD-Learning"},"content":"Goal of RL is to learn to make decisions that maximize \\sum \\beta^t R_t\n\nKnowing q^*(s, a) tells us maximium value of being in state s and choosing a\n\nIf we knew q^*, acting optimally would be easy: a^*(s) = \\text{argmax}_{a\n         \\in \\mathcal{A}(s)} q^*(s, a)\n\nHowever, we rarely if ever know q^*, so we must approximate it\n\nWe will let Q(s, a) represent our approximation of q^*\n\n","type":"content","url":"/rl-02-tdlearning#acting-with-q","position":23},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Approximating q^*","lvl2":"TD-Learning"},"type":"lvl3","url":"/rl-02-tdlearning#approximating-q","position":24},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Approximating q^*","lvl2":"TD-Learning"},"content":"The goal of TD learning is to find an accurate approximation Q(s, a) such that Q(s, a) \\approx q^*(s, a) \\; \\forall s, a)\n\nThere are many RL algorithms that seek to do this\n\nWe’ll focus on two:\n\nSarsa\n\nQ-learning\n\n","type":"content","url":"/rl-02-tdlearning#approximating-q","position":25},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl2":"Temporal Differences"},"type":"lvl2","url":"/rl-02-tdlearning#temporal-differences","position":26},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl2":"Temporal Differences"},"content":"The Bellman equaition for our approximation Q(s, a) is Q(s,a) = E[R' + \\beta \\max_{a'} Q(s', a') | s,a]\n\nSuppose that we interacted with environment and have in hand (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\n\nNow we plug these into the Bellman by:\n\nUsing the form of the Bellman\n\nBut drop E and \\max because we already know the transition that did occur from (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\n\nChange = an \\approx because this isn’t evaluating full Bellman\nQ(S_t,A_t) \\approx R_{t+1} + \\beta  Q(S_{t+1}, A_{t+1})\n\nThe difference between the left and right and sides is known as a temporal difference: TD(0)(Q) \\equiv R_{t+1} + \\beta Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)\n\nThere are extensions to the temporal difference that allow for multiple time periods. The 0 in TD(0) indicates that this is one-step TD learning. See Chapters 7 and 12 of Sutton/Barto for more info\n\n","type":"content","url":"/rl-02-tdlearning#temporal-differences","position":27},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Learning using TD(0)","lvl2":"Temporal Differences"},"type":"lvl3","url":"/rl-02-tdlearning#learning-using-td-0","position":28},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Learning using TD(0)","lvl2":"Temporal Differences"},"content":"We can use temporal differences to improve our approximation Q:\n\nLet Q_t(s, a) represent our approximation at the start of period t\n\nSimilar to gradient descent methods, we will take a step from Q_t in a direction that improves its accuracy\n\nTo improve accuracy we step in direction of TD(0)(Q_t) (using step size \\alpha):\n\\begin{aligned}\nQ_{t+1}(S_t, A_t) &= Q_t(S_t, A_t) + \\alpha TD(0)(Q_t) \\\\\n&= Q_t(S_t, A_t) + \\alpha \\left[R_{t+1} + \\beta Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t, A_t) \\right]\n\\end{aligned}\n\n","type":"content","url":"/rl-02-tdlearning#learning-using-td-0","position":29},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Sarsa Algorithm","lvl2":"Temporal Differences"},"type":"lvl3","url":"/rl-02-tdlearning#sarsa-algorithm","position":30},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Sarsa Algorithm","lvl2":"Temporal Differences"},"content":"The Sarsa algorithm applies the update rule we just described\n\nThe algorithm is summarized by Barto and Sutton as follows (section 6.4)\n\nfrom collections import defaultdict\nimport random\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass TabularQ:\n    def __init__(self):\n        self.Q = defaultdict(lambda: 0)\n\n    def __call__(self, s, a):\n        return self.Q[(s.observable_state(), a)]\n\n    def __setitem__(self, k, v):\n        s, a = k\n        self.Q[(s.observable_state(), a)] = v\n\n    def get_greedy(self, s, A_s):\n        vals = [self(s, a) for a in A_s]\n        max_val = max(vals)\n        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])\n\nclass Sarsa(object):\n    def __init__(self, environment, epsilon=0.9, alpha=0.1, beta=1.0):\n        self.env = environment\n        self.Q = TabularQ()\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.beta = beta\n\n        self.restart_episode()\n\n    def restart_episode(self):\n        # These will be (S, A) in our notation. Need to initialize\n        self.s = self.env.reset()\n        self.a = self.act(self.s, self.env.enumerate_options(self.s))\n\n    def get_greedy(self, s, A_s):\n        return self.Q.get_greedy(s, A_s)\n\n    def act(self, s, A_s):\n        if random.random() > self.epsilon:\n            return random.choice(A_s)\n        return self.get_greedy(s, A_s)\n\n    def done(self, s=None) -> bool:\n        return self.env.done(s if s else self.s)\n\n    def step(self):\n        # first take the step (s, a)\n        s, a = self.s, self.a\n        sp, r = self.env.step(s, a)\n\n        if self.done(sp):\n            # game is over\n            self.s = sp\n            return\n\n        # then use policy to compute ap\n        A_sp = self.env.enumerate_options(sp)\n        ap = self.act(sp, A_sp)\n\n        # now we know S-A-R-S'-A' -- ready to do update\n        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n        Q[(s, a)] = Q(s, a) + α * (r + β * Q(sp, ap) - Q(s, a))\n\n        # step forward in time\n        self.s = sp\n        self.a = ap\n\n","type":"content","url":"/rl-02-tdlearning#sarsa-algorithm","position":31},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Example: Farkle","lvl2":"Temporal Differences"},"type":"lvl3","url":"/rl-02-tdlearning#example-farkle","position":32},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Example: Farkle","lvl2":"Temporal Differences"},"content":"In a separate video we implemented the dice game farkle\n\nWe’ll re-use that code as an environment for Sarsa algorithm\n\nFor a review of farkle, see video\n\nToday we’ll approach it like the RL algorihtm will:\n\nA stochastic environment that sends states, a list of possible actions, and rewards\n\nWe will not specialize based on rules of game\n\n","type":"content","url":"/rl-02-tdlearning#example-farkle","position":33},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Farkle env","lvl2":"Temporal Differences"},"type":"lvl3","url":"/rl-02-tdlearning#farkle-env","position":34},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl3":"Farkle env","lvl2":"Temporal Differences"},"content":"Let’s wrap farkle code into environment Sarsa expects\n\nNeed a few key methods:\n\nreset() -> State\n\nenumerate_options(state) -> List[Action]\n\nstep(s: State, a: Action) -> Tuple[State, Reward]\n\ndone(s:State) -> bool\n\nfrom farkle import State, Action, RandomFarklePlayer, FarklePlayer, STOP, BANKRUPT\nfrom typing import List, Tuple\n\nclass FarkleEnv:\n    # first, some helper methods\n    def __init__(\n            self,\n            opponent: FarklePlayer=RandomFarklePlayer(),\n            points_to_win=10_000,\n            verbose: bool = False\n        ):\n        self.points_to_win = points_to_win\n        self.opponent = opponent\n        self.n_players = 2\n        self._state = State(self.n_players)\n        self._history: List[Tuple[State, Action]] = []\n\n    @property\n    def state(self) -> State:\n        return self._state\n\n    def set_state(self, action: Action, new_state: State):\n        self._history.append((self.state, action))\n        self._state = new_state\n\n    def opponent_turn(self, s: State) -> State:\n        choices = s.enumerate_options()\n        action = self.opponent.act(s, choices)\n        sp = s.step(action)\n\n        # check if player chose to stop\n        if sp.current_player != 1:\n            return sp\n\n        # Player didn't stop, but still their turn. Call again\n        return self.opponent_turn(sp)\n\n    # key methods needed\n    def done(self, state) -> bool:\n        return any(score > self.points_to_win for score in state.scores)\n\n    def reset(self):\n        self._state = State(self.n_players)\n        self._history = []\n        return self.state.roll()\n\n    def step(self, s: State, a: Action) -> Tuple[State, int]:\n        sp = s.step(a)\n        r = 0\n\n        # see if we ended\n        if sp.current_player != 0:\n            if a is STOP:\n                # only score when we choose to stop\n                r = s.turn_sum\n\n            # take opponent turn\n            sp = self.opponent_turn(sp)\n\n        self.set_state(a, sp)\n        return sp, r\n\n    def enumerate_options(self, s: State) -> List[Action]:\n        return self.state.enumerate_options(s.rolled_dice)\n\n","type":"content","url":"/rl-02-tdlearning#farkle-env","position":35},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl4":"Playing Farkle","lvl3":"Farkle env","lvl2":"Temporal Differences"},"type":"lvl4","url":"/rl-02-tdlearning#playing-farkle","position":36},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl4":"Playing Farkle","lvl3":"Farkle env","lvl2":"Temporal Differences"},"content":"Let’s try it out!\n\nWe need to create an env, then pass it to sarsa\n\nWe’ll also define a helper function to play a game for us\n\nenv = FarkleEnv()\nsarsa = Sarsa(env, epsilon=0.9, alpha=0.6)\n\ndef play_game(algo):\n    algo.restart_episode()\n    while not algo.done():\n        algo.step()\n    return algo\n\nplay_game(sarsa)\nsarsa.s\n\n\n\nscores = [h[0].scores for h in env._history]\nplt.plot(scores)\nplt.legend([\"sarsa\", \"random\"])\n\nfrom farkle import HumanFarklePlayer\nenv_human = FarkleEnv(opponent=HumanFarklePlayer(name=\"Spencer\"), points_to_win=2000)\nsarsa_human = Sarsa(env_human)\n\n# play_game(sarsa_human)\n\nsarsa_human.s\n\n","type":"content","url":"/rl-02-tdlearning#playing-farkle","position":37},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl4":"Learning","lvl3":"Farkle env","lvl2":"Temporal Differences"},"type":"lvl4","url":"/rl-02-tdlearning#learning","position":38},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl4":"Learning","lvl3":"Farkle env","lvl2":"Temporal Differences"},"content":"Great! Our algorithm can play Farkle\n\nBut... it needs to play many games to learn how to play well\n\nLet’s let it play many more games to build up some intelligence\n\n%%time\n\ndef play_many_games(N):\n    terminal_states = []\n    print_skip = N // 10\n    for i in range(N):\n        play_game(sarsa)\n        terminal_states.append(sarsa.s)\n        if i % print_skip == 0:\n            print(f\"Done with {i}/{N} (len(Q) = {len(sarsa.Q.Q)})\")\n    return terminal_states\n\n# WARNING: this takes a *long time* and requires a lot of ram!\n# Only use on a computer with at least 32 GB ram\n# There are ways we could optimize this... such as only including\n# final score in `terminal_states` and dropping things like current_round\n# from the state\nsarsa_history = play_many_games(5000)\n\n","type":"content","url":"/rl-02-tdlearning#learning","position":39},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl4":"Did we learn?","lvl3":"Farkle env","lvl2":"Temporal Differences"},"type":"lvl4","url":"/rl-02-tdlearning#did-we-learn","position":40},{"hierarchy":{"lvl1":"Reinforcement Learning – TD learning","lvl4":"Did we learn?","lvl3":"Farkle env","lvl2":"Temporal Differences"},"content":"Let’s analyze the history and see if the algorithm learned with experience\n\nwon = np.array([s.scores[0] > s.scores[1] for s in sarsa_history])\ngame_idx = np.arange(len(won))\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(game_idx, won.cumsum())\nax.plot(game_idx, 0.5 * game_idx)\nplt.legend([\"sarsa\", \"E[random agent]\"])","type":"content","url":"/rl-02-tdlearning#did-we-learn","position":41},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning"},"type":"lvl1","url":"/rl-03-qlearning","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning"},"content":"Prerequisites\n\nLinear Algebra\n\nStatistics and Probability\n\nDynamic Programming\n\nReinforcement Learning Introduction\n\nReinforcement Learning Sarsa algorithm\n\nOutcomes\n\nKnow the difference between on policy and off policy learning\n\nLearn the Q-learning algorithm for off policy TD based control\n\nReferences\n\nBarto & Sutton book (online by authors \n\nhere) chapters 4-6\n\n","type":"content","url":"/rl-03-qlearning","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl2":"Recap"},"type":"lvl2","url":"/rl-03-qlearning#recap","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl2":"Recap"},"content":"In the RL problem agent observes S, makes decision A, sees reward and next state R, S' -- then process repeats S, A, R, S', A', ...\n\nSarsa uses a (S, A, R, S', A') quintuple to learn Q(s, a) that approximates q^*(s, a)\n\nNotice Sarsa uses \\epsilon-greedy policy to propose a' AND uses that A' when updating Q\n\n\n","type":"content","url":"/rl-03-qlearning#recap","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"On-policy vs off-policy methods","lvl2":"Recap"},"type":"lvl3","url":"/rl-03-qlearning#on-policy-vs-off-policy-methods","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"On-policy vs off-policy methods","lvl2":"Recap"},"content":"Because Sarsa uses the Q about which it is learning to generate A', it is known as an on-policy learning method\n\nOn policy: make decisions based on value (policy) function being learned\n\nAlternative: follow any policy for proposing A', but use the greedy policy derived from Q when computing TD(0)...\n\nThis is what we’ll explore today\n\n","type":"content","url":"/rl-03-qlearning#on-policy-vs-off-policy-methods","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl2":"Q-learning"},"type":"lvl2","url":"/rl-03-qlearning#q-learning","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl2":"Q-learning"},"content":"An early theoretical breakthrough in RL was the idea of off-policy learning\n\nThe Q-learning algorithm was the first off-policy control algorithm to be suggested\n\nIt allows the algorithm to make use of S, A, R, S' transitions obtained from any source, and still learn an approximation Q that converges to q^* with probability 1\n\nConvergence requires some conditions, most importantly that the transitions S, A, R, S' cover the action space of q^*\n\nCoverage means all (s, a) pairs that are optimal under q^* must be visited by the S, A, R, S' transitions\n\n","type":"content","url":"/rl-03-qlearning#q-learning","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Example: Self-Driving Car","lvl2":"Q-learning"},"type":"lvl3","url":"/rl-03-qlearning#example-self-driving-car","position":8},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Example: Self-Driving Car","lvl2":"Q-learning"},"content":"Goal: train RL agent to safely drive vechicle\n\nSarsa method:\n\nGive control of vehicle over to Sarsa, so it can choose A and observe implied R, S' transitions\n\nOff-policy:\n\nLet human expert driver drive vehicle in intended way\n\nRecord S, A, R, S' transitions visited by human driver\n\nTrain RL agent based on data generated from human experience\n\n","type":"content","url":"/rl-03-qlearning#example-self-driving-car","position":9},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"The Q-learning Algorithm","lvl2":"Q-learning"},"type":"lvl3","url":"/rl-03-qlearning#the-q-learning-algorithm","position":10},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"The Q-learning Algorithm","lvl2":"Q-learning"},"content":"\n\nA that are suggested are \\epsilon-greedy in Q\n\nThis is a suggestion for how to generate A, but anything else (including totally random) could be used\n\nWhen updating the t+1 component of TD(0)(Q) there is an explicit max_{a'} Q(S', a') -- it is always greedy - By computing $TD(0)(Q)$ updates that are greedy in $Q$, Q-learning can converge to $q^*$ regardless of how $A$ are generated \n\n","type":"content","url":"/rl-03-qlearning#the-q-learning-algorithm","position":11},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl2":"Q-learning Farkle"},"type":"lvl2","url":"/rl-03-qlearning#q-learning-farkle","position":12},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl2":"Q-learning Farkle"},"content":"Let’s implement the Q-learning algorithm to solve our farkle game\n\nFirst, some code optimizations:\n\nfarkle.py has been updated to include a method State.observable_state\n\nThis method returns a tuple containing only how many dice are rollable, sum collected in turn, and what rolled dice are showing\n\nDrops scores, round, etc.\n\nImplication -- we will have agent learn to maximize score each turn\n\nIf agent scores high every turn, should be able to win game\n\nLoses ability to customize behavior based on “stage” of game (aggressive play to catch up, or conservative to maintain lead)\n\nCreatd a TablularQ class below that uses this State.observable_state method\n\nAllows RL algorithms to not worry about State.observable_state\n\nRemove history tracking from FarkleEnv (see FarkleEnv in farkle.py file)\n\nfrom collections import defaultdict\nimport random\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nclass TabularQ:\n    def __init__(self, default_value= lambda: 0):\n        val = default_value if callable(default_value) else lambda x: default_value\n        self.Q = defaultdict(lambda: default_value)\n\n    def __call__(self, s, a):\n        return self.Q[(s.observable_state(), a)]\n\n    def __setitem__(self, k, v):\n        s, a = k\n        self.Q[(s.observable_state(), a)] = v\n\n    def get_greedy(self, s, A_s):\n        vals = [self(s, a) for a in A_s]\n        max_val = max(vals)\n        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])\n\n","type":"content","url":"/rl-03-qlearning#q-learning-farkle","position":13},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Q-learning implementation","lvl2":"Q-learning Farkle"},"type":"lvl3","url":"/rl-03-qlearning#q-learning-implementation","position":14},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Q-learning implementation","lvl2":"Q-learning Farkle"},"content":"We implement Q-learning in the QLearning class below\n\nclass Qlearning(object):\n    def __init__(self, environment, default_value=0, epsilon=0.9, alpha=0.1, beta=1.0):\n        self.env = environment\n        self.Q = TabularQ(default_value=default_value)\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.beta = beta\n\n        self.restart_episode()\n\n    def restart_episode(self):\n        self.s = self.env.reset()\n\n    def get_greedy(self, s, A_s):\n        return self.Q.get_greedy(s, A_s)\n\n    def generate_A(self, s, A_s):\n        if random.random() > self.epsilon:\n            return random.choice(A_s)\n        return self.get_greedy(s, A_s)\n\n    def done(self, s=None) -> bool:\n        return self.env.done(s if s else self.s)\n\n    def step(self):\n        s = self.s\n        # first generate an A\n        A_s = self.env.enumerate_options(s)\n        a = self.generate_A(s, A_s)\n\n        # take step\n        sp, r = self.env.step(s, a)\n\n        if self.done(sp):\n            # game is over\n            self.s = sp\n            return\n\n        # get greedy a' based on Q and sp\n        A_sp = self.env.enumerate_options(sp)\n        ap = self.get_greedy(sp, A_sp)\n\n        # Do TD update\n        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n        Q[(s, a)] = Q(s, a) + α * (r + β * Q(sp, ap) - Q(s, a))\n\n        # step forward in time\n        self.s = sp\n\n","type":"content","url":"/rl-03-qlearning#q-learning-implementation","position":15},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Single Game test","lvl2":"Q-learning Farkle"},"type":"lvl3","url":"/rl-03-qlearning#single-game-test","position":16},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Single Game test","lvl2":"Q-learning Farkle"},"content":"\n\nfrom farkle import FarkleEnv, play_game, play_many_games\n\nrandom.seed(40)\n\nenv = FarkleEnv(track_history=False)\nql = Qlearning(env)\nplay_game(ql)\nql.s\n\n","type":"content","url":"/rl-03-qlearning#single-game-test","position":17},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Longer training","lvl2":"Q-learning Farkle"},"type":"lvl3","url":"/rl-03-qlearning#longer-training","position":18},{"hierarchy":{"lvl1":"Reinforcement Learning – Q-Learning","lvl3":"Longer training","lvl2":"Q-learning Farkle"},"content":"Let’s now let our qlearning algorithm train on 5,000 games\n\n%%time\n\nrandom.seed(42)  # reset seed for reproducibility\nqlearning_history = play_many_games(ql, 5000)\n\ndef plot_win_rate(history):\n    won = np.array([s.scores[0] > s.scores[1] for s in history])\n    game_idx = np.arange(len(won))\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    ax.plot(game_idx, won.cumsum())\n    ax.plot(game_idx, 0.5 * game_idx)\n    plt.legend([\"algo\", \"E[random agent]\"])\n    print(f\"won {sum(won)}/{len(won)} games\")\n\nplot_win_rate(qlearning_history)\n\nExcellent! Our Q-learning algorithm seems to be doing quite a bit better than we would expect a random agent to do","type":"content","url":"/rl-03-qlearning#longer-training","position":19},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example"},"type":"lvl1","url":"/rl-04-career-rl","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example"},"content":"Prerequisites\n\nDynamic Programming\n\nReinforcement Learning Introduction\n\nReinforcement Learning Sarsa algorithm\n\nReinforcement Learning Q-learning algorithm\n\nOutcomes\n\nUnderstand the main points of the career choice model of Derek Neal\n\nSee how career choice model can be cast as a RL problem\n\nUse Sarsa and QLearning to “solve” career choice model\n\nReferences\n\nBarto & Sutton book (online by authors \n\nhere) chapters 4-6\n\nQuantEcon lecture on career choice model\n\n","type":"content","url":"/rl-04-career-rl","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl2":"Disclaimer: Julia"},"type":"lvl2","url":"/rl-04-career-rl#disclaimer-julia","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl2":"Disclaimer: Julia"},"content":"The code for this notebook is written in the \n\nJulia programming language\n\nJulia is a newer language created specificly for doing numerical computing\n\nIt is friendly like Python, but can run much faster\n\nMost of the code for this example in in a file neal.jl, which we will not study directly\n\nThe examples that do appear in the notebook will be commented and should be similar enough to Python for us to follow\n\nTo learn more about Julia, we recommend the \n\nQuantEcon Julia Lectures\n\nNote: using Julia was necessary to have the code run quickly enough...\n\n","type":"content","url":"/rl-04-career-rl#disclaimer-julia","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl2":"Career Model"},"type":"lvl2","url":"/rl-04-career-rl#career-model","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl2":"Career Model"},"content":"We’ll summarize the model here\n\nSee the \n\nQuantEcon lecture for more details\n\nWorkers have a career and a job\n\nCareer is general field\n\nJob is position within a firm -- many jobs per career\n\nWage has career and job component: w_t = \\underbrace{\\theta_t}_{\\text{career part}} + \\underbrace{\\epsilon_t}_{\\text{job part}}\n\nEach period, worker has three mutually exclusive choices\n\nStay at job -- “stay put”\n\nSwitch jobs in same career -- “new job”\n\nSwich careers (requires new job) -- “new life”\n\nDraws of \\theta and \\epsilon are time-invariant:\\theta_t \\sim F, \\epsilon_t \\sim G \\quad \\forall t\n\nNew \\epsilon and \\theta are only drawn when a worker switches jobs or careers\n\n","type":"content","url":"/rl-04-career-rl#career-model","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"State Value Function","lvl2":"Career Model"},"type":"lvl3","url":"/rl-04-career-rl#state-value-function","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"State Value Function","lvl2":"Career Model"},"content":"State of model is s = (\\theta, \\epsilon)\n\nThe state value function obeys\n\\begin{aligned}\nv(\\theta, \\epsilon) &= \\max\\{I, II, III\\}, \\text{ where} \\\\\n& I = \\theta + \\epsilon + \\beta v(\\theta, \\epsilon) \\\\\n& II = \\theta + E[ \\epsilon' + \\beta  v(\\theta, \\epsilon')  | \\epsilon' \\sim G] \\\\\n& III = E[\\theta' + \\epsilon' + \\beta v(\\theta', \\epsilon') |  \\epsilon' \\sim G, \\theta' \\sim F]\n\\end{aligned}\n\nI, II, and III correspond to value of “stay put”, “new job”, “new life”; respectively\n\n","type":"content","url":"/rl-04-career-rl#state-value-function","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Action Value Function","lvl2":"Career Model"},"type":"lvl3","url":"/rl-04-career-rl#action-value-function","position":8},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Action Value Function","lvl2":"Career Model"},"content":"The action space is \\mathcal{A} = \\{\\text{stay put}, \\text{ new job}, \\text{ new life} \\} for all states\n\nThe action value function Q(s, a) is:\n\\begin{aligned}\nQ(\\theta, \\epsilon, \\text{ stay put}) &= \\theta + \\epsilon + \\beta \\max_{a'} Q(\\theta, \\epsilon, a') \\\\\nQ(\\theta, \\epsilon, \\text{ new job}) &= \\theta + E[ \\epsilon' + \\beta  \\max_{a'} Q(\\theta, \\epsilon', a')  | \\epsilon' \\sim G] \\\\\nQ(\\theta, \\epsilon, \\text{ new life}) &= E[\\theta' + \\epsilon' + \\beta  \\max_{a'} Q(\\theta', \\epsilon', a')  | \\epsilon' \\sim G, \\theta' \\sim F] \\\\\n\\end{aligned}\n\n","type":"content","url":"/rl-04-career-rl#action-value-function","position":9},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Discrete Distributions","lvl2":"Career Model"},"type":"lvl3","url":"/rl-04-career-rl#discrete-distributions","position":10},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Discrete Distributions","lvl2":"Career Model"},"content":"In our examples today, we will use discrete distributions for F and G\n\nThere will be a finite number of possible \\epsilon and \\theta -- each with a fixed probability of being drawn from their repsective distributions\n\n# set up all dependencies for this code\nusing Pkg\nPkg.activate(\"..\")\nPkg.add\n\n# load up example code\ninclude(\"src/Career.jl\")\nusing .Career\n\nm = NealDSDC(N=15);\nplot_distributions(m)\n\n\nNotes:\n\nBoth \\theta and \\epsilon live in [0, 5]\n\n\\theta is uniformly distributed\n\n\\epsilon looks to be nearly Gaussian on that range\n\n","type":"content","url":"/rl-04-career-rl#discrete-distributions","position":11},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Optimal solution via Dynamic Programming","lvl2":"Career Model"},"type":"lvl3","url":"/rl-04-career-rl#optimal-solution-via-dynamic-programming","position":12},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Optimal solution via Dynamic Programming","lvl2":"Career Model"},"content":"We can apply dynamic programming methods to solve this model numerically\n\nWe have code that can solve model this via value function iteration\n\nHere’s the state value function v^* for this model:\n\nplot_vf(m)\n\n","type":"content","url":"/rl-04-career-rl#optimal-solution-via-dynamic-programming","position":13},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl4":"Policy function a^*(s)","lvl3":"Optimal solution via Dynamic Programming","lvl2":"Career Model"},"type":"lvl4","url":"/rl-04-career-rl#policy-function-a-s","position":14},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl4":"Policy function a^*(s)","lvl3":"Optimal solution via Dynamic Programming","lvl2":"Career Model"},"content":"Optimal policy function is easier to read/understand\n\nWe’ll look more at it\n\nplot_policy(m)\n\nNotes\n\nIf both job and career are good, worker stays put\n\nIf career is good, worker explores new jobs until \\epsilon is sufficiently high\n\nIf neither is very good, worker re-draws both career and job\n\n","type":"content","url":"/rl-04-career-rl#policy-function-a-s","position":15},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl4":"Comment on model knowledge","lvl3":"Optimal solution via Dynamic Programming","lvl2":"Career Model"},"type":"lvl4","url":"/rl-04-career-rl#comment-on-model-knowledge","position":16},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl4":"Comment on model knowledge","lvl3":"Optimal solution via Dynamic Programming","lvl2":"Career Model"},"content":"See code for bellman_operator! repeated below\n\nNote in doing VFI and leveraging this operator, we let algorithm (VFI) know the model for \\theta and \\epsilon (we use F and G to take expectations)function bellman_operator!(m::NealDSDC, v::Array, out::Array;\n                           ret_policy=false)\n    # new life. This is a function of the distribution parameters and is\n    # always constant. No need to recompute it in the loop\n    v3 = (m.G_mean + m.F_mean + m.beta .* (m.F_probs' * v * m.G_probs))\n\n    for j=1:m.N\n        for i=1:m.N\n            # stay put\n            v1 = m.θ[i] + m.ϵ[j] + m.beta * v[i, j]\n\n            # new job\n            v2 = m.θ[i] + m.G_mean + m.beta * dot(v[i, :], m.G_probs)\n\n            if ret_policy\n                if v1 >= max(v2, v3)\n                    action = 1\n                elseif v2 >= max(v1, v3)\n                    action = 2\n                else\n                    action = 3\n                end\n                out[i, j] = action\n            else\n                out[i, j] = max(v1, v2, v3)\n            end\n        end\n    end\nend\n\n","type":"content","url":"/rl-04-career-rl#comment-on-model-knowledge","position":17},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl2":"Model-free learning"},"type":"lvl2","url":"/rl-04-career-rl#model-free-learning","position":18},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl2":"Model-free learning"},"content":"RL algorithms like Sarsa and Q-learning are model-free algorithms\n\nThey do not take into account any specialized structure about the environment\n\nSarsa and Q-learning learn entirely from trial and error\n\nBenefits: can learn in situations where we don’t know dynamics\n\nDownfalls: don’t leverage model structure to accelerate learning ==> takes longer!\n\n","type":"content","url":"/rl-04-career-rl#model-free-learning","position":19},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Sarsa for career model","lvl2":"Model-free learning"},"type":"lvl3","url":"/rl-04-career-rl#sarsa-for-career-model","position":20},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Sarsa for career model","lvl2":"Model-free learning"},"content":"Inside the src/neal.jl file you see the following code for the Sarsa algorithm# Barto & Sutton Algorithm in figure 6.5\nmutable struct Sarsa <: AbstractQAlgorithm\n    Q::QFunction  # wrapper around 3d array (a, θ, ϵ)\n    α::Float64\n    \n    # store (s, a) to use in updates\n    s::State\n    a::Action\nend\n\nfunction update!(wp::NealDSDC, actor::AbstractActor, algo::Sarsa)\n    # use current state and action to do transition\n    sp, r = step(wp, algo.s, algo.a)\n\n    # get next action using actor's strategy\n    ap = get_action(wp, algo, actor, sp)[1]\n\n    # Apply TD update\n    algo.Q[algo.s, algo.a] = (1-algo.α)*algo.Q[algo.s, algo.a] +\n                             algo.α*(r + wp.beta * algo.Q[sp, ap])\n\n    # step forward in time\n    algo.s = sp\n    algo.a = ap\nend\n\nBelow we’ll set up a Sarsa model and plot the initial policy function\n\n# set random seed\nusing Random\nRandom.seed!(56)\nsarsa = Sarsa(m, 0.01);\n\n# plot initial policy\nplot_policy(m, get_greedy(m, sarsa.Q))\n\nNow we will have Sarsa learn a policy by taking 150,000,000 steps\n\n# epsilson is prob of random action\nactor = EpsilonGreedy(1.0, 0.1, 10_000_000)\n@time learn(m, sarsa, actor; maxit=150_000_000,  should_plot=false);\n\n# plot final policy\nplot_policy(m, get_greedy(m, sarsa.Q))\n\nNotes\n\nThis is not bad, but...\n\nSarsa  under-estimates the Q-function systematically\n\nWe’ll see this by looking at “true q*” obtained via QFI\n\nopt_Q = Career.qfi(m);\n\nWe’ll compare opt_Q to Sarsa’s Q in the upper right corner of state space (where it seems to struggle most)\n\nopt_Q[:, 8:10, 8:10] - sarsa.Q.q[:, 8:10, 8:10]\n\nq^* higher everywhere\n\nPart of issue is that TD(0) is only accurate when Q is accurate and \\epsilon is zero (greedy updates)\n\nWe don’t set \\epsilon to zero, so we make some sub-optimal transitions, which introduces downward bias in Q\n\n","type":"content","url":"/rl-04-career-rl#sarsa-for-career-model","position":21},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Q-learning for career model","lvl2":"Model-free learning"},"type":"lvl3","url":"/rl-04-career-rl#q-learning-for-career-model","position":22},{"hierarchy":{"lvl1":"Reinforcement Learning -- Economic Example","lvl3":"Q-learning for career model","lvl2":"Model-free learning"},"content":"We can use the off-policy Q-learning algorithm to overcome this problem\n\nIn Q-learning, TD(0) is always computed using greedy policy derived from Q\n\nThis will remove large portion of downward bias we saw with Sarsa\n\nMy implementation of Q-learning for this model is found below# Barto & Sutton Algorithm in figure 6.7\nmutable struct QLearning <: AbstractQAlgorithm\n    Q::QFunction\n    s::State\n    α::Float64\nend\n\nfunction update!(wp::NealDSDC, actor::AbstractActor, algo::QLearning)\n    # choose a\n    a = get_action(wp, algo, actor, algo.s)[1]\n\n    # use current state and action to do transition\n    sp, r = step(wp, algo.s, a)\n\n    # ap is greedy in Q\n    ap = get_greedy(algo, sp)\n\n    # Apply TD update\n    algo.Q[algo.s, a] = (1-algo.α)*algo.Q[algo.s, a] +\n                           algo.α*(r + wp.beta * algo.Q[sp, ap])\n\n    # step forward in time\n    algo.s = sp\nend\n\nWe’ll run same experiment with Q-learning\n\n# reset random seed\nRandom.seed!(56)\nqlearning = QLearning(m, 0.01);\n\n# plot initial policy\nplot_policy(m, get_greedy(m, qlearning.Q))\n\nactor = EpsilonGreedy(1.0, 0.1, 10_000_000)\n@time learn(m, qlearning, actor; maxit=150_000_000,  should_plot=false);\n\n# plot final policy\nplot_policy(m, get_greedy(m, qlearning.Q))\n\nCheck the Q-learning Q function against q^* and see bias is smaller:\n\nopt_Q[:, 8:10, 8:10] - qlearning.Q.q[:, 8:10, 8:10]","type":"content","url":"/rl-04-career-rl#q-learning-for-career-model","position":23},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces"},"type":"lvl1","url":"/rl-05-continuous-s","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces"},"content":"Prerequisites\n\nTensorflow\n\nFunction Approximation/Interpolation\n\nReinforcement Learning Introduction, Sarsa, QLearning\n\nOutcomes\n\nUnderstand the limits of tabular RL algorithms when facing continuous state or action spaces\n\nBe able to apply linear interpolation as a function approximation tool for representing Q\n\nBe able to use tensorflow to use a neural network to approximate Q\n\nReferences\n\nBarto & Sutton book (online by authors \n\nhere) chapters 9-11\n\n","type":"content","url":"/rl-05-continuous-s","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl2":"Tabular Q – review"},"type":"lvl2","url":"/rl-05-continuous-s#tabular-q-review","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl2":"Tabular Q – review"},"content":"Previous examples (farkle, career choice) have had discrete state and action spaces\n\nCareer choice:\n\\begin{aligned}\n\\mathcal{S} &= \\{\\epsilon_i\\}_{i=1}^N \\times \\{\\theta_i\\}_{i=1}^N \\\\ \n\\mathcal{A} &= \\{\\text{ stay put}, \\text{ new job}, \\text{ new life}\\}\n\\end{aligned}\n\nFarkle:\n\\begin{aligned}\n\\mathcal{S} &= \\text{round} \\times \\text{scores} \\times \\text{can roll} \\times \\text{rolled} \\times \\text{turn sum} \\\\ \n\\mathcal{A}(s) &\\subseteq \\text{roll} \\times \\text{stop} \\times \\text{play dice}\n\\end{aligned}\n\nThis allowed us to use a lookup table for storing Q(s, a)\n\nQ was a dict, each key was (s, a) pair and each value was associated Q\n\n","type":"content","url":"/rl-05-continuous-s#tabular-q-review","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Tabular Q – limitations","lvl2":"Tabular Q – review"},"type":"lvl3","url":"/rl-05-continuous-s#tabular-q-limitations","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Tabular Q – limitations","lvl2":"Tabular Q – review"},"content":"Not all environments or RL problems have discrete state and action spaces\n\nSome have much larger state or action spaces\n\nSelf driving car: all possible images, sensor readings\n\nStock trading: sequences of prices, liquidity, funds\n\nMedical imaging: all possible medical images\n\nUsing a lookup table (dict) to represent Q in these cases is not feasible\n\nWe need a generalization strategy for Q\n\nOur response: function approximation\n\n","type":"content","url":"/rl-05-continuous-s#tabular-q-limitations","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl2":"Example: Cart-Pole"},"type":"lvl2","url":"/rl-05-continuous-s#example-cart-pole","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl2":"Example: Cart-Pole"},"content":"As a motivating example, let’s consider the Cart-Pole problem\n\nin this problem a pole is fastened to the side of a cart, loose enough for\nthe pole to freely rotate\n\nThe agent must move the cart on a straight line (1 dimension) to keep the\npole from falling below a certain angle\n\nfrom IPython.display import YouTubeVideo\n\nYouTubeVideo(\"46wjA6dqxOM\", width=800, height=600)\n\n","type":"content","url":"/rl-05-continuous-s#example-cart-pole","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"State Space","lvl2":"Example: Cart-Pole"},"type":"lvl3","url":"/rl-05-continuous-s#state-space","position":8},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"State Space","lvl2":"Example: Cart-Pole"},"content":"\\mathcal{S} \\subset \\mathbb{R}^4\n\nNum\n\nObservation\n\nMin\n\nMax\n\n0\n\nCart Position\n\n-4.8\n\n4.8\n\n1\n\nCart Velocity\n\n-Inf\n\nInf\n\n2\n\nPole Angle\n\n-0.418 rad (-24 deg)\n\n0.418 rad (24 deg)\n\n3\n\nPole Angular Velocity\n\n-Inf\n\nInf\n\nThis is a continuous space and therefore using a lookup table is infeasible (infinitely many s \\in \\mathcal{S})\n\n","type":"content","url":"/rl-05-continuous-s#state-space","position":9},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Action Space","lvl2":"Example: Cart-Pole"},"type":"lvl3","url":"/rl-05-continuous-s#action-space","position":10},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Action Space","lvl2":"Example: Cart-Pole"},"content":"\\mathcal{A} = {0, 1}:\n\nNum\n\nAction\n\n0\n\nPush cart to the left\n\n1\n\nPush cart to the right\n\nAction space is discrete\n\n","type":"content","url":"/rl-05-continuous-s#action-space","position":11},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"OpenAI Gym","lvl2":"Example: Cart-Pole"},"type":"lvl3","url":"/rl-05-continuous-s#openai-gym","position":12},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"OpenAI Gym","lvl2":"Example: Cart-Pole"},"content":"OpenAI is a ML research company from the US\n\nNow famous for NLP models such as ChatGPT\n\nA few years ago created the gym python library for RL research\n\ngym contains implementations of dozens of RL environments that adhere to a consistent API\n\nResearchers can implement RL agents once and test them on a variety of RL problems\n\nSince focusing on ChatGPT, OpenAI has stopped developing and maintaining gym\n\nA team from  the Farama Foundation has created a fork of the project and released a new python package called gymnasium. We will use this package\n\n","type":"content","url":"/rl-05-continuous-s#openai-gym","position":13},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"type":"lvl3","url":"/rl-05-continuous-s#cartpole-in-gym","position":14},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"content":"Let’s load up the gym library and see how it works\n\nWe first import gym and call gym.make with the name of our env (CartPole-v0)\n\n# if you get errors importing gym below, try to uncomment line below to install gym\n# %pip install --user gymnasium interpolation\n\nimport gymnasium as gym\nimport random\nimport pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\n\nenv = gym.make(\"CartPole-v1\")\n\n","type":"content","url":"/rl-05-continuous-s#cartpole-in-gym","position":15},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"State Space","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"type":"lvl4","url":"/rl-05-continuous-s#state-space-1","position":16},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"State Space","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"content":"We can ask gym to tell us the state space for an env\n\nenv.observation_space\n\nBox with 4 dimensions means a continus subspace of \\mathbb{R}^4\n\nWe can ask for the minimum and maximum value for each of the Box’ dimensions\n\nprint(\"min: \", env.observation_space.low)\nprint(\"max: \", env.observation_space.high)\n\n","type":"content","url":"/rl-05-continuous-s#state-space-1","position":17},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Action Space","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"type":"lvl4","url":"/rl-05-continuous-s#action-space-1","position":18},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Action Space","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"content":"We can also ask for the action space\n\nenv.action_space\n\nDiscrete spaces contain the elements {0, \\dots, N-1}\n\nIn this case we have \\mathcal{A} = \\{0, 1\\}\n\n0 in env.action_space\n\n2 in env.action_space\n\n","type":"content","url":"/rl-05-continuous-s#action-space-1","position":19},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Sampling from spaces","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"type":"lvl4","url":"/rl-05-continuous-s#sampling-from-spaces","position":20},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Sampling from spaces","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"content":"We can sample from spaces\n\nprint(\"sample s: \", env.observation_space.sample())\nprint(\"sample a: \", env.action_space.sample())\n\n","type":"content","url":"/rl-05-continuous-s#sampling-from-spaces","position":21},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"A sample episode","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"type":"lvl4","url":"/rl-05-continuous-s#a-sample-episode","position":22},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"A sample episode","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"content":"Let’s use a completely random agent to showcase how gym works\n\nSee comments in the code\n\n# restart the env. Will return s_0\ns = env.reset()\nfor t in range(100):\n    env.render()                    # update display\n    time.sleep(0.1)\n    print(s)                        # print s\n    a = env.action_space.sample()   # random sample from A(s)\n    s, r, done, _truncated, _info = env.step(a)  # step using a\n    if done:\n        print(\"Episode finished after {} timesteps\".format(t+1))\n        break\nenv.close()\n\n","type":"content","url":"/rl-05-continuous-s#a-sample-episode","position":23},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Many sample episodes","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"type":"lvl4","url":"/rl-05-continuous-s#many-sample-episodes","position":24},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Many sample episodes","lvl3":"Cartpole in Gym","lvl2":"Example: Cart-Pole"},"content":"Let’s play many sample episodes and see how the random agent does over time\n\nrandom_episode_length = []\nfor _ in range(5000):\n    env.reset()\n    for n_steps in range(200):\n        a = env.action_space.sample()\n        _, _, done, _, _ = env.step(a)\n        if done:\n            random_episode_length.append(n_steps)\n            break\n\npd.Series(random_episode_length).rolling(20).mean().plot()\n\nThis agent does not do well...\n\nThe CartPole env terminates successfully after 200 steps.\n\nThis agent doesn’t quite get 15% of the way there\n\nTo do better we need some tools for handling the continuous state space\n\n","type":"content","url":"/rl-05-continuous-s#many-sample-episodes","position":25},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl2":"Linear Function Approximation"},"type":"lvl2","url":"/rl-05-continuous-s#linear-function-approximation","position":26},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl2":"Linear Function Approximation"},"content":"Let’s begin our generalizing Q with linear function approximation\n\nLet x(s): \\mathcal{S} \\rightarrow \\mathbb{R}^D be a feature map for state s\n\nx could be identity function, polynomial features, spline basis functions, etc\n\nFor the discrete action space case we will let w_a \\in \\mathbb{R}^D represent weights such that Q(s, a | w) = w_a^Tx(s)\n\nNotice that our approximation to Q is linear in the features x(s)\n\n","type":"content","url":"/rl-05-continuous-s#linear-function-approximation","position":27},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"SGD","lvl2":"Linear Function Approximation"},"type":"lvl3","url":"/rl-05-continuous-s#sgd","position":28},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"SGD","lvl2":"Linear Function Approximation"},"content":"We would like to choose weights w_a that minimize the difference between q^*(s,a) and Q(s,a|w_a)\n\nTo do this we will use the MSE loss function between q^*(s,a) and Q(s,a|w_a)\n\nIf we apply SGD using this loss function, we want to update w as follows:\n\\begin{aligned}\nw_A &= w_A - \\frac{1}{2} \\alpha \\nabla_w \\big[q^*(S,A) - Q(S, A | w) \\big]^2 \\\\\n   &= w_A + \\alpha \\big[q^*(S, A) - Q(S, A | w) \\big] \\nabla_w Q(S, A | w)\n\\end{aligned}\n\nThe issue is that we don’t know q^*(S, A)...\n\n","type":"content","url":"/rl-05-continuous-s#sgd","position":29},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"TD(0) to the rescue","lvl2":"Linear Function Approximation"},"type":"lvl3","url":"/rl-05-continuous-s#td-0-to-the-rescue","position":30},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"TD(0) to the rescue","lvl2":"Linear Function Approximation"},"content":"Notice how the update rule for w_A looks very similar to the update rule for tabular Q-learning:\n\\begin{aligned}\nw_A &= w_A + \\alpha \\big[q^*(S, A) - Q(S, A | w) \\big] \\nabla_w Q(S, A | w) \\\\\nQ(S, A) &= Q(S, A) + \\alpha \\big[R + \\beta \\max_{a'} Q(S', a') - Q(S, A) \\big]\n\\end{aligned}\n\nIn Q-learning we take small steps in the direction of q^*(s, a) using the TD(0) error\n\nWe’ll extend this knowledge to the SGD for w_A and use R + \\beta \\max_{a'} Q(S', a'| w) in place of q^*(S, A):\nw_A = w_A + \\alpha \\big[R + \\beta \\max_{a'} Q(S', a'| w) - Q(S, A | w) \\big] \\nabla_w Q(S, A | w)\n\nThere are proofs that provide guarantees that if we use this update rule Q(s, a | w) will converge to q^*\n\nWe will take these proofs as given\n\n","type":"content","url":"/rl-05-continuous-s#td-0-to-the-rescue","position":31},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"SGD + TD(0) + Linear Approximator","lvl2":"Linear Function Approximation"},"type":"lvl3","url":"/rl-05-continuous-s#sgd-td-0-linear-approximator","position":32},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"SGD + TD(0) + Linear Approximator","lvl2":"Linear Function Approximation"},"content":"We now combine the concepts of SGD, TD(0) learning, and the fact that we consider forms of Q(s, a | w) = w_a^Tx(s) that are linear in features x(s)\n\nUnder our linearity assumption \\nabla_w Q(s, a | w) = x(s)\n\nThis makes our update rule for w equal to: w_A = w_A + \\alpha \\big[R + \\beta \\max_{a'} Q(S', a'| w) - Q(S, A | w) \\big] x(s)\n\nNote that the TD(0) part of the update rule is a scalar\n\nWe update our weights in the direction of our feature vector x(s), with scaled step size according to TD(0) term and learning rate\n\n","type":"content","url":"/rl-05-continuous-s#sgd-td-0-linear-approximator","position":33},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Example: Complete Polynomials","lvl2":"Linear Function Approximation"},"type":"lvl3","url":"/rl-05-continuous-s#example-complete-polynomials","position":34},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl3":"Example: Complete Polynomials","lvl2":"Linear Function Approximation"},"content":"Let’s try this procedure out using a function x(s) that computes a complete polynomial in s\n\nA complete polynomial of degree D is composed of all possible multiplicative terms of the elements s that are of total degree less than or equal to D\n\nExample: s = [a, b, c] and D = 2: x(s) = \\begin{bmatrix}1 & a & b & c & a^2 & a b & a c & b^2 & b c & c^2 \\end{bmatrix}^T\n\nDoes not include a^2 b term because that has total degree of 3 > 2\n\nfrom interpolation.complete_poly import complete_polynomial\ncomplete_polynomial([1, 2, 3], 2)\n\nWe’ll now create a class that uses complete_polynomial under the hood, and keeps track of w for us\n\nclass CompletePolyInterpolator:\n    def __init__(self, N: int, D: int=2):\n        self.D = D\n        self.w = np.random.rand(complete_polynomial(np.zeros(N), D).size)\n\n    def basis(self, s):\n        \"return basis matrix x(s)\"\n        return complete_polynomial(s, self.D)\n\n    def __call__(self, s):\n        return self.basis(s) @ self.w\n\nWe’ll wrap this in a class that stores one CompletePolyInterpolator per discrete action\n\nclass ContinuousSDiscreteAQ:\n    def __init__(self,  Ndim_s: int, Na:int, approximator = CompletePolyInterpolator):\n        self.Q = [approximator(Ndim_s) for _ in range(Na)]\n        self.Na = Na\n\n    @property\n    def w(self):\n        return np.row_stack([Qa.w for Qa in self.Q])\n\n    def __call__(self, s, a):\n        return self.Q[a](s)\n\n    def __getitem__(self, a):\n        return self.Q[a]\n\n    def basis(self, s, a):\n        return self.Q[a].basis(s)\n\n    def get_greedy(self, s, A_s):\n        vals = [self(s, a) for a in A_s]\n        max_val = max(vals)\n        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])\n\nAnd now we’ll create a Qlearning class that can leverage it\n\nclass ApproxQlearning(object):\n    def __init__(\n            self,\n            environment,\n            approximator=CompletePolyInterpolator,\n            epsilon=0.9, alpha=0.05, beta=0.5\n        ):\n        self.env = environment\n        Ns = env.observation_space.shape[0]\n\n        assert isinstance(env.action_space, gym.spaces.Discrete)\n        Na = env.action_space.n\n        self.A_s = np.arange(Na)\n        self.Q = ContinuousSDiscreteAQ(Ns, Na, approximator=approximator)\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.beta = beta\n\n        self.restart_episode()\n\n    def restart_episode(self):\n        self.s, _ = self.env.reset()\n\n    def get_greedy(self, s):\n        return self.Q.get_greedy(s, self.A_s)\n\n    def generate_A(self, s):\n        if random.random() > self.epsilon:\n            return self.env.action_space.sample()\n        return self.get_greedy(s)\n\n    def step(self):\n        s = self.s\n\n        # first generate an A\n        a = self.generate_A(s)\n\n        # take step\n        sp, r, done, _truncated, _info = self.env.step(a)\n\n        if done:\n            # game is over\n            self.s = sp\n            return done\n\n        # get greedy a' based on Q and sp\n        ap = self.get_greedy(sp)\n\n        # Do TD update\n        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n        Q[a].w = Q[a].w + α * (r + β * Q(sp, ap) - Q(s, a)) * Q[a].basis(s)\n\n        # step forward in time\n        self.s = sp\n\nWe’ll train qlearning for 5,000 steps and again plot the average number of steps per game\n\nql = ApproxQlearning(env, epsilon=0.98, beta=0.8, alpha=0.05)\n\ndef play_episode(ql, render=False):\n    ql.env.reset()\n    done = False\n    i = 0\n    while not done:\n        i += 1\n        if render:\n            ql.env.render()\n        done = ql.step()\n\n    if render:\n        env.close()\n\n    return i\n\nepisode_length = [play_episode(ql) for _ in range(5_000)]\n\npd.Series(episode_length).rolling(20).mean().plot()\n\nql.Q.w\n\n","type":"content","url":"/rl-05-continuous-s#example-complete-polynomials","position":35},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Summary","lvl3":"Example: Complete Polynomials","lvl2":"Linear Function Approximation"},"type":"lvl4","url":"/rl-05-continuous-s#summary","position":36},{"hierarchy":{"lvl1":"Reinforcement Learning – Continuous State Spaces","lvl4":"Summary","lvl3":"Example: Complete Polynomials","lvl2":"Linear Function Approximation"},"content":"Our complete polynomial approximation seems to do better than random\n\nThis is a good start, but this has its limits:\n\nOnly way to add more flexibility to our approximator is increase polynomial degree\n\nThe min/max values for the velocity variables are extreme\n\nTrying to compute quadratic or greater terms in very large values will likely cause numerical instability\n\nWe need a different paradigm for representing Q in a more flexible, but numerically stable way","type":"content","url":"/rl-05-continuous-s#summary","position":37},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks"},"type":"lvl1","url":"/rl-06-dqn-intro","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks"},"content":"Prerequisites\n\nTensorflow, keras\n\nReinforcement Learning -- Q learning with continuous state spaces\n\nOutcomes\n\nBe able to use tensorflow to use a neural network to approximate Q(s, a) for continuous spaces \\mathcal{S}\n\nReferences\n\nBarto & Sutton book (online by authors \n\nhere) chapters 9-11\n\nfrom collections import deque\nimport random\nfrom typing import List\n\nimport keras\nimport gymnasium as gym\nimport tensorflow as tf\n\nimport numpy as np\n\n","type":"content","url":"/rl-06-dqn-intro","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl2":"Review: CartPole"},"type":"lvl2","url":"/rl-06-dqn-intro#review-cartpole","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl2":"Review: CartPole"},"content":"We previously studied the cart pole problem:\n\nPole fastened to a cart, but can freely rotate\n\nPole starts vertical, but with some angular velocity\n\nGoal: move cart left and right to keep pole vertical\n\n\\mathcal{S} = \\{\\text{ cart position}, \\text{cart velocity}, \\text{ pole angle}, \\text{ pole angular velocity} \\} \\subset \\mathbb{R}^4\n\n\\mathcal{A}(s) = \\{\\text{ left, right }\\}\\; \\forall s\n\nNeed Q to generalize between observations from continuous \\mathcal{S}\n\nUsed complete polynomial to represent Q and obtained about 120/200 tiem steps (random 30/120)\n\nNeed more flexible method for approximating Q...\n\n","type":"content","url":"/rl-06-dqn-intro#review-cartpole","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl3":"Objective: DQN","lvl2":"Review: CartPole"},"type":"lvl3","url":"/rl-06-dqn-intro#objective-dqn","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl3":"Objective: DQN","lvl2":"Review: CartPole"},"content":"The objective for this lecture will be to use a MLP for approximating Q\n\nA few key concepts:\n\nWill represent Q(s): \\mathcal{S} -> \\mathbb{R}^{|\\mathcal{A}|}\n\nTo support mini-batch training (and other reasons we’ll learn about in another lecture 😉) we will use experience replay\n\nExperience replay:\n\nStore (s, a, r, s') transitions in a memory bank of fixed size\n\nAs new transitions are added, “forget” oldest transitions if memory full\n\nWhen training, sample randomly from current memory bank to form batch\n\n","type":"content","url":"/rl-06-dqn-intro#objective-dqn","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl2":"DQN"},"type":"lvl2","url":"/rl-06-dqn-intro#dqn","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl2":"DQN"},"content":"Below we implement a Deep Q Network -- or a Q learning agent that uses a deep neural network for representing Q\n\nclass DQN(object):\n    def __init__(\n            self,\n            environment,\n            optimizer=keras.optimizers.Adam(learning_rate=0.01),\n            loss=keras.losses.mean_squared_error,\n            hidden_layer_sizes: List[int] = [24, 24],\n            batch_size: int = 64,\n            memory_size: int = 5_000,\n            epsilon: float=0.9,\n            beta: float=0.5\n        ):\n        # check that environment is what we think it is\n        self.env = environment\n        self.Ns = self.env.observation_space.shape[0]\n\n        assert isinstance(self.env.action_space, gym.spaces.Discrete)\n        self.Na = self.env.action_space.n\n        self.A_s = np.arange(self.Na)\n\n        # set up Q function\n        self.Q = keras.Sequential(\n            [keras.layers.InputLayer((self.Ns,))] +\n            [keras.layers.Dense(n, activation=\"relu\") for n in hidden_layer_sizes] +\n            [keras.layers.Dense(\n                self.Na, activation=\"linear\",\n                kernel_initializer=tf.keras.initializers.RandomUniform(\n                    minval=-0.03, maxval=0.03\n                ),\n                bias_initializer=tf.keras.initializers.Constant(-0.2)\n            )]\n        )\n        self.Q.compile(loss=loss, optimizer=optimizer)\n        self.optimizer = optimizer\n        self.loss = loss\n\n        # set up memory\n        self.memory = deque(maxlen=memory_size)\n        self.batch_size = batch_size\n\n        # store hyper parameters\n        self.epsilon = epsilon\n        self.beta = beta\n\n    def get_greedy(self, s):\n        assert s.shape[0] == 1\n        Q_s = self.Q.predict(s, verbose=0)[0]\n        max_val = max(Q_s)\n        return random.choice(self.A_s[Q_s == max_val])\n\n    def remember(self, s, a, r, sp, done):\n        self.memory.append((s, a, r, sp, done))\n\n    def act(self, s):\n        if random.random() > self.epsilon:\n            return self.env.action_space.sample()\n        return self.get_greedy(s)\n\n    def learn_replay(self):\n        if len(self.memory) < self.batch_size:\n            # not enough memory yet...\n            return\n\n        # sample a batch\n        batch = random.sample(self.memory, self.batch_size)\n        sarsd = list(zip(*batch))\n\n        # reconstruct s, a, r, s' arrays\n        s = keras.ops.concatenate(sarsd[0], axis=0)\n        a = np.row_stack(sarsd[1])[:, 0]\n        r = np.row_stack(sarsd[2])[:, 0]\n        sp = np.concatenate(sarsd[3])\n\n        # compute temporal difference target using greedy policy\n        td_target = r + self.beta * tf.reduce_max(self.Q.predict(sp, verbose=0), axis=1)\n\n        # apply one hot encoding for easy application of `a` below\n        a_hot = keras.ops.one_hot(a, self.Na)\n\n        # compute the loss between current Q(s, a) and the targets\n        with tf.GradientTape() as tape:\n            Q_s = self.Q(s)\n            Q_sa = tf.reduce_sum(Q_s * a_hot, axis=1)\n            l = self.loss(td_target, Q_sa)\n\n        # backprop -- compute and then allow optimizer to apply gradients\n        grads = tape.gradient(l, self.Q.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))\n\n","type":"content","url":"/rl-06-dqn-intro#dqn","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl3":"Setup","lvl2":"DQN"},"type":"lvl3","url":"/rl-06-dqn-intro#setup","position":8},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl3":"Setup","lvl2":"DQN"},"content":"Below we set random seeds, create env, optimizer, and agent\n\ntf.random.set_seed(42)\nrandom.seed(42)\nnp.random.seed(42)\nenv = gym.make(\"CartPole-v1\")\noptimizer = keras.optimizers.Adam(learning_rate = 0.01, )\nagent = DQN(env, optimizer, epsilon=0.9, beta=0.8, batch_size=64)\n\nagent.Q.summary()\n\n","type":"content","url":"/rl-06-dqn-intro#setup","position":9},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl3":"Training DQN","lvl2":"DQN"},"type":"lvl3","url":"/rl-06-dqn-intro#training-dqn","position":10},{"hierarchy":{"lvl1":"Reinforcement Learning – Intro to Deep Q Networks","lvl3":"Training DQN","lvl2":"DQN"},"content":"Below we have a routine for training our DQN\n\nNotice that we need to make sure that the s array is [1, Ns] before handing to tensorflow\n\nTensorflow expects first dimension to be for batch size and subsequent dimensions to be for data\n\ndef train(dqn, N_episodes=100):\n    for episode in range(N_episodes):\n        s_, _ = dqn.env.reset()\n        s = s_[None, :]\n        step = 0\n\n        while True:\n            step += 1\n            a = dqn.act(s)\n            sp, r, done, _truncated, _ = env.step(a)\n            sp = sp[None, :]\n            r = r if (not done or step >= 200) else -r  # penalize learner when fails\n\n            dqn.remember(s, a, r, sp, done)\n            if done:\n                # if episode % 100 == 0:\n                print(f\"episode: {episode}, steps: {step}\")\n                break\n\n            # step forward in time\n            s = sp\n\n            # learn!\n            dqn.learn_replay()\n\ntrain(agent, 50)\n\nAfter only 50 episodes our agent is regularly achieving the full 200 steps\n\nWe chose \\epsilon=0.9, so we are forcing the agent to make random decisions 10% of the time\n\nIf we evaluate in greedy mode, we would expect to see perfect scores more often\n\nThe added flexibility and generalization power we get from the MLP (relative to complete polynomial) is sufficient to succesfully complete this task!\n\nWe’ll learn more about DQN and its recent exciting applications in another lecture","type":"content","url":"/rl-06-dqn-intro#training-dqn","position":11},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind"},"type":"lvl1","url":"/rl-07-deepmind","position":0},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind"},"content":"Prerequisites\n\nRL intro\n\nQ learning\n\nQ learning with continuous \\mathcal{S}\n\nDQN intro\n\nOutcomes\n\nUnderstand key technological breakthroughs in RL theory and application made by DeepMind team at Google\n\nReferences\n\nBarto & Sutton book (online by authors \n\nhere) chapters 9-11, 16\n\nDeepMind website\n\nDeepMind papers:\n\n“Human-level control through deep reinforcement learning” (DQN, 2013 & 2015)\n\n“Mastering the game of Go with deep neural networks and tree search” (AlphaGo, 2016)\n\n“Mastering the game of Go without human knowledge” (AlphaGo Zero, 2017)\n\n“Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm” (AlphaZero, 2017)\n\n“Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model” (MuZero, 2020)|\n\n","type":"content","url":"/rl-07-deepmind","position":1},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"DeepMind Overview"},"type":"lvl2","url":"/rl-07-deepmind#deepmind-overview","position":2},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"DeepMind Overview"},"content":"Company/Research group from London (100s of papers)\n\nHave made incredible progress on RL algorithms since 2013\n\nAcquired by Google in 2014\n\nSpecializes in Search algorithms and Deep Learning\n\nCombined them to create novel RL algorithms and solve difficult problems\n\n","type":"content","url":"/rl-07-deepmind#deepmind-overview","position":3},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"DQN and Atari (2013 & 2015)"},"type":"lvl2","url":"/rl-07-deepmind#dqn-and-atari-2013-2015","position":4},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"DQN and Atari (2013 & 2015)"},"content":"\n\n","type":"content","url":"/rl-07-deepmind#dqn-and-atari-2013-2015","position":5},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Overview","lvl2":"DQN and Atari (2013 & 2015)"},"type":"lvl3","url":"/rl-07-deepmind#overview","position":6},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Overview","lvl2":"DQN and Atari (2013 & 2015)"},"content":"Paper that put DeepMind in the spotlight\n\nConstructed RL algorithm to play Atari games at super human levels using\nonly pixels from screen\n\nOpened the door for a flood of new research and academic focus on RL, mostly using DL techniques\n\nIn coming years DeepMind would publish dozens of papers with variants of\nthe deep Q network introduced in this paper\n\n","type":"content","url":"/rl-07-deepmind#overview","position":7},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Components of Approach","lvl2":"DQN and Atari (2013 & 2015)"},"type":"lvl3","url":"/rl-07-deepmind#key-components-of-approach","position":8},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Components of Approach","lvl2":"DQN and Atari (2013 & 2015)"},"content":"s is pixels on Atari screen\n\nr is direction of change in game score (+1 if score went up, -1 if went down, 0 if no change)\n\n\\mathcal{A}: nothing, 8 directions, button, 8 directions + button \\Longrightarrow 18 choices\n\nQ: \\mathcal{S} \\rightarrow \\mathcal{R}^{|\\mathcal{A}|}: convolutional neural network\n\nQ-learning\n\nExperience replay\n\n","type":"content","url":"/rl-07-deepmind#key-components-of-approach","position":9},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Q-network","lvl2":"DQN and Atari (2013 & 2015)"},"type":"lvl3","url":"/rl-07-deepmind#q-network","position":10},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Q-network","lvl2":"DQN and Atari (2013 & 2015)"},"content":"\n\n","type":"content","url":"/rl-07-deepmind#q-network","position":11},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"DQN: Q-function Atari","lvl2":"DQN and Atari (2013 & 2015)"},"type":"lvl3","url":"/rl-07-deepmind#dqn-q-function-atari","position":12},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"DQN: Q-function Atari","lvl2":"DQN and Atari (2013 & 2015)"},"content":"\n\n","type":"content","url":"/rl-07-deepmind#dqn-q-function-atari","position":13},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Algorithm","lvl2":"DQN and Atari (2013 & 2015)"},"type":"lvl3","url":"/rl-07-deepmind#algorithm","position":14},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Algorithm","lvl2":"DQN and Atari (2013 & 2015)"},"content":"\n\n","type":"content","url":"/rl-07-deepmind#algorithm","position":15},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Comments","lvl2":"DQN and Atari (2013 & 2015)"},"type":"lvl3","url":"/rl-07-deepmind#comments","position":16},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Comments","lvl2":"DQN and Atari (2013 & 2015)"},"content":"Experience replay needed to reduce correlations and avoid falling into patterns\n\nwe used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution (see below for details).\n\nUpdating Q every C steps also helps reduce correlations that lead to local maximua\n\nwe used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.\n\n","type":"content","url":"/rl-07-deepmind#comments","position":17},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"AlphaGo (2016)"},"type":"lvl2","url":"/rl-07-deepmind#alphago-2016","position":18},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"AlphaGo (2016)"},"content":"Algorithm that plays the board game Go at super-human levels\n\nAbout Go\n\nGo originated in China over 3,000 years ago\n\nWidely considered to be amongst most difficult games for computer\n\n","type":"content","url":"/rl-07-deepmind#alphago-2016","position":19},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Why is Go difficult?","lvl2":"AlphaGo (2016)"},"type":"lvl3","url":"/rl-07-deepmind#why-is-go-difficult","position":20},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Why is Go difficult?","lvl2":"AlphaGo (2016)"},"content":"Go is a game of perfect information (no randomness)\n\nIn theory, can be solved by recursive methods like value function iteration \\Rightarrow v^*(s)\n\nWould require searching across b^d possible moves\n\nb: game’s breadth -- number of local moves per position\n\nd: game’s depth -- number or turns in game\n\nFor go b \\approx 250 and d \\approx 150\n\nb^d is a HUGE number\n\nb = 250\nd = 150\nb**d\n\nTrying to do an exhaustive search is not feasible\n\n","type":"content","url":"/rl-07-deepmind#why-is-go-difficult","position":21},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Making Search Feasible","lvl2":"AlphaGo (2016)"},"type":"lvl3","url":"/rl-07-deepmind#making-search-feasible","position":22},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Making Search Feasible","lvl2":"AlphaGo (2016)"},"content":"Two strategies for making search feasible\n\nLower depth of search by replacing v^*(s) with an approximation v(s) that predicts outcome of game, based on state s\n\nLower breadth of search by sampling from a policy p(a | s) that is a probability distribution over a, given s\n\n","type":"content","url":"/rl-07-deepmind#making-search-feasible","position":23},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"AlphaGo (2016)"},"type":"lvl3","url":"/rl-07-deepmind#key-algorithm-components","position":24},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"AlphaGo (2016)"},"content":"s: current structure of board\n\na: where to place next piece\n\nr: 0 until terminal then +1 if victory and -1 if loss\n\np_{\\sigma}(a |s):\n\nCNN based policy function\n\nInput (s) uses depth to encode piece position and meaning according to rules of game\n\nTrained as supervised learning problem based on expert human moves from large database\n\nObjective of supervised learning is to predict human action, given state\n\np_{\\pi}(a| s)\n\nShallow MLP based policy function\n\nInput (s) is large vector encoding rules and current positions\n\nLess accurate that p_{\\sigma}, but very fast to evaluate\n\nUsed when searching trees (see MCTS...)\n\nTrained same way as p_{\\sigma}\n\np_p(a | s)\n\nCNN based Q function\n\nSame input s and network structure as p_{\\sigma}\n\nWeights are initialized to final p_{\\sigma} weights\n\nTrained with self play using Q-learning\n\nv^p(s)\n\nCNN based v function\n\nPredicts outocme of game from state s, assuming policy p is follwed by both players\n\nMonte carlo tree search (MCTS)...\n\n","type":"content","url":"/rl-07-deepmind#key-algorithm-components","position":25},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"MCTS","lvl2":"AlphaGo (2016)"},"type":"lvl3","url":"/rl-07-deepmind#mcts","position":26},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"MCTS","lvl2":"AlphaGo (2016)"},"content":"\n\n","type":"content","url":"/rl-07-deepmind#mcts","position":27},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Training","lvl2":"AlphaGo (2016)"},"type":"lvl3","url":"/rl-07-deepmind#alphago-training","position":28},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Training","lvl2":"AlphaGo (2016)"},"content":"Final training of AlphaGo took incredible computational resources (thanks Google!)\n\nMonths of training time on 50 GPUs\n\nThis doesn’t count trial and error training to figure out how to set everything up\n\n","type":"content","url":"/rl-07-deepmind#alphago-training","position":29},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Results","lvl2":"AlphaGo (2016)"},"type":"lvl3","url":"/rl-07-deepmind#alphago-results","position":30},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Results","lvl2":"AlphaGo (2016)"},"content":"Results are incredible\n\nFor first time, computer can beat a human grand master\n\nFamous match against world champion Lee Sodol in 2016\n\nWon 4 of 5 games in a series against Sodol\n\nPlayed “unconventional” moves that human experts say they had never seen and taught them new concepts about Go\n\n","type":"content","url":"/rl-07-deepmind#alphago-results","position":31},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"AlphaGo Zero (2017)"},"type":"lvl2","url":"/rl-07-deepmind#alphago-zero-2017","position":32},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"AlphaGo Zero (2017)"},"content":"Building on success of AlphaGo,\n\nAlphaGo Zero vastly out performs AlphaGo entirely from self play\n\nUnlike AlphaGo, the Zero version doesn’t ever see or use database of expert human moves\n\n","type":"content","url":"/rl-07-deepmind#alphago-zero-2017","position":33},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"AlphaGo Zero (2017)"},"type":"lvl3","url":"/rl-07-deepmind#key-algorithm-components-1","position":34},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"AlphaGo Zero (2017)"},"content":"Will summarize differences from AlphaGo\n\ns raw board position and history\n\nAlphaGo used hand engineered features that encoded scoring systems and rule\n\nOnly uses one neural network (a Q network) for policy network, value network, and rollout policy\n\n","type":"content","url":"/rl-07-deepmind#key-algorithm-components-1","position":35},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Zero Training","lvl2":"AlphaGo Zero (2017)"},"type":"lvl3","url":"/rl-07-deepmind#alphago-zero-training","position":36},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Zero Training","lvl2":"AlphaGo Zero (2017)"},"content":"Vastly simpler ML setup and training procedure\n\nMuch more efficient use of\n\n","type":"content","url":"/rl-07-deepmind#alphago-zero-training","position":37},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Zero Results","lvl2":"AlphaGo Zero (2017)"},"type":"lvl3","url":"/rl-07-deepmind#alphago-zero-results","position":38},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaGo Zero Results","lvl2":"AlphaGo Zero (2017)"},"content":"Not relying on expert human moves to bootstrap training allows AlphaGo Zero to learn unique playing style\n\nDifficult for human players because many moves defy “standard play” amongst humans\n\nOver the course of millions of AlphaGo vs AlphaGo games, the system progressively learned the game of Go from scratch, accumulating thousands of years of human knowledge during a period of just a few days. AlphaGo Zero also discovered new knowledge, developing unconventional strategies and creative new moves that echoed and surpassed the novel techniques it played in the games against Lee Sedol and Ke Jie.\n\n","type":"content","url":"/rl-07-deepmind#alphago-zero-results","position":39},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"AlphaZero (2017)"},"type":"lvl2","url":"/rl-07-deepmind#alphazero-2017","position":40},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"AlphaZero (2017)"},"content":"Why stop at Go?\n\nLater in 2017, DeepMind team published a paper on AlphaZero that can play Go, Chess, and Shogi\n\nConstructed a single RL algorithm and neural network structure (modulo necessary differences in input layer) that can play all three games\n\n","type":"content","url":"/rl-07-deepmind#alphazero-2017","position":41},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"AlphaZero (2017)"},"type":"lvl3","url":"/rl-07-deepmind#key-algorithm-components-2","position":42},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"AlphaZero (2017)"},"content":"Will summarize differences from AlphaGo Zero\n\nDoes not take into account symmetry amongst s\n\nmany go board layouts are symmetric/identical, not true for other games\n\nUtilized in AlphaGo Zero to reduce size of state space\n\nSelf play opponent is always most recent agent\n\nWas “best” historical agent\n\nRemoves need to evaluate most recent against current best\n\nHyperparameters (learning rates, network sizes etc) fixed for all games\n\nAlphaGo Zero used Bayesian techniques to find optimal hyperparameters\n\n","type":"content","url":"/rl-07-deepmind#key-algorithm-components-2","position":43},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaZero Training","lvl2":"AlphaZero (2017)"},"type":"lvl3","url":"/rl-07-deepmind#alphazero-training","position":44},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"AlphaZero Training","lvl2":"AlphaZero (2017)"},"content":"\n\n","type":"content","url":"/rl-07-deepmind#alphazero-training","position":45},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"MuZero (2020)"},"type":"lvl2","url":"/rl-07-deepmind#muzero-2020","position":46},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"MuZero (2020)"},"content":"Why stop at board games?\n\nMuZero, introduced in 2020, can use the same RL algorithm to achieve super-human performance in Go, Shogi, Chess, and Atari games\n\nShows power of the RL, MCTS, and CNN tools used in previous DeepMind applications\n\n","type":"content","url":"/rl-07-deepmind#muzero-2020","position":47},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"MuZero (2020)"},"type":"lvl3","url":"/rl-07-deepmind#key-algorithm-components-3","position":48},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"Key Algorithm Components","lvl2":"MuZero (2020)"},"content":"Builds on AlphaZero, but adds a learned model\n\nThe learned model is composed of a hidden state, that is updated by the agent after each action\n\nModel is compoosed of three key elements\n\nThe value v: how good is the current position?\n\nThe policy p: which action is the best to take?\n\nThe reward r: how good was the last action?\n\n","type":"content","url":"/rl-07-deepmind#key-algorithm-components-3","position":49},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"What is a Learned Model?","lvl2":"MuZero (2020)"},"type":"lvl3","url":"/rl-07-deepmind#what-is-a-learned-model","position":50},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"What is a Learned Model?","lvl2":"MuZero (2020)"},"content":"Inputs: history obervations \\{o_t\\}_t from environment\n\nOutputs:\n\nRepresentation function: s^0 = h_\\theta(o_1, \\dots, o_t) (hidden state)\n\nPrediction function: p^k, v^k = f_{\\theta}(s^k) (policy and value)\n\nDynamics function :r^k, s^k = g_{\\theta}(s^{k-1}, a^k) (reward and hidden state)\n\n","type":"content","url":"/rl-07-deepmind#what-is-a-learned-model","position":51},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"How to use a Model?","lvl2":"MuZero (2020)"},"type":"lvl3","url":"/rl-07-deepmind#how-to-use-a-model","position":52},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"How to use a Model?","lvl2":"MuZero (2020)"},"content":"With model (h_{\\theta}, f_{\\theta}, g_{\\theta}) in hand, it can be used in RL training\n\nRoll model forward to fill out tree in MCTS: results in MCTS values \\nu and policies \\pi\n\nCan select actions derived from MCTS policy: a \\sim \\pi\n\n","type":"content","url":"/rl-07-deepmind#how-to-use-a-model","position":53},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"How to Learn a Model?","lvl2":"MuZero (2020)"},"type":"lvl3","url":"/rl-07-deepmind#how-to-learn-a-model","position":54},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"How to Learn a Model?","lvl2":"MuZero (2020)"},"content":"On each step, model is used to generate an action\n\nAction is taken and\n\n","type":"content","url":"/rl-07-deepmind#how-to-learn-a-model","position":55},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"MuZero Model Summary","lvl2":"MuZero (2020)"},"type":"lvl3","url":"/rl-07-deepmind#muzero-model-summary","position":56},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"MuZero Model Summary","lvl2":"MuZero (2020)"},"content":"\n\n","type":"content","url":"/rl-07-deepmind#muzero-model-summary","position":57},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"MuZero Outcomes","lvl2":"MuZero (2020)"},"type":"lvl3","url":"/rl-07-deepmind#muzero-outcomes","position":58},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl3":"MuZero Outcomes","lvl2":"MuZero (2020)"},"content":"The key outocme from MuZero is the ability to apply RL techniques for optimizing decision making in environment with unknown dynamics\n\nThis opens the door to applications where\n\nResearchers don’t know underlying dynamics\n\nDynamics are formed from a “messy” system that would be difficult to encode as in classical RL environments\n\nMuZero agent learns a hidden state s_t that may be very different from observation o_t\n\nIt is free to do this and will learn whatever s_t is useful for prediction value, action, reward\n\n","type":"content","url":"/rl-07-deepmind#muzero-outcomes","position":59},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"Summary: Evolution of Algorithms"},"type":"lvl2","url":"/rl-07-deepmind#summary-evolution-of-algorithms","position":60},{"hierarchy":{"lvl1":"Reinforcement Learning – DeepMind","lvl2":"Summary: Evolution of Algorithms"},"content":"","type":"content","url":"/rl-07-deepmind#summary-evolution-of-algorithms","position":61},{"hierarchy":{"lvl1":"Welcome"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Welcome"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nWelcome to week 1!\n\nThis week we will get to know eachother and get on the same page with respect to what this class is all about.\n\nWe will also dive right in to the computational infrastructure we will need by installing the Julia programming langauge and learning a bit about how to use it.\n\nEach week lecture notes will be distributed as a collection of Jupyter notebooks. The notebooks will follow a strict naming convention, where each notebook has a name such as L@@.##_XXX.ipynb where @@ is a two digit lecture number and ## is a two digit file number and XXX is one or more words describing the content of the notebook. We will work through the notebooks in the order indicated by the ##. The XXX are to provide easier access when reviewing notes after class.\n\n","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Welcome","lvl2":"About Me"},"type":"lvl2","url":"/#about-me","position":2},{"hierarchy":{"lvl1":"Welcome","lvl2":"About Me"},"content":"Spencer Lyon (\n\nspencer​.lyon@ucf​.edu)\n\nEconomics PhD from NYU (2018)\n\nLove to teach: mostly economics, data science, AI/ML – all have\nprogramming/computational element\n\nMoved to Orlando in July 2018 with wife and 5 (yes!) kids\n\nRun consulting (training/projects) business:   Valorum Data\n\nWorking on a couple startups\n\n","type":"content","url":"/#about-me","position":3},{"hierarchy":{"lvl1":"Welcome","lvl2":"About you"},"type":"lvl2","url":"/#about-you","position":4},{"hierarchy":{"lvl1":"Welcome","lvl2":"About you"},"content":"Background?\n\nProgress in program?\n\nAreas of interest? (meaningful answers here! they matter…)\n\nRumors about the course?\n\n","type":"content","url":"/#about-you","position":5},{"hierarchy":{"lvl1":"Welcome","lvl2":"About the course"},"type":"lvl2","url":"/#about-the-course","position":6},{"hierarchy":{"lvl1":"Welcome","lvl2":"About the course"},"content":"Interdisciplinary by nature\n\n“Living course”: only been taught twice, content is flexible\n\nMore ideas/topics than time!\n\nHas been altered fairly significantly since last year (but the heart/soul of the course remains the same)\n\nTheoretical concepts\n\nGraphs: less detail than network science course\n\nGame Theory: strategic interaction/competition\n\nSocial, information, financial networks\n\nApplications\n\nTwitter\n\nEconomic markets and trade\nnetwork\n\nMore...\n\n","type":"content","url":"/#about-the-course","position":7},{"hierarchy":{"lvl1":"Welcome","lvl2":"Expectations"},"type":"lvl2","url":"/#expectations","position":8},{"hierarchy":{"lvl1":"Welcome","lvl2":"Expectations"},"content":"Take the time in the first 2 weeks to learn Julia\n\nYou’ll be glad you did\n\nContent moves quick and you don’t want syntax/programming to distract you or hold you back\n\nStudy reading assignments before class\n\nComplete assignments on time -- no exceptions\n\nParticipate in in-class discussions\n\nDeliverables\n\nHomework (9 – 30%)\n\nExam (1 – 30%)\n\nProject (1 – 30%)\n\nCitizenship (throughout - 10%)\n\nFirst best: attend class in person, actively participate\n\nAcceptable: attend virtually, but keep video on and be ready to speak up when called on\n\nUnacceptable: attend virtually, but keep video off and/or don’t participate in discussions\n\n","type":"content","url":"/#expectations","position":9},{"hierarchy":{"lvl1":"Welcome","lvl2":"Tools/Resources"},"type":"lvl2","url":"/#tools-resources","position":10},{"hierarchy":{"lvl1":"Welcome","lvl2":"Tools/Resources"},"content":"Core textbooks\n\nNetworks, Crowds, and Markets by David Easley and Jon Kleinberg\n\nEconomics Networks by John Stachurski and Thomas Sargent\n\nWill be supplemented by lecture notes for more computational content\n\nJulia Programming language\n\nLecture notes AND assignments in Jupyter notebooks\n\nAll communication will happen through webcourses (Canvas)\n\nAll documents will be shared via webcourses\n\nLecture nodes me -> you\n\nAssignments me <-> you\n\nFeedback on assigments me -> you\n\nOfficial grades will be visible on canvas","type":"content","url":"/#tools-resources","position":11},{"hierarchy":{"lvl1":"Julia Setup"},"type":"lvl1","url":"/l01-02-setup","position":0},{"hierarchy":{"lvl1":"Julia Setup"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nLaptop or personal computer with internet connection\n\nOutcomes\n\nInstall Julia and IJulia locally\n\nInstall VS Code\n\nOpen lecture notes locally\n\nReferences\n\nLecture notes\n\nQuantEcon lectures\n\nSetting up Julia\n\nGit, GitHub, Version Control\n\nJulia tools and editors\n\n","type":"content","url":"/l01-02-setup","position":1},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Step 1: Install Julia"},"type":"lvl2","url":"/l01-02-setup#step-1-install-julia","position":2},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Step 1: Install Julia"},"content":"The first step is to install Julia\n\n","type":"content","url":"/l01-02-setup#step-1-install-julia","position":3},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Install Julia","lvl2":"Step 1: Install Julia"},"type":"lvl3","url":"/l01-02-setup#task-install-julia","position":4},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Install Julia","lvl2":"Step 1: Install Julia"},"content":"Download and install Julia, from \n\ndownload page , accepting all default options\n\n","type":"content","url":"/l01-02-setup#task-install-julia","position":5},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Launch Julia REPl","lvl2":"Step 1: Install Julia"},"type":"lvl3","url":"/l01-02-setup#task-launch-julia-repl","position":6},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Launch Julia REPl","lvl2":"Step 1: Install Julia"},"content":"Launch the Julia REPL (read-eval-print-loop) by clicking on the Julia icon\n\nYou will be greated with a prompt that looks like this:               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.9.2 (2023-07-05)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia>\n\nYou are now in what is called the Julia REPL (read-eval-print-loop). This is a place where you can type Julia commands and have them executed immediately.\n\nTest it out by typing rand(2, 2) and pressing enter\n\nYou should see something like this (note the numbers will be different for you):julia> rand(2, 2)\n2×2 Matrix{Float64}:\n 0.698944  0.676245\n 0.17375   0.448243\n\nCongratulations! You have successfully installed Julia and are ready to move on to the next step!\n\n","type":"content","url":"/l01-02-setup#task-launch-julia-repl","position":7},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Step 2: Install IJulia.jl"},"type":"lvl2","url":"/l01-02-setup#step-2-install-ijulia-jl","position":8},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Step 2: Install IJulia.jl"},"content":"With Julia installed we are now ready to install some Julia packages\n\nThe first package we’ll be using is called IJulia\n\nThis is a package integrates with Jupyter to allow us to run Julia code from notebooks\n\nSame Jupyter you may have used with Python\n\n","type":"content","url":"/l01-02-setup#step-2-install-ijulia-jl","position":9},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Install IJulia","lvl2":"Step 2: Install IJulia.jl"},"type":"lvl3","url":"/l01-02-setup#task-install-ijulia","position":10},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Install IJulia","lvl2":"Step 2: Install IJulia.jl"},"content":"Launch the Julia REPL (by typing julia from the Linux/OSX terminal prompt or using the start menu on Windows)\n\nAt the juila> prompt, type ]\n\nPrompt will switch to (@v1.10) pkg> \n\nOnce there type add IJulia and press enter\n\nIJulia.jl will be downloaded and installed on your machine\n\n","type":"content","url":"/l01-02-setup#task-install-ijulia","position":11},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Start IJulia","lvl2":"Step 2: Install IJulia.jl"},"type":"lvl3","url":"/l01-02-setup#task-start-ijulia","position":12},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Start IJulia","lvl2":"Step 2: Install IJulia.jl"},"content":"At the juila> prompt (press backspace to exit Pkg mode if needed), type using IJulia\n\nThen run the command: IJulia.notebook()\n\nA web browser should pop open with the IJulia.jl interface (should look similar to what I’m using)\n\n","type":"content","url":"/l01-02-setup#task-start-ijulia","position":13},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Step 3: Install Visual Studio Code"},"type":"lvl2","url":"/l01-02-setup#step-3-install-visual-studio-code","position":14},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Step 3: Install Visual Studio Code"},"content":"While we can and will use the native Julia REPL and first-party Jupyter software for our lecture notes, it is also helpful to have the ability to edit and run Julia code in a more full-featured editor\n\nFor this we will use Visual Studio Code (VSCode)\n\nVSCode is a free, open-source, cross-platform editor that has a large community of users and developers\n\nIt also has great support for Julia\n\nLearning how to use a general purpose text editor will help you in the long run\n\nYou can use it for Julia, Python, R, C, C++, etc.\n\nYou can use it for writing papers, taking notes, etc.\n\n","type":"content","url":"/l01-02-setup#step-3-install-visual-studio-code","position":15},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Download/Install VS Code","lvl2":"Step 3: Install Visual Studio Code"},"type":"lvl3","url":"/l01-02-setup#task-download-install-vs-code","position":16},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Download/Install VS Code","lvl2":"Step 3: Install Visual Studio Code"},"content":"Download and install VSCode from \n\nhere\n\nFollow all system prompts and accept default options\n\nOpen/launch VSCode\n\n","type":"content","url":"/l01-02-setup#task-download-install-vs-code","position":17},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Install Julia extension","lvl2":"Step 3: Install Visual Studio Code"},"type":"lvl3","url":"/l01-02-setup#task-install-julia-extension","position":18},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Install Julia extension","lvl2":"Step 3: Install Visual Studio Code"},"content":"VSCode has a large ecosystem of extensions that add functionality to the editor\n\nWe will install the Julia extension to add Julia support to VSCode\n\nIn VSCode, click on the “Extensions” icon in the left-hand toolbar (looks like a box with four squares in it)\n\nSearch for “Julia” and click the “Install” button on the “Julia Language Support” extension\n\n","type":"content","url":"/l01-02-setup#task-install-julia-extension","position":19},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: run Julia code in VSCode","lvl2":"Step 3: Install Visual Studio Code"},"type":"lvl3","url":"/l01-02-setup#task-run-julia-code-in-vscode","position":20},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: run Julia code in VSCode","lvl2":"Step 3: Install Visual Studio Code"},"content":"We can now run Julia code in VSCode\n\nOpen a new file in VSCode (File -> New File)\n\nType the following code into the file:println(\"Hello world!\")\n\nSave the file as hello.jl in a location you can find later\n\nPress Ctrl + Shift + P (or Cmd + Shift + P on Mac) to open the command palette\n\nType “Julia” and select “Julia: Run file in new process”\n\nYou should see the output of the code show up in a new terminal window within VS Code\n\n","type":"content","url":"/l01-02-setup#task-run-julia-code-in-vscode","position":21},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: run code with shift + Enter","lvl2":"Step 3: Install Visual Studio Code"},"type":"lvl3","url":"/l01-02-setup#task-run-code-with-shift-enter","position":22},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: run code with shift + Enter","lvl2":"Step 3: Install Visual Studio Code"},"content":"Go back to hello.jl and add the coden = 5\nP = rand(n, n)\n\nThen, put your cursor on the line that contains n = and press shift and enter at the same time\n\nThis will start a new Julia REPL within your VS Code session and execute the code for you\n\nIt will also move your cursor down to the P = line. Press shift + enter again to execute that line\n\n","type":"content","url":"/l01-02-setup#task-run-code-with-shift-enter","position":23},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Open Notebooks Locally"},"type":"lvl2","url":"/l01-02-setup#open-notebooks-locally","position":24},{"hierarchy":{"lvl1":"Julia Setup","lvl2":"Open Notebooks Locally"},"content":"You should view/follow along with lectures on the your computer\n\nBeing able to run code on your machine is critical for success in this course and will allow you to take skills with you after the course\n\n","type":"content","url":"/l01-02-setup#open-notebooks-locally","position":25},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Download notebooks","lvl2":"Open Notebooks Locally"},"type":"lvl3","url":"/l01-02-setup#task-download-notebooks","position":26},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Download notebooks","lvl2":"Open Notebooks Locally"},"content":"Go to Canvas and download the notebooks for the first week\n\nMove the downloaded folder from your “Downloads” folder to wherever you’d like to store materials for the semester\n\nFollow the instructions above to start Julia, load IJulia, and open the Jupyter notebook with IJulia\n\nInside your local Jupyter notebook instance (in your web browser), navigate to where you copied the “Handouts” folder and open up this notebook\n\n","type":"content","url":"/l01-02-setup#task-download-notebooks","position":27},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Open notebook in vscode","lvl2":"Open Notebooks Locally"},"type":"lvl3","url":"/l01-02-setup#task-open-notebook-in-vscode","position":28},{"hierarchy":{"lvl1":"Julia Setup","lvl3":"Task: Open notebook in vscode","lvl2":"Open Notebooks Locally"},"content":"You can also run Jupyter notebooks directly in VS Code\n\nOpen VS Code and click on the “File” menu and select “Open Folder”\n\nNavigate to where you copied the “Handouts” folder and open it\n\nClick on the “Handouts” folder in the left-hand toolbar\n\nClick on the “L02_02_setup.ipynb” file in the left-hand toolbar\n\nYou should see the notebook open in VS Code\n\nYou can run cells by clicking the “Run Cell” button in the top right of each cell, or by pressing Shift + Enter while your cursor is in the cell\n\nA menu will appear, choose the option containing Julia Release\n\nTest this out below\n\n# some random Julia code\nprintln(\"That's all, folks!\")","type":"content","url":"/l01-02-setup#task-open-notebook-in-vscode","position":29},{"hierarchy":{"lvl1":"Julia Foundations"},"type":"lvl1","url":"/l01-03-julia-basics","position":0},{"hierarchy":{"lvl1":"Julia Foundations"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nLaptop or personal computer with internet connection\n\nOutcomes\n\nUnderstand the main benefits and features of Julia\n\nSee how to define variables, functions, and types in Julia\n\nInstall commonly used packages for Graphs, DataFrames, Plotting and more\n\nReferences\n\nPackages and Software Engineering sections of QuantEcon julia lectures\n\nJulia \n\ndocumentation\n\nDocumentation for packages: \n\nGraphs, \n\nDataFrames, \n\nPlots\n\n","type":"content","url":"/l01-03-julia-basics","position":1},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"What is Julia?"},"type":"lvl2","url":"/l01-03-julia-basics#what-is-julia","position":2},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"What is Julia?"},"content":"Julia is a relatively new programming language (first public release in 2012, 1.0 release in 2018)\n\nGeneral purpose, but specializes in numerical computation\n\nLeverages advanced compiler technology to generate very efficient code\n\nIt can be as clear to read and write as Python, and as quick to evaluate as C!\n\nfunction dumb_loop(n)\n    out = 0\n    for i in 1:n\n        out += 1\n    end\n    return out\nend\n\nusing BenchmarkTools\n\n@btime dumb_loop(100_000_000_0000000)\n\n@code_llvm dumb_loop(2100000)\n\n\n\n","type":"content","url":"/l01-03-julia-basics#what-is-julia","position":3},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Core Types"},"type":"lvl2","url":"/l01-03-julia-basics#core-types","position":4},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Core Types"},"content":"We’ll start by learning about the core datatypes built in to Julia\n\nAlong the way we’ll pick up some of the key syntax elements\n\nWe will move quickly, so some prior programming experience would be helpful\n\n","type":"content","url":"/l01-03-julia-basics#core-types","position":5},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Numbers","lvl2":"Core Types"},"type":"lvl3","url":"/l01-03-julia-basics#numbers","position":6},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Numbers","lvl2":"Core Types"},"content":"Let’s start with numbers\n\nTo work with a number, just type it!\n\n42\n\nWe can also do basic arithmetic in the way you would expect\n\n10 * 3\n\n1 + 2\n\nSo far we’ve worked with integers (whole numbers)\n\nJulia can also work with numbers containing a decimal\n\nIn Julia these are called floating point numbers\n\n1.234 ^ 2.2\n\n553.34 / 12.9\n\nWe can mix and match integers and floats\n\n25 / 2.5\n\n25 / 2  # dividing integers returns a float (notice the `.`)\n\nnotice we used # to define a comment\n\n","type":"content","url":"/l01-03-julia-basics#numbers","position":7},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Text Data"},"type":"lvl2","url":"/l01-03-julia-basics#text-data","position":8},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Text Data"},"content":"Not all data is numerical\n\nSome is textual\n\nTo represent text in Julia we use a String\n\nTo define a String we use quotation marks (\") as below\n\n\"My name is Spencer\"\n\n\"1\"  # an integer in a string\n\nYou cannot use single quotes for strings as in other languages (like Python or Javascript)\n\nGo ahead... try it by removing the # and excuting the cell below\n\n# 'hello'\n\n\"\"\"\nThis\n\nis\n\nalso\n\na\n\nstring\n\"\"\"\n\n","type":"content","url":"/l01-03-julia-basics#text-data","position":9},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Arrays"},"type":"lvl2","url":"/l01-03-julia-basics#arrays","position":10},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Arrays"},"content":"When doing numerical work, we often need to deal with multiple pieces of data at the same time\n\nIn Julia the default way of doing this is to use an array\n\nArrays are defined with [ and ] as below\n\n[1, 2, 3.14]  # a 3 element array\n\n[1 2 3]  # a 1x3 matrix\n\n[1 2; 3 4]  # a 2x2 matrix\n\n[1 2\n 3 4]  # another way to write a 2x2 matrix\n\n[1 \"hello\"; 2 \"world\"]  # a 2x2 matrix with int and string\n\n","type":"content","url":"/l01-03-julia-basics#arrays","position":11},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Accessing array items","lvl2":"Arrays"},"type":"lvl3","url":"/l01-03-julia-basics#accessing-array-items","position":12},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Accessing array items","lvl2":"Arrays"},"content":"We can use [N] to access the Nth element\n\nWe can also use [i:j] to access items i through j\n\nFinally we can use [[n1, n2]] to access the n1th and n2th elements\n\n[100, 101, 102, 103][2]\n\n[100, 101, 102, 103][2:4]\n\n[100, 101, 102, 103][[1, 3]]\n\nNote that unlike Python, Julia starts counting at 1\n\n","type":"content","url":"/l01-03-julia-basics#accessing-array-items","position":13},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Tuples"},"type":"lvl2","url":"/l01-03-julia-basics#tuples","position":14},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Tuples"},"content":"There is another data type for holding “lists” of data called a tuple\n\nTuples are create using parenthesis instead of square brackets as follows\n\n(1, 2, 3, \"hello\")\n\n(\"hello\", 5)\n\n(\"hello\", 5)[2]\n\nThe main differences between tuples and arrays are\n\nTuples are meant to hold immutable or non-changing data\n\nTuples aren’t usually meant for computation or linear algebra\n\n","type":"content","url":"/l01-03-julia-basics#tuples","position":15},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Dictionary"},"type":"lvl2","url":"/l01-03-julia-basics#dictionary","position":16},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Dictionary"},"content":"Very often in programming we want to be able to associate a key or name to a specific value\n\nOne data type for doing that is a Dict\n\nDicts are created with the somewhat inconvenient syntax Dict(name => value, ...) where the ... means we can repeat the pattern multiple times\n\nThey keys and values can be of any type\n\nDict(\"x\" => 1, 2 => \"y\", [\"w\", \"z\"] => [1, 2, 3])\n\n# use `[name]` to acces element with `name`\nDict(\"x\" => 1, 2 => \"y\", [\"w\", \"z\"] => [1, 2, 3])[2]\n\nDict(\"x\" => 1, \"y\" =>2)\n\nDictionaries are often used for passing around groups of parameters\n\nWe’ll see examples later on\n\n","type":"content","url":"/l01-03-julia-basics#dictionary","position":17},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Named Tuples"},"type":"lvl2","url":"/l01-03-julia-basics#named-tuples","position":18},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Named Tuples"},"content":"The final “collection” we’ll talk about is the named tuple\n\nIt is a hybrid between a tuple and a dictionary\n\nTo create them we use the synax (name = value, ...)\n\nThey names or keys need to be just names (not numbers or arrays). The values can be anything\n\n(x = 1, y = 2, z=\"hello\")\n\n(x = 1, y = 2, z=\"hello\").z # use `.name` to access item\n\nNamed tuples are a newer feature of Julia\n\nThey are often used for the same purpsoes as dictionaries because the syntax is much cleaner\n\n","type":"content","url":"/l01-03-julia-basics#named-tuples","position":19},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Variables"},"type":"lvl2","url":"/l01-03-julia-basics#variables","position":20},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Variables"},"content":"Often when programming, we need to refer to the same piece of data more than once\n\nTo do this we use a variable\n\nVariables are defined using an =, as in name = value\n\nx = 1\n\ny = 42\n\nx + y  # 'use' or 'refer to' x and y\n\nm1 = [1 0; 0 1]\n\nm2 = [1 2; 3 4]\n\nm1 * m2  # matrix multiplication\n\nm2 * m2  # again -- but with something besides identity matrix!\n\nd = Dict(\"X\" => 1, \"Y\" => 2)\n\nd[\"X\"]\n\n","type":"content","url":"/l01-03-julia-basics#variables","position":21},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Functions"},"type":"lvl2","url":"/l01-03-julia-basics#functions","position":22},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Functions"},"content":"Most Julia programs do more than basic arithmetic operations on data\n\nTo apply an operation to a piece of data, we call a function\n\nTo call a function we use the function_name(data1, data2)\n\nA very handy function is the typeof function\n\ntypeof(1)\n\ntypeof(2.0)\n\ntypeof([1,2,3])\n\ntypeof([1 2; 3 4.0])\n\nMany standard operations are built in to Julia as functions\n\nsum([1, 2, 3])  # compute sum of array of numbers\n\ninv([1 2; 3 4])  # matrix inverse\n\nsize([1 2; 3 4])  # number of (rows, columns)  in matrix\n\nlength([1, 2, 3])  # number of elements in array\n\nlength([1 2; 3 4])  # returns total number of elements in a Matrix\n\nrand(2, 2, 2)  # a 2x2x2 array of random numbers, sampled from uniform[0,1] dist\n\nJulia has 1000s of functions\n\nWe’ll learn more as we go along...\n\nJust watch for the pattern with parentisis: name(args)\n\n","type":"content","url":"/l01-03-julia-basics#functions","position":23},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Defining Functions","lvl2":"Functions"},"type":"lvl3","url":"/l01-03-julia-basics#defining-functions","position":24},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Defining Functions","lvl2":"Functions"},"content":"Functions are used to execute a predefined set of operations\n\nDefining our own funcitons allows us to break programs into small, easily written an understood components\n\nWe define functions using the syntaxfunction name(arg1, arg2)\n    # steps\nend\n\nfunction mean(x)\n    total = sum(x)\n    N = length(x)\n    total / N\nend\n\nmean([1, 2, 3])\n\n# mean of 1000 random samples from U[0,1] -- should be ~ 0.5\nmean(rand(1000))\n\nIf a function only contains one line of code, you can also use a shorthand notation:function_name(arg1, arg2) = # step\n\nadd_two(x) = x + 2\n\nadd_two(40)\n\n","type":"content","url":"/l01-03-julia-basics#defining-functions","position":25},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Getting help for functions"},"type":"lvl2","url":"/l01-03-julia-basics#getting-help-for-functions","position":26},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Getting help for functions"},"content":"Given that there are so many functions, sometimes it is hard to remember exactly what a function does\n\nThankfully we can get help from Julia\n\nIf we type ?function_name, Julia will present us with documentation about the function\n\n?map\n\n?extrema\n\n","type":"content","url":"/l01-03-julia-basics#getting-help-for-functions","position":27},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Control Flow"},"type":"lvl2","url":"/l01-03-julia-basics#control-flow","position":28},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Control Flow"},"content":"Julia has the basic elements of control flow:\n\nif-else statements\n\nfor loops\n\nif 1 > 2 # no parenthesis needed\n    println(\"what???\")\nelse     # else is optional\n    return mean([1, 2, 3])\n    print(\"phew\")\nend      # all \"blocks\" terminate with word `end`\n\nfor i in 1:5 # range of numbers 1 to 5\n    println(i, \" \", i^2)\nend\n\nWe will see many more examples as we go forward\n\n","type":"content","url":"/l01-03-julia-basics#control-flow","position":29},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Packages"},"type":"lvl2","url":"/l01-03-julia-basics#packages","position":30},{"hierarchy":{"lvl1":"Julia Foundations","lvl2":"Packages"},"content":"Julia comes ready to go with many powerful functions and data types\n\nHowever, there is a very active community of Julia programmers who are experts in different subfields of science and engineering\n\nThis has led to the development of vibrant and exciting ecosystem of packages or toolboxes for performing specific tasks\n\nWe can access these routines by using Julia packages\n\n","type":"content","url":"/l01-03-julia-basics#packages","position":31},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Loading packages","lvl2":"Packages"},"type":"lvl3","url":"/l01-03-julia-basics#loading-packages","position":32},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Loading packages","lvl2":"Packages"},"content":"By default Julia ships with a “standard library”\n\nThese are packages that come bundled with Julia itself and are pre-installed\n\nTo load a package and all of its types/functions use the using keyword\n\nFor example, we can load the Dates package and start using it\n\nusing Dates\n\nt1 = Dates.now()\n\nDates.format(t1, \"yyyy-mm-dd\")\n\nt2 = Dates.now()\n\nt2 > t1\n\nt3 = DateTime(1776, 7, 4)\n\n\"America is $(t1 - t3) ($(floor(t1 - t3, Dates.Day))) old\"\n\n","type":"content","url":"/l01-03-julia-basics#loading-packages","position":33},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Installing Packages","lvl2":"Packages"},"type":"lvl3","url":"/l01-03-julia-basics#installing-packages","position":34},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Installing Packages","lvl2":"Packages"},"content":"In addition to the standard library, we can also use packages created by other Julia users\n\nTo use a 3rd party package, we first need to install it\n\nThere are two ways to do this\n\n1]add PackageName\n\n2using Pkg  # a standard library package\nPkg.add(\"PackageName\")\n\nLet’s try them both\n\n]add Plots\n\nusing Pkg\nPkg.add(\"DataFrames\")\n\nAfter installing packages, we can load and use them just as we did the standard library packages\n\nusing Plots  # Python: from Plots import *\n\nplot([sin, cos], -2pi, 2pi)\n\nusing DataFrames\ndf = DataFrame(c1=1:10, c2=(1:10).^2)\n\n","type":"content","url":"/l01-03-julia-basics#installing-packages","position":35},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Package Composability","lvl2":"Packages"},"type":"lvl3","url":"/l01-03-julia-basics#package-composability","position":36},{"hierarchy":{"lvl1":"Julia Foundations","lvl3":"Package Composability","lvl2":"Packages"},"content":"One unique feature sof Julia is that most of the language itself, in addition to packages, are written in Julia\n\nFor other languages like Python or R the “built in” part of the language is often written in another language like C\n\nThis difference has a large impact for Julia users\n\nBuilt in code and user code (including packages) are given the same “treatment”\n\nAnything the language creators can do, so can you\n\nA practical implication of this is that packages can operate on built in types (like we saw in our examples above) as well as types from other packages\n\nLet’s see what this looks like by plotting a DataFrame\n\n# install \"StatsPlots\", which links Plots and DataFrames\nPkg.add(\"StatsPlots\")\n\nusing StatsPlots\n\n@df df scatter(:c1, :c2)\n\nPkg.add(\"RDatasets\") # common datasets from R programming language\nusing RDatasets\nschool = RDatasets.dataset(\"mlmRev\",\"Hsb82\")\n@df school density(:MAch, group = :Sx)\n\n@df school density(:MAch, group = (:Sx, :Sector))","type":"content","url":"/l01-03-julia-basics#package-composability","position":37},{"hierarchy":{"lvl1":"Julia Types and Methods"},"type":"lvl1","url":"/l02-01-julia-types-methods","position":0},{"hierarchy":{"lvl1":"Julia Types and Methods"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nLaptop or personal computer with internet connection\n\nJulia intro lecture\n\nOutcomes\n\nUnderstand key components of Julia’s type system: abstract types, primitive types, composite types, and parametric types\n\nBe able to define our own custom types to hold data\n\nUnderstand the concept of multiple dispatch\n\nBe able to leverage the mulitple dispatch system to define custom behavior for built-in and custom types\n\nReferences\n\nLecture notes\n\nJulia documentation on \n\ntypes and \n\nmethods (these are technical, but comprehensive and well-written)\n\nQuantEcon lectures on \n\ntypes and generic programming\n\n","type":"content","url":"/l02-01-julia-types-methods","position":1},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl2":"Types in Julia"},"type":"lvl2","url":"/l02-01-julia-types-methods#types-in-julia","position":2},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl2":"Types in Julia"},"content":"Julia is both very expressive and runtime efficient\n\nThis is made possible because of the underlying compiler technology\n\nThe main strategy for user interaction with the compiler is by defining custom types and methods that operate on those types\n\nTypes and multiple dispatch go hand in hand and are key to effective Julia\n\n","type":"content","url":"/l02-01-julia-types-methods#types-in-julia","position":3},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"What is a type?","lvl2":"Types in Julia"},"type":"lvl3","url":"/l02-01-julia-types-methods#what-is-a-type","position":4},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"What is a type?","lvl2":"Types in Julia"},"content":"Each piece of data in a program resides in memory (RAM) on the host computer\n\nWe often assign names to data, which we call variables (in x = \"hello\", x is a variable)\n\nAt its most basic level, a variable is composed of\n\nAn arrangment of 0’s and 1’s called bits\n\nAn address to where in memory the data is recorded\n\nA Symbol representing the name we gave the data\n\nA type in Julia represents what kind of object is represented at a certain memory address\n\nJulia uses this type information to enable syntax (e.g. the $ in a string to interpolate or the . access for an objects fields) and ultimiately decide what behaviors are defined to operate on the data\n\n","type":"content","url":"/l02-01-julia-types-methods#what-is-a-type","position":5},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Organizing types","lvl2":"Types in Julia"},"type":"lvl3","url":"/l02-01-julia-types-methods#organizing-types","position":6},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Organizing types","lvl2":"Types in Julia"},"content":"In Julia types are organized into a hierarchy\n\nAt the top of the hierarcy is Any -- all objects are instances of Any\n\nAt the bottom of the hierarchy is Union{} -- no objects are instances of Union{}\n\nIn between these endpoints we have a rich family of types\n\nEach type can have at most one parent type (if not specified, default parent is Any)\n\nTypes can actually come in a few different flavors...\n\n","type":"content","url":"/l02-01-julia-types-methods#organizing-types","position":7},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Types of Types","lvl2":"Types in Julia"},"type":"lvl3","url":"/l02-01-julia-types-methods#types-of-types","position":8},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Types of Types","lvl2":"Types in Julia"},"content":"Abstract Types: cannot be created directly, but serve as nodes in a type hierarchy. Help us organize types into families and provide shared behavior for all members of the family\n\nPrimitive types: provided to us by Julia and represent a collection of bits (e.g. Float64, Bool, and Int8). We could create them, but we won’t. We won’t say anything else about them here\n\nComposite Types: types that contain additional data called fields. An instance can be treated as a single value. This is what we typically define and use\n\nNOTE: all objects in Julia are instances of either primitive or composite types, and can be related to one another by sharing common abstract type ancestors\n\n","type":"content","url":"/l02-01-julia-types-methods#types-of-types","position":9},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl2":"Abstract Types"},"type":"lvl2","url":"/l02-01-julia-types-methods#abstract-types","position":10},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl2":"Abstract Types"},"content":"Abstract types help organize composite types into families\n\nFor example, the number system in Julia looks like this (really -- look \n\nhere)abstract type Number end\nabstract type Real     <: Number end\nabstract type AbstractFloat <: Real end\nabstract type Integer  <: Real end\nabstract type Signed   <: Integer end\nabstract type Unsigned <: Integer end\n\nNote:\n\nNumber’s parent type is Any\n\nReal is a special kind of Number and can be broken into two subgroups: AbstractFloat and Integer\n\n","type":"content","url":"/l02-01-julia-types-methods#abstract-types","position":11},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Why Abstract Types?","lvl2":"Abstract Types"},"type":"lvl3","url":"/l02-01-julia-types-methods#why-abstract-types","position":12},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Why Abstract Types?","lvl2":"Abstract Types"},"content":"We said before we can’t create an instance of abstract types...\n\nSo, why do we have them?\n\nThe primary reason to have abstract types is to introduced shared functionality via methods defined on the abstract type\n\nExample: suppose you needed to define a function isint to determine if an object is an integer\n\nWithout abstract types, you could have a long sequence of checks for if a variable is any integer type:\n\nfunction isint1(x)\n\tfor T in [\n\t\tInt8, UInt8, Int16, UInt16, \n\t\tInt32, UInt32, Int64, UInt64, \n\t\tInt128, UInt128\n\t]\n\t\tif isa(x, T)\n\t\t\treturn true\n\t\tend\n\tend\n\treturn false\nend\n\nisint1(10), isint1(\"Hello\")\n\nWith abstract types we can define two methods:\n\nisint(x) = false\nisint(x::Integer) = true\n\nisint(10), isint(\"Hello\")\n\nThis has many benefits\n\nMuch simpler to write/reason about\n\nMore “fool proof”: what if we forgot one of the “UIntXX” types?\n\nMore “future proof”: what if a new type of integer gets introduced (e.g. UInt256 like is widely used in blockchain data!)\n\nPushes work into the compiler:\n\n@code_lowered isint1(\"hello\")\n\n@code_lowered isint1(UInt128(12341234123423134))\n\n@code_lowered isint(\"hello\") \n\n@code_lowered isint(UInt128(12341234123423134))\n\n","type":"content","url":"/l02-01-julia-types-methods#why-abstract-types","position":13},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl2":"Composite Types"},"type":"lvl2","url":"/l02-01-julia-types-methods#composite-types","position":14},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl2":"Composite Types"},"content":"Abstract types are very useful when used in conjunction with multiple dispatch (defining multiple methods of function with same name, but varying code depending on argument types)\n\nHowever, most often we create types to hold collections of related data together\n\nWe do this using composite types\n\nA composite type can be created as follows:struct Name <: AbstractParentType\n    field1::Field1Type\n\t# more fields\nend\n\nNote that the <: AbstractParentType is optional, as are types on all fields\n\n","type":"content","url":"/l02-01-julia-types-methods#composite-types","position":15},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Composite Types: Examples","lvl2":"Composite Types"},"type":"lvl3","url":"/l02-01-julia-types-methods#composite-types-examples","position":16},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Composite Types: Examples","lvl2":"Composite Types"},"content":"\n\nstruct Foo\n   bar\n   baz::Int\n   qux::Float64\nend\n\nfoo = Foo(\"Hello, world.\", 23, 1.5)\n\ntypeof(foo)\n\n# this will not work. Uncomment and try it out\n# Foo((), 23.5, 1)\n\nfieldnames(Foo)\n\nfoo.bar\n\nfoo.baz\n\nfoo.qux\n\n","type":"content","url":"/l02-01-julia-types-methods#composite-types-examples","position":17},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Composite Types and Dispatch","lvl2":"Composite Types"},"type":"lvl3","url":"/l02-01-julia-types-methods#composite-types-and-dispatch","position":18},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Composite Types and Dispatch","lvl2":"Composite Types"},"content":"Above we saw an example of defining multiple methods of isint, using an abstract type to route dispatch\n\nWe can also use composite types\n\nisint(x::Foo) = isint(x.bar)\n\nisint(10), isint(1.0), isint(UInt128(234901324987213)), isint(Foo(\"not an int\", 12, 1.0))\n\n@code_lowered isint(Foo(\"not an int\", 12, 1.0))\n\n@code_lowered isint(UInt128(12341234123423134))\n\nisint(foo)\n\nfoo, isint(foo)\n\nisint(Foo(1, 23, 1.5))\n\n","type":"content","url":"/l02-01-julia-types-methods#composite-types-and-dispatch","position":19},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Exercises","lvl2":"Composite Types"},"type":"lvl3","url":"/l02-01-julia-types-methods#exercises","position":20},{"hierarchy":{"lvl1":"Julia Types and Methods","lvl3":"Exercises","lvl2":"Composite Types"},"content":"Create an abstract type called Person\n\nCreate two composite subtypes of Person called Friend and Foe\n\nEach of these should have fields name and height_inches\n\nFor friend you should also have a field favorite_color\n\nMAKE SURE TO ADD TYPES FOR ALL FIELDS\n\nCreate a third composite subtype of Person called Stranger, but without any fields\n\nSuppose we are trying to decide who to invite to a dinner party. Our rule is that friends should get a definite yes. Enemies a definite no. Strangers a 50%/50% toss up. However, if our spouse says we should invite a person, the answer is always yes\n\nCreate a function should_invite_to_party that implements that logic\n\nHINT: you will need 4 methods. 3 of these have only one argument, the 4th has two\n\nIn the cell at the bottom we have written a test case. You will know you’ve done this correctly when all the tests pass\n\n# Your code here\n\nusing Test\n\nfunction tests()\n\t@testset \"people\" begin\n\t\n\t\t@test fieldnames(Friend) == (:name, :height_inches, :favorite_color)\n\t\t@test fieldnames(Foe) == (:name, :height_inches)\n\t\t@test fieldnames(Stranger) == tuple()\n\n\t\tjim = Friend(\"Jim\", 64, \"blue\")\n\t\tdwight = Foe(\"Dwight\", 61)\n\t\tcreed = Stranger()\n\n\t\t@test jim isa Person\n\t\t@test dwight isa Person\n\t\t@test creed isa Person\n\n\t\t@test should_invite_to_party(jim) \n\t\t@test !should_invite_to_party(dwight)\n\t\t@test should_invite_to_party(dwight, true)\n\t\t\n\t\tcreed_invites = map(i->should_invite_to_party(creed), 1:100)\n\t\t@test any(creed_invites)\n\t\t@test any(map(!, creed_invites))\n\n\t\tcreed_invites_spouse = map(i->should_invite_to_party(creed, true), 1:100)\n\t\t@test all(creed_invites_spouse)\t\n\tend\nend\n\n# uncomment and run this cell when you are ready to test your code\ntests()","type":"content","url":"/l02-01-julia-types-methods#exercises","position":21},{"hierarchy":{"lvl1":"Graphs"},"type":"lvl1","url":"/l03-01-graphs","position":0},{"hierarchy":{"lvl1":"Graphs"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nJulia setup\n\nJulia basics\n\nJulia types and methods\n\nOutcomes\n\nUnderstand key components of networks/graphs\n\nUse the Graphs.jl package for working with graphs in Julia\n\nImplement the breadth-first search algorithm\n\nReferences\n\nEasley and Kleinberg chapter 2\n\n","type":"content","url":"/l03-01-graphs","position":1},{"hierarchy":{"lvl1":"Graphs","lvl2":"Introduction"},"type":"lvl2","url":"/l03-01-graphs#introduction","position":2},{"hierarchy":{"lvl1":"Graphs","lvl2":"Introduction"},"content":"\n\n","type":"content","url":"/l03-01-graphs#introduction","position":3},{"hierarchy":{"lvl1":"Graphs","lvl3":"Why Study Graphs?","lvl2":"Introduction"},"type":"lvl3","url":"/l03-01-graphs#why-study-graphs","position":4},{"hierarchy":{"lvl1":"Graphs","lvl3":"Why Study Graphs?","lvl2":"Introduction"},"content":"Economic, cultural, political, and social interactions are influenced by structure of relationships\n\nTransmission of viruses\n\nInternational trade, supply chains, marketplaces\n\nSpread of information, diffusion of innovation\n\nPolitical persuasion, localized voting patterns\n\nHuman behaviors influenced by network of friends (sports, clothes, music)\n\nBehaviors can be effected by social networks\n\n“Influencers”\n\nCircles of followers can create echo chambers\n\n","type":"content","url":"/l03-01-graphs#why-study-graphs","position":5},{"hierarchy":{"lvl1":"Graphs","lvl3":"Edges and Nodes","lvl2":"Introduction"},"type":"lvl3","url":"/l03-01-graphs#edges-and-nodes","position":6},{"hierarchy":{"lvl1":"Graphs","lvl3":"Edges and Nodes","lvl2":"Introduction"},"content":"A graph specifies relationships between a collection of items\n\nEach item is called a node\n\nA relationship between nodes is represented by an edge\n\nVisually, graphs might look like this:\n\nHere the nodes are A, B, C, D\n\nThe edges connect nodes A-B, B-C, B-D, C-D\n\n","type":"content","url":"/l03-01-graphs#edges-and-nodes","position":7},{"hierarchy":{"lvl1":"Graphs","lvl3":"Adjacency Matrix","lvl2":"Introduction"},"type":"lvl3","url":"/l03-01-graphs#adjacency-matrix","position":8},{"hierarchy":{"lvl1":"Graphs","lvl3":"Adjacency Matrix","lvl2":"Introduction"},"content":"How might we represent the graph above numerically?\n\nOne very common approach is to use a matrix of 0’s and 1’s called an adjancency matrix\n\nSuppose we have a graph of N nodes\n\nWithout loss of generality, we’ll represent them as integers 1:N\n\nLet A \\in \\{0,1\\}^{N \\times N} be our adjacency matrix\n\nElement A_{ij} will be zero unless there is an edge between nodes i and j (diagonal is left as 0)\n\nIn our above we had\n\nNodes A, B, C, D (or 1, 2, 3, 4 respectively)\n\nEdges connecting nodes 1-2, 2-3, 2-4, 3-4\n\nThe adjacency matrix for this example is\nA = \\begin{bmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 1 \\\\\n0 & 1 & 0 & 1 \\\\\n0 & 1 & 1 & 0\n\\end{bmatrix}\n\n","type":"content","url":"/l03-01-graphs#adjacency-matrix","position":9},{"hierarchy":{"lvl1":"Graphs","lvl2":"Graphs in Julia"},"type":"lvl2","url":"/l03-01-graphs#graphs-in-julia","position":10},{"hierarchy":{"lvl1":"Graphs","lvl2":"Graphs in Julia"},"content":"In Julia there are a few ways we could represent our example graph above\n\nWe could start with the adjacency matrix concept as follows\n\nA = [\n\t0 1 0 0\n\t1 0 1 1\n\t0 1 0 1\n\t0 1 1 0\n]\n\n","type":"content","url":"/l03-01-graphs#graphs-in-julia","position":11},{"hierarchy":{"lvl1":"Graphs","lvl3":"Working with Adjacency Matrices","lvl2":"Graphs in Julia"},"type":"lvl3","url":"/l03-01-graphs#working-with-adjacency-matrices","position":12},{"hierarchy":{"lvl1":"Graphs","lvl3":"Working with Adjacency Matrices","lvl2":"Graphs in Julia"},"content":"An adjacency matrix gives us a lot of information about the structure of the graph\n\nWe could compute all of the following\n\nTotal number of nodes: number of rows or columns of A\n\nTotal number of edges: \\sum_{ij} A_{ij}\n\nNode with most edges: \\text{argmax}_{i} \\sum_{j} A_{i,j}\n\nAverage number of edges per node: N \\cdot \\sum_{i,j} A_{i,j}\n\n","type":"content","url":"/l03-01-graphs#working-with-adjacency-matrices","position":13},{"hierarchy":{"lvl1":"Graphs","lvl3":"Exercise: Adjacency Matrix","lvl2":"Graphs in Julia"},"type":"lvl3","url":"/l03-01-graphs#exercise-adjacency-matrix","position":14},{"hierarchy":{"lvl1":"Graphs","lvl3":"Exercise: Adjacency Matrix","lvl2":"Graphs in Julia"},"content":"In the cell below we have defined an adjacency matrix called A_ex1\n\nUsing A_ex1 answer the following questions:\n\nHow many nodes are in the graph?\n\nHow many edges?\n\nNode with most edges (hint, use the dims argument to sum and then the argmax function)\n\nAverage number of edges per node\n\nNumber of connections for node 7: \\sum_j A_{j7}1\n\nA_ex1 = rand(0:1, 30, 30)\n\n# remove diagonal elements\nfor i in 1:30\n    A_ex1[i, i] = 0\nend\n\n# Your code here\n\nex1_total_nodes = missing\nex1_total_edges = missing\nex1_node_most_edges = missing\nex1_average_edges_per_node = missing\nex1_connections_node_7 = missing\n\n","type":"content","url":"/l03-01-graphs#exercise-adjacency-matrix","position":15},{"hierarchy":{"lvl1":"Graphs","lvl3":"Graphs.jl","lvl2":"Graphs in Julia"},"type":"lvl3","url":"/l03-01-graphs#graphs-jl","position":16},{"hierarchy":{"lvl1":"Graphs","lvl3":"Graphs.jl","lvl2":"Graphs in Julia"},"content":"There are many smart graph theory experts in the Juila community\n\nThey have built a package called Graphs for working with graphs (as well as ancillary pacakges for extra features)\n\nWe can build a Graphs.Graph object directly from our adjacency matrix\n\nusing Graphs\n\nG1 = Graph(A)\n\ncollect(edges(G1))  # collect turns an `iterator` into an array\n\ncollect(vertices(G1))  # Graphs refers to nodes as `vertices`\n\n","type":"content","url":"/l03-01-graphs#graphs-jl","position":17},{"hierarchy":{"lvl1":"Graphs","lvl4":"Visualizing Graphs","lvl3":"Graphs.jl","lvl2":"Graphs in Julia"},"type":"lvl4","url":"/l03-01-graphs#visualizing-graphs","position":18},{"hierarchy":{"lvl1":"Graphs","lvl4":"Visualizing Graphs","lvl3":"Graphs.jl","lvl2":"Graphs in Julia"},"content":"We can use the GraphPlot package to visualize our graph\n\nNote that the actual placement of the nodes is randomly generated and then tweaked to clearly show all nodes and edges\n\nThe important thing is not the placement of nodes, but rather their relative structure\n\nusing GraphPlot  # load GraphPlot package\n\ngplot(G1)\n\n","type":"content","url":"/l03-01-graphs#visualizing-graphs","position":19},{"hierarchy":{"lvl1":"Graphs","lvl3":"Size considerations","lvl2":"Graphs in Julia"},"type":"lvl3","url":"/l03-01-graphs#size-considerations","position":20},{"hierarchy":{"lvl1":"Graphs","lvl3":"Size considerations","lvl2":"Graphs in Julia"},"content":"Using Array{Int64,2} to store an adjacency matrix turns out to be a rather costly way to store a graph\n\nIn the original example graph we had 4 nodes and 4 edges\n\nTo store this we needed to have a 4x4 matrix of 64 bit integers\n\nThis is only (Int(16 * 64 / 8) == 128) bytes in our exapmle,\n\nBut consider a graph of websites and links between them -- that graph would have millions of nodes and edges...\n\nThere are a few approaches to reducing this storage cost:\n\nOnly store the upper triangle of the matrix\n\nUse Array{Bool,2} instead of Array{Int64,2} to store adjacency matrix ( each element only sizeof(Bool) == 1 bit!)\n\nUse a \n\nSparseMatrix\n\nStore as a Vector{Vector{Int}}\n\n# Vector{Vector{Int}}\nA2 = [[2], [1, 3, 4], [2, 4], [2, 3]]\n\nG1.fadjlist\n\n","type":"content","url":"/l03-01-graphs#size-considerations","position":21},{"hierarchy":{"lvl1":"Graphs","lvl2":"Graph Theory Concepts"},"type":"lvl2","url":"/l03-01-graphs#graph-theory-concepts","position":22},{"hierarchy":{"lvl1":"Graphs","lvl2":"Graph Theory Concepts"},"content":"Let’s explore some concepts often used in analysis of graphs\n\n","type":"content","url":"/l03-01-graphs#graph-theory-concepts","position":23},{"hierarchy":{"lvl1":"Graphs","lvl3":"Paths","lvl2":"Graph Theory Concepts"},"type":"lvl3","url":"/l03-01-graphs#paths","position":24},{"hierarchy":{"lvl1":"Graphs","lvl3":"Paths","lvl2":"Graph Theory Concepts"},"content":"When studying graphs it is often natural to ask about how things travel or flow across the graph\n\nFor example, how information spreads amongst a group of friends, how data travels the internet, how diseases are transmitted from one person to another, and how people navigate a metro subway system\n\nIn each of these cases, the flow of things goes from node to node across edges\n\nA flow from one any node to another node is called a path\n\n","type":"content","url":"/l03-01-graphs#paths","position":25},{"hierarchy":{"lvl1":"Graphs","lvl3":"Arpanet Example","lvl2":"Graph Theory Concepts"},"type":"lvl3","url":"/l03-01-graphs#arpanet-example","position":26},{"hierarchy":{"lvl1":"Graphs","lvl3":"Arpanet Example","lvl2":"Graph Theory Concepts"},"content":"Consider the following Graph of the first iteration of the internet\n\nThere are many possible paths through this network\n\nConsider a path from UCSB to MIT: UCSB-UCLA-RAND-BBN-MIT\n\nAnother possible path from UCSB to MIT is UCSB-SRI-UTAH-MIT\n\n","type":"content","url":"/l03-01-graphs#arpanet-example","position":27},{"hierarchy":{"lvl1":"Graphs","lvl3":"Graphs.jl Arpanet","lvl2":"Graph Theory Concepts"},"type":"lvl3","url":"/l03-01-graphs#graphs-jl-arpanet","position":28},{"hierarchy":{"lvl1":"Graphs","lvl3":"Graphs.jl Arpanet","lvl2":"Graph Theory Concepts"},"content":"Let’s define the Arpanet using Graphs as it will be helpful throughout this lecture\n\nnodes = [\n\t\t\"UCSB\" => [\"SRI\", \"UCLA\"],\n\t\t\"SRI\" => [\"UCSB\", \"UCLA\", \"STAN\", \"UTAH\"],\n\t\t\"UCLA\" => [\"SRI\", \"UCSB\", \"STAN\", \"RAND\"],\n\t\t\"STAN\" => [\"SRI\", \"UCLA\"],\n\t\t\"UTAH\" => [\"SRI\", \"SDC\", \"MIT\"],\n\t\t\"SDC\" => [\"UTAH\", \"RAND\"],\n\t\t\"RAND\" => [\"UCLA\", \"SDC\", \"BBN\"],\n\t\t\"MIT\" => [\"UTAH\", \"BBN\", \"LINC\"],\n\t\t\"BBN\" => [\"MIT\", \"RAND\", \"HARV\"],\n\t\t\"LINC\" => [\"MIT\", \"CASE\"],\n\t\t\"CASE\" => [\"LINC\", \"CARN\"],\n\t\t\"CARN\" => [\"CASE\", \"HARV\"],\n\t\t\"HARV\" => [\"CARN\", \"BBN\"]\n\t]\nnode_ints = Dict(zip(first.(nodes), 1:length(nodes)))\narpa = SimpleGraph(length(nodes))\nfor (node, edges) in nodes\n    for e in edges\n        add_edge!(arpa, node_ints[node], node_ints[e])\n    end\nend\n\n# save graph for loading in future\nsavegraph(\"arpanet.lg\", arpa)\n\narpa\n\ngplot(arpa, nodelabel=first.(nodes))\n\n","type":"content","url":"/l03-01-graphs#graphs-jl-arpanet","position":29},{"hierarchy":{"lvl1":"Graphs","lvl3":"Cycles","lvl2":"Graph Theory Concepts"},"type":"lvl3","url":"/l03-01-graphs#cycles","position":30},{"hierarchy":{"lvl1":"Graphs","lvl3":"Cycles","lvl2":"Graph Theory Concepts"},"content":"An important concept when analyzing graphs is the concept of a cycle\n\nA cycle is a path that starts and ends at the same node\n\nFor the ARPA net, an example cycle is LINC-CASE-CARN-HARV-BBN-MIT-LINC\n\nQuestion... what is the shortest possible cycle in a graph (including all endpoints)?\n\nGraphs can tell us if a graph is connected\n\nis_connected(arpa)\n\nIt is natural to believe that many real-world networks are connected\n\nTransportation: you can get to any station\n\nInternet: you can visit any website\n\nBut it is entirely possible to have a non-connected graph\n\nSocial networks (nodes: people, edges: friendships) of college students who different countries\n\nSuppliers for a textile company vs a microchip manufacturer\n\n","type":"content","url":"/l03-01-graphs#cycles","position":31},{"hierarchy":{"lvl1":"Graphs","lvl3":"Distance","lvl2":"Graph Theory Concepts"},"type":"lvl3","url":"/l03-01-graphs#distance","position":32},{"hierarchy":{"lvl1":"Graphs","lvl3":"Distance","lvl2":"Graph Theory Concepts"},"content":"We can extend concept of paths between nodes, to include a notion of distance\n\nThe length of a path is the number of steps it takes from beginning to end\n\nMIT-BBN-RAND-UCLA has length 3 (starting from MIT take three steps before ending at UCLA)\n\nThe distance between two nodes, is the length of the shortest path between those nodes\n\nGraphs can compute distances using the gdistances function\n\nBelow we compute the distance between UCLA and all nodes\n\nDict(zip(first.(nodes), gdistances(arpa, node_ints[\"UCLA\"])))\n\n","type":"content","url":"/l03-01-graphs#distance","position":33},{"hierarchy":{"lvl1":"Graphs","lvl2":"Breadth-First Search"},"type":"lvl2","url":"/l03-01-graphs#breadth-first-search","position":34},{"hierarchy":{"lvl1":"Graphs","lvl2":"Breadth-First Search"},"content":"If asked, how would you go about computing the distance between the HARV node and all other nodes?\n\nOne iterative approach might be:\n\nStart with HARV: note it is distance zero to HARV\n\nMove on to all nodes directly connected to HARV: these are distance 1\n\nThen move to all nodes connected to nodes that are distance 1 from HARV (excluding any you may have already found): declare these to be at distance 2 from HARV\n\nContinue traversing edges until you have visited all nodes\n\nThis algorihtm is called breadth-first search\n\n","type":"content","url":"/l03-01-graphs#breadth-first-search","position":35},{"hierarchy":{"lvl1":"Graphs","lvl3":"Example: Breadth-First Search from MIT","lvl2":"Breadth-First Search"},"type":"lvl3","url":"/l03-01-graphs#example-breadth-first-search-from-mit","position":36},{"hierarchy":{"lvl1":"Graphs","lvl3":"Example: Breadth-First Search from MIT","lvl2":"Breadth-First Search"},"content":"The image below shows how breadth-first search would proceed for the MIT node\n\n","type":"content","url":"/l03-01-graphs#example-breadth-first-search-from-mit","position":37},{"hierarchy":{"lvl1":"Graphs","lvl3":"Exercise (difficult!): BFS","lvl2":"Breadth-First Search"},"type":"lvl3","url":"/l03-01-graphs#exercise-difficult-bfs","position":38},{"hierarchy":{"lvl1":"Graphs","lvl3":"Exercise (difficult!): BFS","lvl2":"Breadth-First Search"},"content":"Now it is time for you to try this out!\n\nOur goal is to use breadth-first search to compute the distance betwen a given node and all other nodes\n\nThe return value you end up with should be an Vector{Vector{Int}}, where element i of this vector contains all node labels at distance i from the starting node\n\nFill in the logic for the breadth_first_distances function below\n\nfunction breadth_first_distances(g, start::Int)\n\tout = Vector{Int}[]\n\t# use push!(out, new_nodes) to add to out\n\tdistance = 0\n\n\t# TODO: your code here...\n\n\t# return out\n\tout\nend\n\n# Test code\n\nfunction test_bfd_methods(val, want)\n    if length(val) == 0\n        error(\"Make sure to `push!` on to `out` in your function\")\n    elseif length(val) != maximum(gdistances(arpa, node_ints[\"HARV\"]))\n        error(\"`out` has incorrect number of elements\")\n    elseif length.(val) != length.(want)\n        error(\"Right number of elements, but not right number in each subvector\")\n    elseif all(map(x12 -> all(sort(x12[1]) .== sort(x12[2])), zip(val, want)))\n        println(\"correct!\")\n    end\nend\n\nfunction run_tests()\n    val = breadth_first_distances(arpa, node_ints[\"HARV\"])\n    want = [[9, 12], [7, 8, 11], [3, 6, 5, 10], [1, 2, 4]]\n    test_bfd_methods(val, want)\nend\n\n# uncomment the code below and run when you are ready to test your code\n# run_tests()\n\n","type":"content","url":"/l03-01-graphs#exercise-difficult-bfs","position":39},{"hierarchy":{"lvl1":"Graphs","lvl3":"BFS with Graphs","lvl2":"Breadth-First Search"},"type":"lvl3","url":"/l03-01-graphs#bfs-with-graphs","position":40},{"hierarchy":{"lvl1":"Graphs","lvl3":"BFS with Graphs","lvl2":"Breadth-First Search"},"content":"The Graphs library contains routines implementing breadth-first search\n\nThe main function is called bfs_tree\n\nbfs_carn = bfs_tree(arpa, node_ints[\"CARN\"])\n\nNotice that the printout says we have a graph with 13 nodes, 12 edges and it is a directed graph\n\nThus far, all graphs we have considered have been undirected\n\nWe have only been concerned about if a connection (edge) exists between nodes\n\nA directed graph extends the notion of connecting nodes with a direction\n\nWe can now say that things flow across edges from one node to another -- always in the same direction\n\nWhy would the breadth-first search routine return a directed graph instead of the undirected type we started with?\n\nLet’s visualize it and see if we can understand why\n\ngplot(bfs_carn, nodelabel=first.(nodes))\n\nNotice that arrows only flow out of CARN\n\nThey also always flow away from CARN\n\nThe use of directed edges allows Graphs to represent the shortest path from CARN to any other node\n\nFor example STAN: CARN -> HARV -> BBN -> RAND -> UCLA -> STAN\n\n","type":"content","url":"/l03-01-graphs#bfs-with-graphs","position":41},{"hierarchy":{"lvl1":"Graphs","lvl3":"Exercise: Explore DiGraph","lvl2":"Breadth-First Search"},"type":"lvl3","url":"/l03-01-graphs#exercise-explore-digraph","position":42},{"hierarchy":{"lvl1":"Graphs","lvl3":"Exercise: Explore DiGraph","lvl2":"Breadth-First Search"},"content":"The bfs_carn object has type $(Markdown.Code(string(typeof(bfs_carn))))\n\nLet’s view the names of its properties (properties)\n\npropertynames(bfs_carn)\n\nbfs_carn.fadjlist\n\nThe fadjlist (forward adjacency list) property is a Vector{Vector{Int64}}\n\nfadjlist has one element per node (call index into outer Vector i for node i)\n\nEach element is itself a vector containing node indices for all nodes j for which there is an edge flowing from i to j\n\nBelow we have set up a new method (see below) for the breadth_first_distances function that takes a DiGraph as an argument\n\nYour task is to implement the the method so that it has the same return value as the previous method from above\n\nfunction breadth_first_distances(g::SimpleDiGraph, start::Int)\n\tout = Vector{Int}[]\n\t# use push!(out, new_nodes) to add to out\n\tdistance = 0\n\n\t# TODO: your code here...\n\n\t# return out\n\tout\nend\n\n# test code\nfunction test_digraph_ex()\n\tval = breadth_first_distances(\n\t\tbfs_tree(arpa, node_ints[\"HARV\"]),\n\t\tnode_ints[\"HARV\"]\n\t)\n\twant = [[9, 12], [7, 8, 11], [3, 6, 5, 10], [1, 2, 4]]\n\ttest_bfd_methods(val, want)\nend\n\n# uncomment the code below when you are ready to test your code!\n# test_digraph_ex()\n\n","type":"content","url":"/l03-01-graphs#exercise-explore-digraph","position":43},{"hierarchy":{"lvl1":"Graphs","lvl2":"Components"},"type":"lvl2","url":"/l03-01-graphs#components","position":44},{"hierarchy":{"lvl1":"Graphs","lvl2":"Components"},"content":"A component of a graph is a self-contained subset of the nodes\n\nMore precisely, a set of nodes is a component if\n\nEvery node in the subset has a path to every other node in the subset\n\nThe subset is not part of a larger set with property (1)\n\nExample:\n\n","type":"content","url":"/l03-01-graphs#components","position":45},{"hierarchy":{"lvl1":"Graphs","lvl2":"Example"},"type":"lvl2","url":"/l03-01-graphs#example","position":46},{"hierarchy":{"lvl1":"Graphs","lvl2":"Example"},"content":"How many components are in this graph?","type":"content","url":"/l03-01-graphs#example","position":47},{"hierarchy":{"lvl1":"Strong and Weak Ties"},"type":"lvl1","url":"/l03-02-strong-weak-ties","position":0},{"hierarchy":{"lvl1":"Strong and Weak Ties"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nIntroduction to Graphs\n\nOutcomes\n\nRecognize open and closed triangles in a graph\n\nUnderstand concept of tradic closure\n\nBe able to identify global and local bridges in a network: visually and programatically\n\nUnderstand the “strength of weak ties”\n\nBe familiar with betwenness centrality and its use in graph partitioning\n\nReferences\n\nEasley and Kleinberg chapter 3\n\n","type":"content","url":"/l03-02-strong-weak-ties","position":1},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Granovetter’s question"},"type":"lvl2","url":"/l03-02-strong-weak-ties#granovetters-question","position":2},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Granovetter’s question"},"content":"In the late 1960s a PhD student Mark Granovetter wanted to understand how people get a new job\n\nHe interviewed many people who recently changed firms and asked how they got their job\n\nUnsurprisingly, people said they got referrals or learned about the job from personal contacts\n\nSurprisingly, most people said the contacts were made via their “aquaintances” and not “friends”\n\nWhy?\n\n","type":"content","url":"/l03-02-strong-weak-ties#granovetters-question","position":3},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Graphs for Information Flow","lvl2":"Granovetter’s question"},"type":"lvl3","url":"/l03-02-strong-weak-ties#graphs-for-information-flow","position":4},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Graphs for Information Flow","lvl2":"Granovetter’s question"},"content":"To answer Granovetter’s question, we’ll turn to graphs\n\nAs we’ll see, this is one of many possible examples of how we can use a graph to study the flow of information\n\nWhat other examples are there?\n\nFrom what sources do you get your information?\n\nDo you get different types of information from different types of source? Why?\n\nOne interesting point to make in this regard is that by studying information flow via graphs we are making taking something inherently interpersonal or emotional (frienships and sharing information) and analyzing it from a structural perspective (as a graph)\n\nKeep these themes in mind as we study the theoreitcal/techinical tools\n\nWe’ll return back to the question Granovetter’s question after building up some tools\n\n","type":"content","url":"/l03-02-strong-weak-ties#graphs-for-information-flow","position":5},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Triangles"},"type":"lvl2","url":"/l03-02-strong-weak-ties#triangles","position":6},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Triangles"},"content":"When studying graphs, the smallest possible structure involving more than two nodes is a triangle\n\nTriangles are made up of three nodes and either two or three edges that connect them\n\nA triangle with only two edges is said to be open\n\nA triangle with three edges is closed\n\nIn the example below, there are 3 triangles, all of which are open\n\nusing Graphs, GraphPlot\n\ng1 = star_graph(4)\nadd_vertex!(g1)\ngplot(g1, nodelabel='A':'E', layout=shell_layout)\n\n","type":"content","url":"/l03-02-strong-weak-ties#triangles","position":7},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Social Triangles","lvl2":"Triangles"},"type":"lvl3","url":"/l03-02-strong-weak-ties#social-triangles","position":8},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Social Triangles","lvl2":"Triangles"},"content":"Suppose our graph is of students arriving for first year of college\n\nEach node is a student\n\nAn edge represents a friendship or connection between students\n\nStory...\n\nA went to and out of school state, but happened to know B, C, and D from various summer camps or family-friend relationships\n\nNone of B, C and D know one another\n\nQuestion: Given only this information, is it more likely that B and E become friends, or B and C? Why?\n\n","type":"content","url":"/l03-02-strong-weak-ties#social-triangles","position":9},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Closing triangles","lvl2":"Triangles"},"type":"lvl3","url":"/l03-02-strong-weak-ties#closing-triangles","position":10},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Closing triangles","lvl2":"Triangles"},"content":"In our example graph the only edges are between A and another node\n\nConsider a triangle formed by nodes A, B, C\n\nTo close this triangle, there would need to be a node between B-C as follows\n\ng2 = copy(g1)\nadd_edge!(g2, 2, 3)\ngplot(g2, nodelabel='A':'E', layout=shell_layout)\n\n","type":"content","url":"/l03-02-strong-weak-ties#closing-triangles","position":11},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Triadic Closure","lvl2":"Triangles"},"type":"lvl3","url":"/l03-02-strong-weak-ties#triadic-closure","position":12},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Triadic Closure","lvl2":"Triangles"},"content":"There is overwhelming empirical evidence to support the intuition that when there are edges A-B and A-C, it is likely that an edge B-C will form\n\nThis is known as triadic closure (closing the final edge of a triangle)\n\nFor this reason, when analyzing social network data it is very common to see triangles\n\n","type":"content","url":"/l03-02-strong-weak-ties#triadic-closure","position":13},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Triangles with Graphs.jl","lvl2":"Triangles"},"type":"lvl3","url":"/l03-02-strong-weak-ties#triangles-with-graphs-jl","position":14},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Triangles with Graphs.jl","lvl2":"Triangles"},"content":"There is good support in Graphs.jl for helping us count triangles in a graph\n\nThe two key functions are triangles(g) and local_clustering(g)\n\ntriangles(g) will return an array of integers, where values correspond to how many closed triangels there are in the graph\n\n@show triangles(g1)\n@show triangles(g2);\n\nThe local_clustering(g) function will return a tuple of two things:\n\nThe number of closed triangles for each node (same as triangles(g))\n\nThe number of possible triangles for each node in g\n\n@show local_clustering(g1)\n@show local_clustering(g2);\n\n","type":"content","url":"/l03-02-strong-weak-ties#triangles-with-graphs-jl","position":15},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Clustering Coefficients","lvl2":"Triangles"},"type":"lvl3","url":"/l03-02-strong-weak-ties#clustering-coefficients","position":16},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Clustering Coefficients","lvl2":"Triangles"},"content":"It is helpful to have a single number summary of how “closed” a graphs triangles are\n\nThe clustering coefficient tells us the fraction of triangles that are closed\n\nThere is a local flavor, where we consider all triangles for a specific node\n\nThere is also a global flavor where we consider all triangles for the entire graph\n\nGraphs.jl has functions local_clustering_coefficient and global_clustering_coefficient to compute these quantities respectively\n\nlocal_clustering_coefficient(g2)\n\nglobal_clustering_coefficient(g2)\n\nQuestion: In each of the graphs below, how many open triangles are there? How many closed?\n\n#graph 1\ngplot(g2, nodelabel='A':'E', layout=shell_layout)\n\n#graph 2\ngplot(path_graph(5), nodelabel='A':'E', layout=shell_layout)\n\n# graph 3\ngplot(dorogovtsev_mendes(7), nodelabel=1:7)\n\n# graph 4\ngplot(smallgraph(:bull), nodelabel=1:5)\n\n","type":"content","url":"/l03-02-strong-weak-ties#clustering-coefficients","position":17},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Bridges"},"type":"lvl2","url":"/l03-02-strong-weak-ties#bridges","position":18},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Bridges"},"content":"Consider the graph below\n\ng3 = barbell_graph(4, 4)\ngplot(g3, nodelabel='A':'H', layout=spring_layout)\n\nNow consider D\n\nNotice that connection between D and any of A B C is somehow different from connection to E\n\nD-E is known as a bridge\n\nA bridge is an edge that, if removed, would cause the nodes involved to be in different components of the graph\n\ng4 = copy(g3)\nrem_edge!(g4, 4, 5)\ngplot(g4, nodelabel='A':'H', layout=spring_layout)\n\n","type":"content","url":"/l03-02-strong-weak-ties#bridges","position":19},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Frequency of bridges","lvl2":"Bridges"},"type":"lvl3","url":"/l03-02-strong-weak-ties#frequency-of-bridges","position":20},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Frequency of bridges","lvl2":"Bridges"},"content":"Given our discussion on triadic closure, bridges are likely to be rare in real social networks\n\nIt is very likely that an edge will form between E and one of A``B``C\n\nEven if that isn’t the case, consider the possibility that the graph we have been looking at is actually a smaller subset of a larger graph:\n\nusing Downloads\nif !isfile(\"local_bridge.lg\")\n    Downloads.download(\"https://ucf-cap-6318-resources.s3.amazonaws.com/data/local_bridge.lg\", \"local_bridge.lg\")\nend\ng5 = loadgraph(\"local_bridge.lg\")\ngplot(g5, nodelabel='A':'M', layout=spring_layout)\n\nLocal Bridges\n\nIn the graph above even if D-E were broken, there would only be one component in our graph\n\nIn other words there is another path from D to E (here D-I-K-M-E)\n\nBecause true bridges are so rare, a looser definition of bridge was created called a local bridge\n\nAn edge is a local bridge if A and B have no edges in common\n\n","type":"content","url":"/l03-02-strong-weak-ties#frequency-of-bridges","position":21},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Detecting Local Bridges","lvl2":"Bridges"},"type":"lvl3","url":"/l03-02-strong-weak-ties#detecting-local-bridges","position":22},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Detecting Local Bridges","lvl2":"Bridges"},"content":"So far we have dealt with example graphs that we can visually inspect\n\nMost real world graphs are far to large for this\n\nTo analyze larger graphs we need computational tools\n\nLet’s build up some code that will allow us to find local bridges\n\nExercise\n\nPopulate the num_shared_neighbors function below\n\n\"\"\"\n    num_shared_neighbors(g, n1, n2)\n\nGiven a graph `g` and node indexes `n1` and `n2`, compute\nhow many neighbors `n1` and `n2` have in common\n\"\"\"\nfunction num_shared_neighbors(g, n1, n2)\n    # HINT: `neighbors(g, n1)` will give you an array of neighbors\n    #        of `n1`\n    \n    \nend\n\n\n\n# test code here\nusing Test\n\nfunction test_num_shared_neighbors()\n    @testset \"num_shared_neighbors\" begin\n        vals = [\n            num_shared_neighbors(g5, 6, 7),\n            num_shared_neighbors(g5, 4, 5)\n        ]\n        @test !isnothing(vals[1])\n        @test vals[1] isa Integer\n        @test vals == [2, 0]\n    end\nend\n\n# uncomment and run the code below when you are ready to test your code\n# test_num_shared_neighbors()\n\nExercise\n\nNow that we have a function for computing the number of shared neighbors, we can use it to build a routine for finding a local bridge\n\nWe’ll do that now\n\nYour task is to fill in the missing logic for the local_bridges(g) function below\n\n\"\"\"\n    local_bridges(g)\n\nGiven a graph `g`, find all local bridges in the graph\n\"\"\"\nfunction local_bridges(g)\n    out = []\n    for n1 in 1:nv(g)\n\n        # TODO determine what to loop over for n2\n\n        # TODO: call num_shared_neighbors(g, n1, n2)\n\n        # TODO: if n1 and n2 have no shared neighbors, **AND** (n1 < n2) \n        #       call push!(out, (n1, n2))\n    end\n    return out\nend\n\nfunction test_local_bridges()\n    @testset \"local_bridges\" begin\n        vals = [\n            en\n            local_bridges(g5)\n            local_bridges(g4)\n        ]\n        @test vals[1] isa Array\n        @test length(vals[1]) == 1\n        @test length(vals[2]) == 0\n        @test vals[1][1] == (4, 5)\n    end\nend\n\n# uncomment and run the code below when you are ready to test your code\n# test_local_bridges()\n\n","type":"content","url":"/l03-02-strong-weak-ties#detecting-local-bridges","position":23},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Example: Twitter connections","lvl2":"Bridges"},"type":"lvl3","url":"/l03-02-strong-weak-ties#example-twitter-connections","position":24},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Example: Twitter connections","lvl2":"Bridges"},"content":"Let’s now consider an example using real social network data\n\nBelow we’ll load up a graph called tw that is a graph of connections between twitter users\n\nEach node is a different twitter account\n\nThere is an edge between nodes if either one of the accounts follows the other\n\nusing Downloads\nif !isfile(\"twitter.lg\")\n    Downloads.download(\"https://ucf-cap-6318-resources.s3.amazonaws.com/data/twitter.lg\", \"twitter.lg\")\nend\ntw = loadgraph(\"twitter.lg\")\n\nNotice that there are 81,306 nodes and 1,342,310 edges\n\nThis is network is far to big to analyze visually\n\nLet’s use a few of our empirical metrics to study the properties of this graph\n\nglobal_clustering_coefficient(tw)\n\ntw_bridges = bridges(tw)\n\nratio_bridges = length(tw_bridges) / ne(tw)\n\nNotice how only 0.372% of edges form a local bridge!\n\n","type":"content","url":"/l03-02-strong-weak-ties#example-twitter-connections","position":25},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Edge Strength"},"type":"lvl2","url":"/l03-02-strong-weak-ties#edge-strength","position":26},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Edge Strength"},"content":"We have so far considered only whether or not two nodes are connected\n\nWe have not discussed the strength of these connections\n\nWe will now extend our analysis to the notion of an edge representing a strong or a weak tie\n\nIn our friendship example, the strong ties would represent friends and the weak ties would represent acquaintances\n\n","type":"content","url":"/l03-02-strong-weak-ties#edge-strength","position":27},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Strong and Weak ties","lvl2":"Edge Strength"},"type":"lvl3","url":"/l03-02-strong-weak-ties#strong-and-weak-ties","position":28},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Strong and Weak ties","lvl2":"Edge Strength"},"content":"In the figure below, we have a representation of a graph similar to our g5 where all edges have been annotated with a S or a W\n\nA S edge represents a strong edge, or friendship\n\nA W edge represents a weak edge or, or acquaintance\n\n","type":"content","url":"/l03-02-strong-weak-ties#strong-and-weak-ties","position":29},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Triadic Closure: Strong Vs Weak","lvl2":"Edge Strength"},"type":"lvl3","url":"/l03-02-strong-weak-ties#triadic-closure-strong-vs-weak","position":30},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Triadic Closure: Strong Vs Weak","lvl2":"Edge Strength"},"content":"Let’s extend the inutition behind triadic closure to our strong/weak setting\n\nOur argument was that because we have edges A-B and A-C, it is likely that an edge B-C will form\n\nNow we’ll state that if A-B and A-C are both strong ties, then it is more likely that B-C will form than if either A-B or A-C were weak\n\nMore formally... we have  the Strong Triadic Closure Property\n\nWe say that a node A violates the Strong Triadic Closure Property if it has strong ties to two other nodes B and C, and there is no edge at all (either a strong or weak tie) between B and C. We say that a node A satisfies the Strong Triadic Closure Property if it does not violate it.\n\n","type":"content","url":"/l03-02-strong-weak-ties#triadic-closure-strong-vs-weak","position":31},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Local Bridges and Weak Ties","lvl2":"Edge Strength"},"type":"lvl3","url":"/l03-02-strong-weak-ties#local-bridges-and-weak-ties","position":32},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Local Bridges and Weak Ties","lvl2":"Edge Strength"},"content":"Given this definition of the Strong Triadic Closure Property, we can make the following claim (see section 3.2 of E&K for proof, intuition in figure below):\n\nClaim: If a node A in a network satifies the Strong Triadic Closure Property and is involved in at least two strong ties, then any local bridge it is involved in must be a weak tie.\n\n","type":"content","url":"/l03-02-strong-weak-ties#local-bridges-and-weak-ties","position":33},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Back to Granovetter","lvl2":"Edge Strength"},"type":"lvl3","url":"/l03-02-strong-weak-ties#back-to-granovetter","position":34},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Back to Granovetter","lvl2":"Edge Strength"},"content":"Recall our original question: why do people report finding jobs through aquantainces (weak ties) more often than close friends (strong ties)?\n\nThe Strong Triadic Closure Property gives the answer...\n\nSuppose A lost its job\n\nBy asking for a referral from any of C, D, or E; A is likely to get a similar set of information as they are strongly connected to one other and likely have access to the same set of information\n\nInstead by talking to the weak tie B, A is likely to get new information from people in B’s social circle (including H and the three unlabeled nodes)\n\ntl;dr: local bridges are weak ties, so it is weak ties that get you access to “new” parts of the network\n\n","type":"content","url":"/l03-02-strong-weak-ties#back-to-granovetter","position":35},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Graph Partitioning"},"type":"lvl2","url":"/l03-02-strong-weak-ties#graph-partitioning","position":36},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Graph Partitioning"},"content":"Social networks often consist of tightly knit regions and weak ties that connect them\n\nOne algorithmic problem that has is been studied and applied in many settings is that of graph partitioning\n\nTo partition a graph is to break it down into the tightly-knit components\n\nWhen a graph is partitioned, it is broken down into components called regions\n\ng6 = let \n    edges = Edge.([\n        (1,2), (1,3), (2,3), (3,7), (7,6), (6,4), (6,5), (4,5), (7,8),\n        (8,9), (9,10), (10,11), (9,11), (8,12), (12,13), (13,14), (12,14)\n    ])\n    Graph(edges) \nend\n\ngplot(g6, nodelabel=1:14, layout=spring_layout)\n\n","type":"content","url":"/l03-02-strong-weak-ties#graph-partitioning","position":37},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Two Approaches","lvl2":"Graph Partitioning"},"type":"lvl3","url":"/l03-02-strong-weak-ties#two-approaches","position":38},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Two Approaches","lvl2":"Graph Partitioning"},"content":"There are to classes of algorithms that can be used to partition a graph:\n\nDivisive: partition a graph by removing local bridges (“spanning links”)  and breaking down the network into large chunks\n\nAgglomerative: start with a single node and construct regions “bottom-up” by iteratively finding nodes highly connected to existing nodes in the region\n\nWe’ll focus on divisive methods here\n\n","type":"content","url":"/l03-02-strong-weak-ties#two-approaches","position":39},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Betweenness Centrality","lvl2":"Graph Partitioning"},"type":"lvl3","url":"/l03-02-strong-weak-ties#betweenness-centrality","position":40},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Betweenness Centrality","lvl2":"Graph Partitioning"},"content":"In order to build a divisive partitioning algorithm, we’ll first define a key metric for analyzing how “central” a node is in a network\n\nWe’ll provide a brief introduction here, and refer you to section 3.6 of E&K for more detail\n\nLet V represent set of all nodes and s, t, v \\in V\n\nLet \\sigma_{st} represent the number of shortest paths between s and t\n\nLet \\sigma_{st}(v) represent the number of shortest paths between s and t that pass through v.\n\nThen, the betweeness centrality  for node v (C_{B}(v)) is defined as:\n(C_{B}(v) = \\sum_{s \\ne v \\ne t \\in V} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}\n\nConceptually, C_B(v) captures how much information “flows” across node v on average\n\n","type":"content","url":"/l03-02-strong-weak-ties#betweenness-centrality","position":41},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Computing C_B(v)","lvl2":"Graph Partitioning"},"type":"lvl3","url":"/l03-02-strong-weak-ties#computing-c-b-v","position":42},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Computing C_B(v)","lvl2":"Graph Partitioning"},"content":"There are various algorithms we could use to compute C_B(v)\n\nFor now we will let Graphs.jl handle it for us ;)\n\nbetweenness_centrality(g6)\n\nNote that 7 and 8 are a local bridge\n\nAlso note that they carry the highest value of C_B...🤔\n\n","type":"content","url":"/l03-02-strong-weak-ties#computing-c-b-v","position":43},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Algorithm: Girvan-Newman","lvl2":"Graph Partitioning"},"type":"lvl3","url":"/l03-02-strong-weak-ties#algorithm-girvan-newman","position":44},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl3":"Algorithm: Girvan-Newman","lvl2":"Graph Partitioning"},"content":"We will not present the algorithm in detail, but will describe the overall steps\n\nRefer to Section 3.6 of E&K for details\n\nThe Girvan-Newman algorithm for graph partitioning:\n\nInputs: node\n\nOutputs: list of edges to delete at each step\n\nAlgorithm:\n\nFind nodes with highest betweenness centrality -- remove them from the network (and add edges connecting them to the network to the list of deleted edges for first step)\n\nRe-compute betweenness centrality for all subgraphs that resulted from deletion in step 1. Remove all nodes with highest betweenness centrality and record list of deleted edges\n\nContinue until all edges have been removed\n\n","type":"content","url":"/l03-02-strong-weak-ties#algorithm-girvan-newman","position":45},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Summary: Key ideas"},"type":"lvl2","url":"/l03-02-strong-weak-ties#summary-key-ideas","position":46},{"hierarchy":{"lvl1":"Strong and Weak Ties","lvl2":"Summary: Key ideas"},"content":"Triangles are key network structures\n\nClosed triangles are common in social networks due to tradic closure\n\nSocial networks are often composed of tightly knit regions bound together with weak ties\n\nWeak ties often form local bridges and are therefore valueable for referrals and information flow\n\nBetwenness centrality captures idea of how essential a node is in connecting regions of a graph (how much information flows across a node)","type":"content","url":"/l03-02-strong-weak-ties#summary-key-ideas","position":47},{"hierarchy":{"lvl1":"Homophily"},"type":"lvl1","url":"/l04-01-homophily","position":0},{"hierarchy":{"lvl1":"Homophily"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nIntroduction to Graphs\n\nStrong and Weak Ties\n\nOutcomes\n\nUnderstand the concept of homophily\n\nPractice working through “by hand” examples of diagnosing homophily\n\nBe prepared to computationally diagnose homophily in a large network\n\nReferences\n\nEasley and Kleinberg chapter 4 (especially section 4.1)\n\n","type":"content","url":"/l04-01-homophily","position":1},{"hierarchy":{"lvl1":"Homophily","lvl2":"Introduction"},"type":"lvl2","url":"/l04-01-homophily#introduction","position":2},{"hierarchy":{"lvl1":"Homophily","lvl2":"Introduction"},"content":"\n\n","type":"content","url":"/l04-01-homophily#introduction","position":3},{"hierarchy":{"lvl1":"Homophily","lvl3":"Main Idea","lvl2":"Introduction"},"type":"lvl3","url":"/l04-01-homophily#main-idea","position":4},{"hierarchy":{"lvl1":"Homophily","lvl3":"Main Idea","lvl2":"Introduction"},"content":"Consider your friends. Do they tend to\n\nEnjoy the same movies, music, hobbies as you?\n\nHold similar religous or political beliefs?\n\nCome from simliar schools, workplaces, or socio-economic settings?\n\nWhat about a random sample of people in the world?\n\nIf you are like me, your answers likely indicate that you have more in common with your friends than you would expect to have with a random sample of people\n\nThis concept -- that we are similar to our friends -- is called homophily\n\n","type":"content","url":"/l04-01-homophily#main-idea","position":5},{"hierarchy":{"lvl1":"Homophily","lvl3":"Homophily in Graphs","lvl2":"Introduction"},"type":"lvl3","url":"/l04-01-homophily#homophily-in-graphs","position":6},{"hierarchy":{"lvl1":"Homophily","lvl3":"Homophily in Graphs","lvl2":"Introduction"},"content":"In the context of graphs or networks, homophily means that nodes that are connected are more similar than nodes at a further distance in the graph\n\nBut what do we mean by more similar?\n\nIdea: We might have common friends.\n\nThis is an intrinsic force that led to node formation (e.g. triadic closure)\n\nAlternative: We may share characteristics or properties that are not represented in the graph -- external forces.\n\nExamples: same race, gender, school, employer, sports team, etc.\n\nThese external forces are what homophily captures\n\n","type":"content","url":"/l04-01-homophily#homophily-in-graphs","position":7},{"hierarchy":{"lvl1":"Homophily","lvl3":"Context","lvl2":"Introduction"},"type":"lvl3","url":"/l04-01-homophily#context","position":8},{"hierarchy":{"lvl1":"Homophily","lvl3":"Context","lvl2":"Introduction"},"content":"To identify if homophily is active in a network, we must have access to context on top of list of nodes and edges\n\nOne way to represent this context would be with a DataFrame in addition to a graph:\n\nOne row per node\n\nOne column indicating the node identifier (or just use row number)\n\nOne column for additional characteristic\n\nusing DataFrames, Graphs, GraphPlot\n\ndf1 = DataFrame(\n    node_id=[\"Spencer\", \"Shannen\", \"Brinley\", \"MJ\", \"LeBron\"],\n    favorite_color=[\"blue\", \"pink\", \"pink\", \"red\", \"blue\"],\n    school=[\"ucf\", \"byu\", \"wes\", \"unc\", \"hs\"],\n    sport=[\"volleyball\", \"wakeboarding\", \"wakeboarding\", \"basketball\", \"basketball\"],\n    favorite_food=[\"pizza\", \"burger\", \"pizza\", \"burger\", \"ice cream\"],\n    gender=['M', 'F', 'F', 'M', 'M'],\n)\n\nedges = [\n    0 1 1 0 1 \n    1 0 1 1 0 \n    1 1 0 0 0\n    0 1 0 0 1\n    1 0 0 1 0\n]\ng1 = Graph(edges)\n\ngplot(g1, nodelabel=df1.node_id)\n\n\n\n","type":"content","url":"/l04-01-homophily#context","position":9},{"hierarchy":{"lvl1":"Homophily","lvl2":"Measuring Homophily"},"type":"lvl2","url":"/l04-01-homophily#measuring-homophily","position":10},{"hierarchy":{"lvl1":"Homophily","lvl2":"Measuring Homophily"},"content":"Our discussion on homophily so far has been conceptual... let’s make it precise\n\nWe’ll frame the discussion in terms of a null hypothesis\n\nConcept should be familiar from statistics, but not exactly the same we we won’t make distributional assumptions\n\n","type":"content","url":"/l04-01-homophily#measuring-homophily","position":11},{"hierarchy":{"lvl1":"Homophily","lvl3":"Random Homophily","lvl2":"Measuring Homophily"},"type":"lvl3","url":"/l04-01-homophily#random-homophily","position":12},{"hierarchy":{"lvl1":"Homophily","lvl3":"Random Homophily","lvl2":"Measuring Homophily"},"content":"Our analytical approach begins with a thought experiment (counter factual) that all edges are randomly formed\n\nIn this case, we should not expect the context around our graph to help us predict its structure\n\nSuppose we consider a characteristic X\n\nWe have N nodes and N_x of them exhibit feature X and N - N_x of them to not\n\nWe’ll work with probabilities: p_x = \\frac{N_x}{N}\n\nThe probability that an arbitrary edge is between two nodes that both share X is equal to p_x^2\n\nProbability of edge between two non X nodes: (1-p_x)^2\n\nProbabillity of edge bewtween one X and one non X:\n\\begin{aligned}\\text{prob}(\\text{edge (X <=> not X)}) &= p_x (1-p_x) + (1-p_x) p_x \\\\ &= 2 p_x (1-p_x)\\end{aligned}\n\nThis will be our “random edge formation” benchmark\n\n","type":"content","url":"/l04-01-homophily#random-homophily","position":13},{"hierarchy":{"lvl1":"Homophily","lvl3":"Counting Frequencies","lvl2":"Measuring Homophily"},"type":"lvl3","url":"/l04-01-homophily#counting-frequencies","position":14},{"hierarchy":{"lvl1":"Homophily","lvl3":"Counting Frequencies","lvl2":"Measuring Homophily"},"content":"Now an empirical value...\n\nLet there be e edges\n\nLet...\n\nvariable\n\nmeaning\n\ne_{xx}\n\n# edges between 2 X\n\ne_{yy}\n\n# edges between 2 not X\n\ne_{xy}\n\n# edges between 1 X  and 1 not X\n\nThen e = e_{xx} + e_{yy} + e_{xy}\n\nWe’ll use these 4 numbers to count frequencies of edges between X types and non-X types\n\n","type":"content","url":"/l04-01-homophily#counting-frequencies","position":15},{"hierarchy":{"lvl1":"Homophily","lvl3":"Testing for Homophily","lvl2":"Measuring Homophily"},"type":"lvl3","url":"/l04-01-homophily#testing-for-homophily","position":16},{"hierarchy":{"lvl1":"Homophily","lvl3":"Testing for Homophily","lvl2":"Measuring Homophily"},"content":"We are now ready to test for homophily\n\nWe’ll consider the assumption (null hypothesis) that there is no homophily in characteristic X\n\n\\Longrightarrow observed proportion of cross-characteristic edges is (approximately) the same as characteristic frequencies in the full population\n\nTo test this assumption, we compare\n\n2 p_x(1-p_x): the likelihood of a cross-characteristic edge forming, under the assumption of purely random edge formation\n\n\\frac{e_{xy}}{e}: the proportion of cross-characteristic edges that exist in the network\n\nWhen comparing these statistics, we could get one of three outcomes:\n\nCondition\n\nresult\n\n\\frac{e_{xy}}{e} >> 2 p_x(1-p_x)\n\ninverse homophily\n\n\\frac{e_{xy}}{e} \\approx 2 p_x(1-p_x)\n\nno homophily\n\n\\frac{e_{xy}}{e} << 2 p_x(1-p_x)\n\nhomophily\n\nIntuition: If observed cross characteristic edge formation is significantly less than what we’d expected under random edge formation, we reject the hypothesis that homophily is not present, and conclude that characteristic X is meaningful for edge formation\n\n","type":"content","url":"/l04-01-homophily#testing-for-homophily","position":17},{"hierarchy":{"lvl1":"Homophily","lvl3":"Example: high school relationships","lvl2":"Measuring Homophily"},"type":"lvl3","url":"/l04-01-homophily#example-high-school-relationships","position":18},{"hierarchy":{"lvl1":"Homophily","lvl3":"Example: high school relationships","lvl2":"Measuring Homophily"},"content":"Recall the graph of romantic relationships between high school students\n\nQuestion: does this graph exhibit homophily in gender? Why?\n\n","type":"content","url":"/l04-01-homophily#example-high-school-relationships","position":19},{"hierarchy":{"lvl1":"Homophily","lvl2":"Example: Lyon and All Stars"},"type":"lvl2","url":"/l04-01-homophily#example-lyon-and-all-stars","position":20},{"hierarchy":{"lvl1":"Homophily","lvl2":"Example: Lyon and All Stars"},"content":"Let’s work through an example of numerically dianosing homophily using our made up data\n\nI’ll repeat it below\n\ndf1\n\ngplot(g1, nodelabel=df1.node_id)\n\n","type":"content","url":"/l04-01-homophily#example-lyon-and-all-stars","position":21},{"hierarchy":{"lvl1":"Homophily","lvl3":"Step 1: Counting frequencies","lvl2":"Example: Lyon and All Stars"},"type":"lvl3","url":"/l04-01-homophily#step-1-counting-frequencies","position":22},{"hierarchy":{"lvl1":"Homophily","lvl3":"Step 1: Counting frequencies","lvl2":"Example: Lyon and All Stars"},"content":"First we need to count frequencies for all our characteristics\n\nWe’ll do that here\n\nusing DataStructures\n\nfunction count_frequencies(vals)\n    counts = DataStructures.counter(vals)\n    total = length(vals)\n    Dict(c => v / total for (c, v) in pairs(counts))\nend\n\ncount_frequencies(df1.gender)\n\nDict(\n    n => count_frequencies(df1[!, n])\n    for n in names(df1)[2:end]\n\n)\n\n","type":"content","url":"/l04-01-homophily#step-1-counting-frequencies","position":23},{"hierarchy":{"lvl1":"Homophily","lvl3":"Step 2: Counting Edges","lvl2":"Example: Lyon and All Stars"},"type":"lvl3","url":"/l04-01-homophily#step-2-counting-edges","position":24},{"hierarchy":{"lvl1":"Homophily","lvl3":"Step 2: Counting Edges","lvl2":"Example: Lyon and All Stars"},"content":"Next we need to count the number of edges of each type\n\nThis step is a bit tricker as it will require that we access both data from the Graph and the DataFrame\n\nTo not spoil the fun, we’ll leave this code as an exercise on the homework\n\nFor now we’ll look at things “by hand”\n\nLet’s consider favorite_color and test if edges form based on favorite color being blue\n\nThere are 6 total edges (e)\n\nOf these, 5 are cross edges (e_{xy})\n\nThe ratio of cross edges is 5/6\n\nThe ratio of nodes that like blue is 2/5 (p_x)\n\nE = ne(g1)\nExy = 5\nn_blue = 2\nN = nv(g1)\npx = n_blue / N\n\n# test\n2 * px * (1-px), Exy/E\n\nHere we have that more cross edges formed than we would expect\n\nAn instance of inverse homophily (opposites attract)\n\n","type":"content","url":"/l04-01-homophily#step-2-counting-edges","position":25},{"hierarchy":{"lvl1":"Homophily","lvl3":"Exercise","lvl2":"Example: Lyon and All Stars"},"type":"lvl3","url":"/l04-01-homophily#exercise","position":26},{"hierarchy":{"lvl1":"Homophily","lvl3":"Exercise","lvl2":"Example: Lyon and All Stars"},"content":"Repeat the counting exercise, but for the gender and favorite sport characteristics\n\nWhat do you find? Do either of these characteristcs exhibit homophily?","type":"content","url":"/l04-01-homophily#exercise","position":27},{"hierarchy":{"lvl1":"Weighted Graphs"},"type":"lvl1","url":"/l04-02-weighted-graphs","position":0},{"hierarchy":{"lvl1":"Weighted Graphs"},"content":"Prerequisites\n\nIntroduction to Graphs\n\nStrong and Weak Ties\n\nOutcomes\n\nKnow what a weighted graph is and how to construct them using SimpleWeightedGraphs.jl\n\nImplement the shortest path algorithm for traversing a weighted graph\n\nReferences\n\nEasley and Kleinberg chapter 5 (especially section 5.1-5-3)\n\n# import Pkg; Pkg.add(\"SimpleWeightedGraphs\")\n\nusing Graphs, GraphPlot, SimpleWeightedGraphs\n\n","type":"content","url":"/l04-02-weighted-graphs","position":1},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Introduction"},"type":"lvl2","url":"/l04-02-weighted-graphs#introduction","position":2},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Introduction"},"content":"So far we have considered a few types of graphs\n\nUndirected graph: nodes A and B are connected by an edge\n\nDirected graph: connection from node A to node B\n\nStrong/weak graphs: each edge is labeled as strong or weak\n\nToday we extend our understanding of networks to talk about weighted graphs\n\nEach edge is assigned a float denoting the strength of tie\n\nTies can be positive (friends) or negative (enemies)\n\nCan also very in strength (+2.0 better friends than +0.2)\n\n","type":"content","url":"/l04-02-weighted-graphs#introduction","position":3},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Weighted Adjacency Matrix","lvl2":"Introduction"},"type":"lvl3","url":"/l04-02-weighted-graphs#weighted-adjacency-matrix","position":4},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Weighted Adjacency Matrix","lvl2":"Introduction"},"content":"In a simple (unweighted) graph, we used a matrix of 0’s and 1’s as an adjacency matrix\n\nA 1 in row i column j marked an edge between i and j (or from i->j for directed)\n\nA 0 marked lack of an edge\n\nG1 = complete_graph(4)\nlocs_x = [1, 2, 3, 2.0]\nlocs_y = [1.0, 0.7, 1, 0]\nlabels1 = collect('A':'Z')[1:nv(G1)]\ngplot(G1, locs_x, locs_y, nodelabel=labels1)\n\nA1 = adjacency_matrix(G1)\n\nWe can extend idea of adjacency matrix to include weighted edges\n\nSuppose nodes A, B, C are friends -- but A-C are best friends\n\nAlso suppose that all of A, B, C consider D an enemy\n\nTo represent this we might say weight of edges is:\n\nA-B and B-C: 1.0\n\nA-C: 2.0\n\nA-D, B-D, C-D: -1.0\n\nHere’s the adjacency matrix\n\nA2 = [0 1 2 -1; 1 0 1 -1; 2 1 0 -1; -1 -1 -1.0 0]\n\nAnd here is how we might visualize this graph (notice the labeled edges)\n\nG2 = SimpleWeightedGraph(A2)\ngplot(\n    G2, locs_x, locs_y,\n    nodelabel=labels1, edgelabel=weight.(edges(G2)),\n)\n\n","type":"content","url":"/l04-02-weighted-graphs#weighted-adjacency-matrix","position":5},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Shortest Paths"},"type":"lvl2","url":"/l04-02-weighted-graphs#shortest-paths","position":6},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Shortest Paths"},"content":"We talked previously about shortest paths for a Graph\n\nThis was defined as the minimum number of edges needed to move from node n1 to node n2\n\nWhen we have a weighted graph things get more interesting...\n\nLet w_{ab} represent the weight connecting nodes A and B\n\nDefine the shortest path between n1 and n2 as the path that minimizes \\sum w_{ab} for all edges A->B along a path\n\n","type":"content","url":"/l04-02-weighted-graphs#shortest-paths","position":7},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Example","lvl2":"Shortest Paths"},"type":"lvl3","url":"/l04-02-weighted-graphs#example","position":8},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Example","lvl2":"Shortest Paths"},"content":"Consider the following directed graph\n\nA3 = [\n    0 1 5 3 0 0 0\n    0 0 0 9 6 0 0\n    0 0 0 0 0 2 0\n    0 0 0 0 0 4 8\n    0 0 0 0 0 0 4\n    0 0 0 0 0 0 1\n    0 0 0 0 0 0 0\n]\nG3 = SimpleWeightedDiGraph(A3)\n\n#plotting details\nlocs_x_3 = [3, 5, 1, 3, 4, 2, 3.0]\nlocs_y_3 = [1, 2, 2, 3, 4, 4, 5.0]\nlabels3 = collect('A':'Z')[1:size(A3, 1)]\ngplot(G3, locs_x_3, locs_y_3, nodelabel=labels3, edgelabel=weight.(edges(G3)))\n\nWe wish to travel from node A to node G at minimum cost\n\nThe shortest path (ignoring weights)  is A-D-G\n\nTaking into account weights we have 3 + 8 = 11\n\nThere are two other paths that lead to lower cost (total of 8)\n\nA-C-F-G has cost 5 + 2 + 1 = 8\n\nA-D-F-G has cost 3 + 4 + 1 = 8\n\nFor this small graph, we could find these paths by hand\n\nFor a larger one, we will need an algorithm...\n\n","type":"content","url":"/l04-02-weighted-graphs#example","position":9},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Shortest path algorithm","lvl2":"Shortest Paths"},"type":"lvl3","url":"/l04-02-weighted-graphs#shortest-path-algorithm","position":10},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Shortest path algorithm","lvl2":"Shortest Paths"},"content":"Let J(v) be the minimum cost-to-go from node v to node G\n\nSuppose that we know J(v) for each node v, as shown below for our example graph\n\nNote J(G) = 0\n\n\n\nWith J(v) in hand, the following algorithm will find the cost-minimizing path from A to G:\n\nStart with v = A\n\nFrom current node v move to any node that solves \\min_{n \\in F_v} w_{vn} + J(n), where F_v is the set of nodes that can be reached from v.\n\nUpdate notation to set v = n\n\nRepeat steps 2-3 (making note of which we visit) until v = G\n\n","type":"content","url":"/l04-02-weighted-graphs#shortest-path-algorithm","position":11},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Exercise: Traversing Cost-Minimizing Path","lvl2":"Shortest Paths"},"type":"lvl3","url":"/l04-02-weighted-graphs#exercise-traversing-cost-minimizing-path","position":12},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Exercise: Traversing Cost-Minimizing Path","lvl2":"Shortest Paths"},"content":"Let’s implement the algorithm above\n\nBelow I have started a function called traverse_graph\n\nYour task is to complete it until you get that the minimum cost path has a cost of 8 and length(4)\n\nJ3 = [8, 10, 3, 5, 4, 1, 0]\n\nfunction traverse_graph(\n        G::SimpleWeightedDiGraph, \n        J::AbstractArray, \n        start_node::Int, end_node::Int\n    )\n    path = Int[start_node]\n    cost = 0.0\n    W = weights(G)\n\n    # TODO: step1, initialize v\n    v = 1  # CHANGE ME\n    num = 0\n    while v != end_node && num < nv(G)  # prevent infinite loop\n        num +=1\n        F_v = neighbors(G, v)\n\n        # TODO: step 2, compute costs for all n in F_v\n        costs = [0 for n in F_v]  # CHANGE ME\n\n        n = F_v[argmin(costs)]\n\n        # TODO: how should we update cost?\n        cost += 0   # CHANGE ME\n\n        push!(path, n)\n\n        # TODO: step 3 -- update v\n        v = v  # CHANGE ME\n    end\n    path, cost\nend\n\ntraverse_graph(G3, J3, 1, 7)\n\n","type":"content","url":"/l04-02-weighted-graphs#exercise-traversing-cost-minimizing-path","position":13},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"But what about J(v)","lvl2":"Shortest Paths"},"type":"lvl3","url":"/l04-02-weighted-graphs#but-what-about-j-v","position":14},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"But what about J(v)","lvl2":"Shortest Paths"},"content":"The shortest path algorithm we presented above sounds simple, but assumed we know J(v)\n\nHow can we find it?\n\nIf you stare at the following equation long enough, you’ll be convinced that J satisfies\nJ(v) = \\min_{n \\in F_v} w_{vn} + J(n)\n\nThis is known as the Bellman equation\n\nIt is a restriction that J must satisfy\n\nWe’ll use this restriction to compute J\n\n","type":"content","url":"/l04-02-weighted-graphs#but-what-about-j-v","position":15},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Computing J: Guess and Iterate","lvl2":"Shortest Paths"},"type":"lvl3","url":"/l04-02-weighted-graphs#computing-j-guess-and-iterate","position":16},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Computing J: Guess and Iterate","lvl2":"Shortest Paths"},"content":"We’ll present the standard algorithm for computing J(v)\n\nThis is an iterative method\n\nLet i represent the iteration we are on and J_i(v) be the guess for J(v) on iteration i\n\nAlgorithm\n\nSet i=0, and J_i(v) = 0 \\forall v\n\nSet J_{i+1}(v) = \\min_{n \\in F_v} w_{vn} + J_i(n) \\forall n\n\nCheck if J_{i+1} and J_i are equal for all v -- if not set i = i+1 and see repeat steps 2-3\n\nThis algorithm converges to J (we won’t prove it here...)\n\n","type":"content","url":"/l04-02-weighted-graphs#computing-j-guess-and-iterate","position":17},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Implementation","lvl2":"Shortest Paths"},"type":"lvl3","url":"/l04-02-weighted-graphs#implementation","position":18},{"hierarchy":{"lvl1":"Weighted Graphs","lvl3":"Implementation","lvl2":"Shortest Paths"},"content":"Let’s now implement the algorithm!\n\nWe’ll walk you through our implementation\n\ncost(W, J, n, v) = W[v, n] + J[n]\n\nfunction compute_J(G::SimpleWeightedDiGraph, dest_node::Int)\n    N = nv(G)\n    # step 1. start with zeros\n    i = 0\n    Ji = zeros(N)\n\n    next_J = zeros(N)\n\n    W = weights(G)\n\n    done = false\n    while !done\n        i += 1\n        for v in 1:N\n            if v == dest_node\n                next_J[v] = 0\n                continue\n            end\n            F_v = neighbors(G, v)\n            costs = [cost(W, Ji, n, v) for n in F_v]\n            next_J[v] = minimum(costs)\n        end\n        done = all(next_J .≈ Ji)\n        copy!(Ji, next_J)\n    end\n    Ji\nend\n\ncompute_J(G3, 7)\n\n","type":"content","url":"/l04-02-weighted-graphs#implementation","position":19},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Exercise: Shortest Path"},"type":"lvl2","url":"/l04-02-weighted-graphs#exercise-shortest-path","position":20},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Exercise: Shortest Path"},"content":"Let’s now combine the two functions to compute a shortest path (and associated cost) for a graph\n\nYour task is to fill in the function below and get the test to pass\n\n\"\"\"\nGiven a weighted graph `G`, enumerate a shortest path between `start_node` and `end_node`\n\"\"\"\nfunction shortest_path(G::SimpleWeightedDiGraph, start_node::Int, end_node::Int)\n    # your code here\nend\n\n","type":"content","url":"/l04-02-weighted-graphs#exercise-shortest-path","position":21},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Summary"},"type":"lvl2","url":"/l04-02-weighted-graphs#summary","position":22},{"hierarchy":{"lvl1":"Weighted Graphs","lvl2":"Summary"},"content":"Weighted graphs allow us to analyze the cost of travsersing paths\n\nApplied in situations like traffic flows (on physical roads/bridges), resource planning, supply chain, international trade (weights as tarrifs), and more\n\nProgramming skills...\n\nWe built up an algorithm shortest_path using two smaller routines: traverse_graph, compute_J\n\nFor each of the 3 functions we were able to write tests to verify code correctness\n\nGood habit to break a hard problem into smaller sub-problems that can be implemented/tested separately\n\nThen compose overall routine using functions for sub-problems\n\nNot all practitioners do this... we’ve seen some scary notebooks and scripts... don’t do that... you know better","type":"content","url":"/l04-02-weighted-graphs#summary","position":23},{"hierarchy":{"lvl1":"Structural Balance"},"type":"lvl1","url":"/l04-03-structural-balance","position":0},{"hierarchy":{"lvl1":"Structural Balance"},"content":"Prerequisites\n\nIntroduction to Graphs\n\nStrong and Weak Ties\n\nWeighted Graphs\n\nOutcomes\n\nUnderstand the structural balance property for sets of three nodes\n\nUnderstand the structural balance theorem for a graph\n\nRecognize structural balance in a weighted graph\n\nReferences\n\nEasley and Kleinberg chapter 5 (especially section 5.1-5-3)\n\n","type":"content","url":"/l04-03-structural-balance","position":1},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Introduction"},"type":"lvl2","url":"/l04-03-structural-balance#introduction","position":2},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Introduction"},"content":"We now shift our discussion to the notion of whether or not a network is balanced\n\nFor this discussion we will use weighted graphs, where weights are one of\n\n1: if nodes are friends (also called +)\n\n0: if they don’t know eachother\n\n-1: if nodes are enemies (also called -)\n\nWe won’t consider strength of ties right now\n\nWe will also limit discussion to complete graphs (cliques) where all nodes are connected to all other nodes\n\n","type":"content","url":"/l04-03-structural-balance#introduction","position":3},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Balance in Triangles"},"type":"lvl2","url":"/l04-03-structural-balance#balance-in-triangles","position":4},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Balance in Triangles"},"content":"To start thinking about balance, consider the possible configurations of + and - edges in a triangle\n\nThere are 4 options as shown below\n\n\n\nIn (a) all people are friends -- this is happy and balanced\n\nIn (c) A-B are friends with a common enemy C -- nobody has reason to change alliances => balanced\n\nIn (b) A is friends with B and C, but they are enemies -- B and C may try to flip A against other => not balanced\n\nIn (d) all are enemies -- two parties have incentive to team up against common enemy => not balanced\n\n","type":"content","url":"/l04-03-structural-balance#balance-in-triangles","position":5},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Balance In Graphs"},"type":"lvl2","url":"/l04-03-structural-balance#balance-in-graphs","position":6},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Balance In Graphs"},"content":"This definition of balance in triangles can be extended to graphs\n\nA complete graph G satisfies the Structural Balance Property if for every set of three nodes, exactly one or three of the edges is labeled +\n\n","type":"content","url":"/l04-03-structural-balance#balance-in-graphs","position":7},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Implications: Balance Theorem","lvl2":"Balance In Graphs"},"type":"lvl3","url":"/l04-03-structural-balance#implications-balance-theorem","position":8},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Implications: Balance Theorem","lvl2":"Balance In Graphs"},"content":"One implication of the Structural Balance Property is the Balance Theorem\n\nIf a labeled complete graph is balanced, then either all pairs of nodes are friends, or else the nodes can be divided into two groups, X and Y , such that every pair of nodes in X like each other, every pair of nodes in Y like each other, and everyone in X is the enemy of everyone in Y .\n\nNotice the strength of the statement: either all + or two mutually exclusive groups of friends that are all enemies with other group\n\n","type":"content","url":"/l04-03-structural-balance#implications-balance-theorem","position":9},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Proving Balance Theorem","lvl2":"Balance In Graphs"},"type":"lvl3","url":"/l04-03-structural-balance#proving-balance-theorem","position":10},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Proving Balance Theorem","lvl2":"Balance In Graphs"},"content":"We will provide some intuition for how to prove the Balance Theorem, which will help us understand why it is true\n\nConsider a complete Graph G\n\nTwo alternative cases:\n\nEveryone is friends: satisfies theorem by definition\n\nThere are some + and some - edges: need to prove\n\nFor case 2, we must be able to split G into X and Y where the following hold\n\nEvery node in X is friends with every other node in X\n\nEvery node in Y is friends with every other node in Y\n\nEvery node in X is enemies with every node in Y\n\n","type":"content","url":"/l04-03-structural-balance#proving-balance-theorem","position":11},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"type":"lvl3","url":"/l04-03-structural-balance#proof-by-construction","position":12},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"content":"Start with complete, balanced graph G (our only assumption!)\n\nWe will prove the balance theorem by constructing sets X and Y and verifying that the members of these sets satisfy the 3 properties outlined above\n\nTo start, pick any node A \\in G\n\nDivide all other nodes that are friends with A into X and enemies into Y\n\nBecause G is complete, this is all nodes\n\n","type":"content","url":"/l04-03-structural-balance#proof-by-construction","position":13},{"hierarchy":{"lvl1":"Structural Balance","lvl4":"Condition 1: \\forall B, C \\in X \\quad B \\rightarrow C = +","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"type":"lvl4","url":"/l04-03-structural-balance#condition-1-forall-b-c-in-x-quad-b-rightarrow-c","position":14},{"hierarchy":{"lvl1":"Structural Balance","lvl4":"Condition 1: \\forall B, C \\in X \\quad B \\rightarrow C = +","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"content":"Let B, C \\in X\n\nWe know A \\rightarrow B = + and A \\rightarrow C = +\n\nBecause graph is balanced, this triangle must have 1 or 3 +\n\nThere are already 2, so it must be that B \\rightarrow C = +\n\nB, C were arbitrary, so this part is proven\n\n","type":"content","url":"/l04-03-structural-balance#condition-1-forall-b-c-in-x-quad-b-rightarrow-c","position":15},{"hierarchy":{"lvl1":"Structural Balance","lvl4":"Condition 2: \\forall D, E \\in Y \\quad D \\rightarrow E = +","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"type":"lvl4","url":"/l04-03-structural-balance#condition-2-forall-d-e-in-y-quad-d-rightarrow-e","position":16},{"hierarchy":{"lvl1":"Structural Balance","lvl4":"Condition 2: \\forall D, E \\in Y \\quad D \\rightarrow E = +","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"content":"Let D, E \\in Y\n\nWe know A \\rightarrow D = - and A \\rightarrow E = -\n\nBecause graph is balanced, this triangle must have 1 or 3 +\n\nThere no + and only one option left, so it must be that D \\rightarrow E = +\n\nD, E were arbitrary, so this part is proven\n\n","type":"content","url":"/l04-03-structural-balance#condition-2-forall-d-e-in-y-quad-d-rightarrow-e","position":17},{"hierarchy":{"lvl1":"Structural Balance","lvl4":"Condition 3: \\forall B \\in X and E \\in Y \\quad B \\rightarrow E = -","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"type":"lvl4","url":"/l04-03-structural-balance#condition-3-forall-b-in-x-and-e-in-y-quad-b-rightarrow-e","position":18},{"hierarchy":{"lvl1":"Structural Balance","lvl4":"Condition 3: \\forall B \\in X and E \\in Y \\quad B \\rightarrow E = -","lvl3":"Proof by construction","lvl2":"Balance In Graphs"},"content":"Let B \\in X and D \\in Y\n\nWe know A \\rightarrow D = - and A \\rightarrow B = +\n\nBecause graph is balanced, this triangle must have 1 or 3 +\n\nThere is one + (A \\rightarrow B) and only one option left, so it must be that B \\rightarrow D = -\n\nB, D were arbitrary, so this part is proven\n\n","type":"content","url":"/l04-03-structural-balance#condition-3-forall-b-in-x-and-e-in-y-quad-b-rightarrow-e","position":19},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Summary","lvl2":"Balance In Graphs"},"type":"lvl3","url":"/l04-03-structural-balance#summary","position":20},{"hierarchy":{"lvl1":"Structural Balance","lvl3":"Summary","lvl2":"Balance In Graphs"},"content":"We’ve just proven that for any complete, balanced graph G; we can partition G into sets X and Y that satisfy the group structure of all friends or two groups of friends\n\nThis has interesting implications for fields like social interactions, international relations, and online behavior\n\n","type":"content","url":"/l04-03-structural-balance#summary","position":21},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Application: International Relations"},"type":"lvl2","url":"/l04-03-structural-balance#application-international-relations","position":22},{"hierarchy":{"lvl1":"Structural Balance","lvl2":"Application: International Relations"},"content":"Consider the evolution of alliances in Europe between 1872 and 1907 as represented in the graphs below\n\n","type":"content","url":"/l04-03-structural-balance#application-international-relations","position":23},{"hierarchy":{"lvl1":"Production Networks"},"type":"lvl1","url":"/l05-01-production-networks","position":0},{"hierarchy":{"lvl1":"Production Networks"},"content":"Prerequisites\n\nIntroduction to Graphs\n\nWeighted Graphs\n\nOutcomes\n\nRecall the key concepts of spectral theory from Linear Algebra\n\nDesribe the key proprties of the Leintief family of production models\n\nExplain the difference between “in”-based centrality and “out”-based centrality\n\nAnalyze the impact of sector specific shocks on other sectors of the US economy\n\nReferences\n\nQuantEcon Networks chapters 1-2  (especially section 1.2)\n\n","type":"content","url":"/l05-01-production-networks","position":1},{"hierarchy":{"lvl1":"Production Networks","lvl2":"Linear Algebra"},"type":"lvl2","url":"/l05-01-production-networks#linear-algebra","position":2},{"hierarchy":{"lvl1":"Production Networks","lvl2":"Linear Algebra"},"content":"Linear algebra is the backbone of modern computational algorithms\n\nGraphics\n\nStatistics\n\nData analysis\n\nOptimization\n\nMachine Learning\n\nWorkhorse for accelerated computing\n\nGPUs work on matrices to efficiently parallellize common computations\n\nSpecialty hardware like the TPU (tensor processing unit) take this even further\n\n","type":"content","url":"/l05-01-production-networks#linear-algebra","position":3},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Building Blocks","lvl2":"Linear Algebra"},"type":"lvl3","url":"/l05-01-production-networks#building-blocks","position":4},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Building Blocks","lvl2":"Linear Algebra"},"content":"Vectors: arrays of numbers representing points in multi-dimensional space\n\nMatrices\n\nRectangular arrays that transform vectors\n\nAlso used to represent certain datasets/relationships: e.g. adjacency matrix in Graph Theory\n\nTensors: higher dimensional collections of numbers that allow high-dimensional\n\nVectors are 1d tensors, matrices 2d tensors, etc.\n\nImplemented in numpy as np.array and Julia as the build in array\n\n","type":"content","url":"/l05-01-production-networks#building-blocks","position":5},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Some Key Theories","lvl2":"Linear Algebra"},"type":"lvl3","url":"/l05-01-production-networks#some-key-theories","position":6},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Some Key Theories","lvl2":"Linear Algebra"},"content":"Linear systems of equations\n\nInner product spaces (length, distance, and angles)\n\nEigenvalues and eigen vectors (spectral theory)\n\n","type":"content","url":"/l05-01-production-networks#some-key-theories","position":7},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Spectral Theory","lvl2":"Linear Algebra"},"type":"lvl3","url":"/l05-01-production-networks#spectral-theory","position":8},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Spectral Theory","lvl2":"Linear Algebra"},"content":"Let’s dig into eigenvalues and eigenvectors\n\nLet A \\in \\mathbb{R}^{n \\times n}\n\nA scalar \\lambda \\in \\mathbb{C} is an eigenvalue of A if there exists a nonzero vector e \\in \\mathbb{C}^n such that A e = \\lambda e\n\nA vector e satisfying this equality is called an eigenvector corresponding to the eigenvalue \\lambda\n\nA vector \\epsilon sastifying A ' \\epsilon = \\lambda \\epsilon is a left-eigenvector of A\n\nusing LinearAlgebra\n\nA = [\n    0 -1\n    1 0\n]\neigvals(A)\n\n","type":"content","url":"/l05-01-production-networks#spectral-theory","position":9},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Definitions","lvl3":"Spectral Theory","lvl2":"Linear Algebra"},"type":"lvl4","url":"/l05-01-production-networks#definitions","position":10},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Definitions","lvl3":"Spectral Theory","lvl2":"Linear Algebra"},"content":"The set of all eigenvalues of A is called the spectrum of A and is written \\sigma(A)\n\nThe spectral radius of A is written r(A) \\equiv \\max \\{ | \\lambda | : \\lambda \\in \\sigma(A) \\}\n\nThe spectral radius important when considering the convergence of a dynamic system driven by A\n\nIf x_{t+1} = A x_t and r(A)< 1 then the sequence \\{x\\}_t is finite and will converge\n\nSome facts (not proven here)\n\nA will have at most n distinct eigenvalues\n\nEigenvectors are only unique to a scalar multiple: if (\\lambda, e) is an eigenpair, then so is (\\lambda, \\alpha e) for any \\alpha \\in \\mathbb{R}, \\alpha \\ne 0\n\nSometimes a matrix will have a repeated eigenvalue (the same value works with multiple distinct eigenvectors)\n\nThe algebraic multiplicity of an eigenvalue is the number of times it is repeated with distinct eigenvectors\n\nAn eigenvalue with an algebraic multiplicity of one is called simple\n\n","type":"content","url":"/l05-01-production-networks#definitions","position":11},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Diagonalization","lvl3":"Spectral Theory","lvl2":"Linear Algebra"},"type":"lvl4","url":"/l05-01-production-networks#diagonalization","position":12},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Diagonalization","lvl3":"Spectral Theory","lvl2":"Linear Algebra"},"content":"A matrix A \\in \\mathbb{R}^{n \\times n} is diagonalizable if A = PDP^{-1}\n\nD = \\text{diag}(\\lambda_1, \\dots, \\lambda_n), P is some invertible matrix\n\nThe decomposition is called the eigen decomposition or spectral decomposition of A\n\nThe asymptotic properties of m \\mapsto A^m are determined by \\sigma(A)\n\nCan be seen when A is diagonalizable\n\nIf A = P \\text{diag}(\\lambda) P^{-1} \\Longrightarrow A^m = P \\text{diag}(\\lambda_i^m) P^{-1}\n\n","type":"content","url":"/l05-01-production-networks#diagonalization","position":13},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Example: Worker Dynamics","lvl2":"Linear Algebra"},"type":"lvl3","url":"/l05-01-production-networks#example-worker-dynamics","position":14},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Example: Worker Dynamics","lvl2":"Linear Algebra"},"content":"Consider a continuum of workers (large number, not counted discretely)\n\nWorkers can be in one of two states: (1) employed and (2) unemployed\n\nEach month workers are hired at a rate \\alpha and fired at a rate \\beta\n\nWe express these dynamics as a weighted directed graph with adjacency matrix:P_w = \\begin{bmatrix}1-\\alpha & \\alpha \\\\ \\beta & 1-\\beta \\end{bmatrix} \\; \\alpha, \\beta \\in [0, 1]\n\nRow 1 gives the probabilities of employment and unemployement for an employed worker\n\nRow 2 gives the probabilities of employment and unemployement for an unemployed worker\n\n","type":"content","url":"/l05-01-production-networks#example-worker-dynamics","position":15},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Transitions","lvl3":"Example: Worker Dynamics","lvl2":"Linear Algebra"},"type":"lvl4","url":"/l05-01-production-networks#transitions","position":16},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Transitions","lvl3":"Example: Worker Dynamics","lvl2":"Linear Algebra"},"content":"Suppose we have a vector x \\in \\Delta(2) (2 element simplex) representing current fraction of workers that are employed and unemployed\n\nQuestion... What matrix operation between P_w and x will give the fraction of workers that start next month as employed and unemployed?\n\nalpha = 0.3\nbeta = 0.1\n\nP = [1-alpha alpha; beta 1-beta]\n\nx = [0.9, 0.1]\n\n# TODO: simulate for many periods\n\n# TODO: compare to largest left eigenvector\n\n","type":"content","url":"/l05-01-production-networks#transitions","position":17},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Neumann Series Lemma","lvl2":"Linear Algebra"},"type":"lvl3","url":"/l05-01-production-networks#neumann-series-lemma","position":18},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Neumann Series Lemma","lvl2":"Linear Algebra"},"content":"We need one more linear alegbra result...\n\nIf A \\in \\mathbb{R}^{n \\times n} and r(A) < 1, then I-A is non-singular and (I-A)^{-1} = \\sum_{m=0}^{\\infty} A^m\n\nThis is known as the Neumann series lemma\n\n","type":"content","url":"/l05-01-production-networks#neumann-series-lemma","position":19},{"hierarchy":{"lvl1":"Production Networks","lvl2":"Production Networks"},"type":"lvl2","url":"/l05-01-production-networks#production-networks","position":20},{"hierarchy":{"lvl1":"Production Networks","lvl2":"Production Networks"},"content":"We will now study a family of economic models that allow us to analyze the economy as a collection of sectors\n\nThese models were proposed by nobel prize winner Wassily Leontief in 1941 and are still commonly used today\n\nThe key idea behind a Leontief model is the input-output table\n\n","type":"content","url":"/l05-01-production-networks#production-networks","position":21},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Input-output tables","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#input-output-tables","position":22},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Input-output tables","lvl2":"Production Networks"},"content":"Firms (companies) are categorized into n distinct sectors\n\nFirms use materials produced by other sectors as part of their production process\n\nThe relationship of flows of value are organized into an input/output table\n\n\n\nsector 1\n\nsector 2\n\nsector 3\n\nsector 1\n\na_{11}\n\na_{12}\n\na_{13}\n\nsector 2\n\na_{21}\n\na_{22}\n\na_{23}\n\nsector 3\n\na_{31}\n\na_{32}\n\na_{33}\n\na_{ij} is called an input-output coefficient and is equal to:a_{ij} = \\frac{\\text{value of sector j inputs purchased from sector i}}{\\text{total sales of sector j}}\n\nIf a_{ij} is large, sector i is an important supplier of intermediate goods to sector j\n\nThe sum of column j is the value of all inputs to sector j\n\nRow i shows how intensively each sector uses good i as an intermediate good\n\nThe input output table can be directly used as the adjacency matrix for a weighted directed graph\n\n","type":"content","url":"/l05-01-production-networks#input-output-tables","position":23},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Input-output data","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#input-output-data","position":24},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Input-output data","lvl2":"Production Networks"},"content":"In the United States, the Bureau of Economic Analysis is responsible for compiling input-output tables for sectors\n\nWe’ll be studying the \n\nInput-Output Accounts Data\n\nThe main set of tables we’ll be using are called the Make-Use tables\n\nThe Make table shows the value of final goods produced by each sector\n\nNote this is predominately a diagonal matrix as each sector primarily produces and sells final goods within their sector\n\nSometimes a firm will have secondary outputs as part of their production process\n\n“Which industries produce which commodities?”\n\nThe Use tables show the intermediate and final use of commodities across sectors\n\n“Who uses or consumes the commodities produced?”\n\nusing CSV, DataFrames, Graphs, SimpleWeightedGraphs, GraphPlot, ColorSchemes, Colors, Downloads, PlotlyBase\n\nusing Downloads\n\nfunction read_remote_csv(url)\n    bn = basename(url)\n    if !isfile(bn)\n        Downloads.download(url, bn)\n    end\n    CSV.read(bn, DataFrame)\nend\n\nmake_15 = read_remote_csv(\"https://compsosci-resources.s3.amazonaws.com/graph-theory-lectures/data/QE-networks/make_15.csv\");\nuse_15 = read_remote_csv(\"https://compsosci-resources.s3.amazonaws.com/graph-theory-lectures/data/QE-networks/use_15.csv\");\ncodes = read_remote_csv(\"https://compsosci-resources.s3.amazonaws.com/graph-theory-lectures/data/QE-networks/codes.csv\");\n\n","type":"content","url":"/l05-01-production-networks#input-output-data","position":25},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Example: US 15 sector data","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#example-us-15-sector-data","position":26},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Example: US 15 sector data","lvl2":"Production Networks"},"content":"\n\nColor of nodes is according to their hub-based eigenvector centrality (see below)\n\nSize of nodes is according to their total outputs (make table)\n\nThickness of edges is according to amount of goods\n\nRepresent a_{ij} and point from sector creating the intermediate good (sector i) to sector  using intermediate good (sector j)\n\n\n\n","type":"content","url":"/l05-01-production-networks#example-us-15-sector-data","position":27},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Eigenvector Centrality","lvl3":"Example: US 15 sector data","lvl2":"Production Networks"},"type":"lvl4","url":"/l05-01-production-networks#eigenvector-centrality","position":28},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Eigenvector Centrality","lvl3":"Example: US 15 sector data","lvl2":"Production Networks"},"content":"The node size above shows the hub-based eigenvector centrality\n\nThis is equal to the dominant eigenvector of the adjacency matrix (eigenvector associated with largest eigenvalue)\n\nThis measure of centrality measures the influence of a node in a network\n\nWhen computing this statistic for a node N, the value will be higher if it is connected to other “influential” nodes\n\nhttps://​youtu​.be​/LYyZqlyDEL4​?si​=​x2kYhe3phZxQtQvV​&​t​=​202\n\n","type":"content","url":"/l05-01-production-networks#eigenvector-centrality","position":29},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Example","lvl3":"Example: US 15 sector data","lvl2":"Production Networks"},"type":"lvl4","url":"/l05-01-production-networks#example","position":30},{"hierarchy":{"lvl1":"Production Networks","lvl4":"Example","lvl3":"Example: US 15 sector data","lvl2":"Production Networks"},"content":"Let’s compute the eigenvector centrality for the data in the image above\n\nI have some code to import the make/use files into a helpful struct\n\nWe’ll see what all the fields are as we progress through the lecture\n\nto_int(x) = parse(Int, x)\nto_int(x::Integer) = Int(x)\n\nstruct SectorData\n    Z::Matrix{Int}\n    X::Vector{Int}\n    A::Matrix{Float64}\n    F::Matrix{Float64}\n    Z_df::DataFrame\n    names::Vector{String}\n    codes::Vector{String}\n    N::Int\n    G::SimpleWeightedDiGraph{Int64, Float64}\nend\n\nconst CODES = let\n    df = read_remote_csv(\"codes.csv\")\n    Dict(zip(df.name, df.code))\nend\n\nfunction load_sector(N)\n    # read csv\n    df = CSV.read(\"use_$(N).csv\", DataFrame)\n\n    # replace `---` with `0`\n    df .= ifelse.(df .== \"---\", \"0\", df)\n\n    # conver to int\n    df[!, 2:end] .= to_int.(df[!, 2:end])\n\n    # first column is industry name, next columns are sector values\n    # first N rows are values\n    Z = Array(df[1:N, 2:(N+1)])\n    names = df[1:N, 1]\n    codes = [CODES[n] for n in names]\n\n    # total industry outputs come from teh `make_N.csv` file\n    X = CSV.read(\"make_$(N).csv\", DataFrame)[1:N, \"Total Industry Output\"]\n\n    # value of sector j's inputs purchased from i / sales of `j`\n    # or ...\n    A = Z ./ X'\n\n    F = Z ./ X\n\n    # make copy of A where small values are set to zero to make plotting clearer\n    A_copy = copy(A)\n    A_copy[A .<= 0.01] .= 0;\n    G = SimpleWeightedDiGraph(A_copy)\n\n    SectorData(Z, X, A, F, df, names, codes, N, G)\nend\n\ndata15 = load_sector(15);\n\nlambda15 = eigenvector_centrality(data15.G)\n\nPlot(bar(x=data15.codes, y=lambda15))\n\n","type":"content","url":"/l05-01-production-networks#example","position":31},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Accounting","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#accounting","position":32},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Accounting","lvl2":"Production Networks"},"content":"Let...\n\nd_i be the final consumer demand for good i\n\nx_i be total sales of sector i\n\nz_{ij} be inter-industry sales from sector i to sector j\n\nFor accounts to add up we must have x_i = \\sum_{i=1}^n z_{ij} + d_i\n\nNotice that \\frac{z_{ij}}{x_j} = a_{ij} = \\text{dollar value of inputs from $i$ per dollar output from $j$}\n\nThis means x_i = \\sum_{j=1} a_{ij} x_j + d_i\n\nWhich can be written x = Ax + d\n\n","type":"content","url":"/l05-01-production-networks#accounting","position":33},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Equilibrium","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#equilibrium","position":34},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Equilibrium","lvl2":"Production Networks"},"content":"An equilibrium in a Leonteif model with input-output coefficient matrix A and a vector d \\in \\mathbb{R}^n of final consumer demands for each sector is a vector x for sector-specific output such that x = Ax + d is satisfied\n\nNote that d and A are treated as given\n\nTo find this vector requires that we trace the impact of final demand on the intermediate linkages through A\n\nExample:\n\nExample\n\nSuppose d_3 \\uparrow\n\nWill cause 3 to consume more from its suppliers (2 and 4)\n\nWhich will cause 2 to demand more from 1\n\nWhich will cause 1 to demand more from 2 and 4\n\n... and so on\n\nComputing an equilibrium is not entirely straight forward...\n\n","type":"content","url":"/l05-01-production-networks#equilibrium","position":35},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Computing an Equilibrium","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#computing-an-equilibrium","position":36},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Computing an Equilibrium","lvl2":"Production Networks"},"content":"Define v_j \\equiv x_j - \\sum_{i=1}^n z_{ij} as the value added by sector j\n\nAssumption: The input-output adjacency matrix A satisfies \\eta_{ij} \\equiv \\sum_{i=1}^n a_{ij} < 1 \\; \\forall j \\in [n]\n\nThis holds whenever v_j > 0 \\forall j\n\nWhen this assumption holds, for each d \\ge 0 there is a unique, nonnegative output solution given by x^* = Ld \\qquad \\text{where } L \\equiv (I-A)^{-1}\n\nThe matrix L is called the Leontief inverse associated with the coefficient matrix A\n\nL15 = inv(I - data15.A)\n\n# propose a demand vector where each element is between [100, 600]\nd = rand(15).* 500 .+ 100\n\nL15 * d\n\nQuestion\n\nWe don’t have a field for d in our SectorData struct.\n\nHow could we compute/derive d given the fields we do have?\n\nfieldnames(SectorData)\n\n","type":"content","url":"/l05-01-production-networks#computing-an-equilibrium","position":37},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Demand Shocks","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#demand-shocks","position":38},{"hierarchy":{"lvl1":"Production Networks","lvl3":"Demand Shocks","lvl2":"Production Networks"},"content":"A common form of economic analysis is to consider the impact of external “shocks”\n\nOften, these are modeled as a one time change in the value of a variable\n\nLet’s consider a demand shock of size \\Delta d such that demand moves from d_0 to d_1 = \\Delta d + d_0\n\nThe equilibrium vector shifts from x_0 = L d_0 to x_1 = L d_1\n\nDefine \\Delta x = L (d_1 - d_0) = L \\delta d\n\n","type":"content","url":"/l05-01-production-networks#demand-shocks","position":39},{"hierarchy":{"lvl1":"Production Networks","lvl3":"NSL","lvl2":"Production Networks"},"type":"lvl3","url":"/l05-01-production-networks#nsl","position":40},{"hierarchy":{"lvl1":"Production Networks","lvl3":"NSL","lvl2":"Production Networks"},"content":"We will further assume that r(A)<1 so that we can write the expression for \\Delta x as an infinite sum using the Neumann Series Lemma: \\Delta x = \\Delta d + A (\\Delta d) + A^2 (\\Delta d) + \\cdots\n\n\\Delta d is the initial response in each sector,\n\nA (\\Delta d) is the response generated by the first round of backward linkages,\n\nA^2 (\\Delta d) is the response generated by the second round, and so on.\n\nThe total response is the sum of the responses at each round\n\nThe typical element l_{ij} of L = \\sum_{m=0}^{\\infty} A^m shows total impact on sector i of a unit change in the demand for good j\n\nL15\n\nPlot(heatmap(z=L15))","type":"content","url":"/l05-01-production-networks#nsl","position":41},{"hierarchy":{"lvl1":"Agent Based Models"},"type":"lvl1","url":"/l06-01-abm-concepts","position":0},{"hierarchy":{"lvl1":"Agent Based Models"},"content":"Computational Analysis of Social Complexity\n\nPrerequisites\n\nNone 😏\n\nOutcomes\n\nUnderstand what a model is\n\nKnow the difference between what we call equation based models and agent based models\n\nUnderstand the key building blocks of agent based models\n\nLearn the key components of the Schelling segregation model\n\nReferences\n\nCioffi-Revilla Chapter 10\n\nhttps://​journal​.sohostrategy​.com​/what​-does​-an​-agent​-based​-model​-look​-like​-dc1fbc17f2f5\n\nhttps://​journal​.sohostrategy​.com​/what​-is​-abm​-abms​-f52ff2f1f712\n\nhttps://​towardsdatascience​.com​/agent​-based​-modeling​-will​-unleash​-a​-new​-paradigm​-of​-machine​-learning​-ff6d3b1ac940​?source​=​search​_post​-​-​-​-​-​-​-​--3\n\n","type":"content","url":"/l06-01-abm-concepts","position":1},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"Why Models?"},"type":"lvl2","url":"/l06-01-abm-concepts#why-models","position":2},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"Why Models?"},"content":"Many topics of interest for social scientists are either unethical or unreasonable to study in a laboratory\n\nImpact on communities of upsurge in illicit drug usage (can’t give people drugs to see impact)\n\nFlow of traffic given a new infrastructure updates (too costly to experiment with)\n\nImpact of new tarrifs in international trading relationships (too costly to coorindate legislation and implement)\n\nFor this reason, we as social scientists turn to models to study our problems\n\nA model is a probability distribution over outcomes\n\nI’ll repeat for emphasis: a model is a probability distribution over outcomes\n\n","type":"content","url":"/l06-01-abm-concepts#why-models","position":3},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Types of Models","lvl2":"Why Models?"},"type":"lvl3","url":"/l06-01-abm-concepts#types-of-models","position":4},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Types of Models","lvl2":"Why Models?"},"content":"A model is a mathematical object: equations, rules, distributional assumptions.\n\nAt its heart, a model is a simplification of some real-world system or phenonmenom\n\nMuch complexity is abstracted away (or not included directly in model)\n\nKey aspects relevant for study are modeled explicitly (e.g. trading response to tarrifs)\n\nFor our purposes, we will think of models as belonging to one of two families\n\nEquation based models\n\nAgent based models\n\nThis is a simplification and not a perfect classification (because equation based models have agents and agent based models have equations), but we will be able to draw useful distinctions with this classification.\n\n","type":"content","url":"/l06-01-abm-concepts#types-of-models","position":5},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Equation Based Models","lvl2":"Why Models?"},"type":"lvl3","url":"/l06-01-abm-concepts#equation-based-models","position":6},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Equation Based Models","lvl2":"Why Models?"},"content":"An equation based model describes the decision making setting for each agent using mathematical equations\n\nTypically, these are posed as (constrained) optimization problems\n\nA set of equaitons is also developed that describe interaction between agents\n\nThese equations can feature random variables and will require specification of model parameters\n\nMost models I study and develop in my economics research are equation based\n\nPros:\n\nAllow precise specification of assumptions, incentives, and outcomes\n\nHave wide toolbox of numerical optimization, and statistical fitting to “solve” model\n\nCons:\n\nOptimization and calibration of parameters can be very difficult\n\nOften subject to the “curse of dimensionality”, which limits size and complexity of model\n\n","type":"content","url":"/l06-01-abm-concepts#equation-based-models","position":7},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Agent Based Models (ABMs)","lvl2":"Why Models?"},"type":"lvl3","url":"/l06-01-abm-concepts#agent-based-models-abms","position":8},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Agent Based Models (ABMs)","lvl2":"Why Models?"},"content":"An agent based model describes rules for how individual agents respond to their environment\n\nThere are usually many agents, each with a set of properties\n\nOne common property is the type of the agent: usually drawn from a small/finite set (buyer-seller, parent-child-teacher, sheep-wolf)\n\nAll agents of the same type have the same set of additional properties\n\nEach agent has a state at each time step t\n\nThe rules are equations that specify how the state of an agent is updated between periods t and t+1\n\nRules are common for all agents of a type, but vary based on that agent’s state and property values\n\nRules will often have random variables as well as parameters\n\nRules often include notion of “neighboring” agents\n\nPros:\n\nFocus on how an individual should respond in a given state without requiring optimization\n\nBecause rules are typically mathematically simple, can have many many agents\n\nCons:\n\nOften lacks notion of equilibrium (could be a feature)\n\nNot very “reusable” -- to study specific topic you usually have to create whole new model\n\nSometimes \n\ntoo many parameters: need for careful calibration\n\n","type":"content","url":"/l06-01-abm-concepts#agent-based-models-abms","position":9},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"ABMs"},"type":"lvl2","url":"/l06-01-abm-concepts#abms","position":10},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"ABMs"},"content":"For the next few lectures we’ll focus on agent base models\n\nWe’ll start by outlining the main components of an ABM\n\nThen we’ll talk about how we could represent them in Julia using the Agents.jl library\n\nThis will require a step up in our Julia skills, so we’ll spend some time covering these concepts in greater detail\n\nFinally we’ll see a few examples of ABMs in practice\n\nNOTE: Most of the study of the Julia skills and ABM examples are not in this notebook\n\n","type":"content","url":"/l06-01-abm-concepts#abms","position":11},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"ABM components","lvl2":"ABMs"},"type":"lvl3","url":"/l06-01-abm-concepts#abm-components","position":12},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"ABM components","lvl2":"ABMs"},"content":"ABMs are made up of 3 distinct components:\n\nAgents\n\nEnvironment\n\nRules\n\n","type":"content","url":"/l06-01-abm-concepts#abm-components","position":13},{"hierarchy":{"lvl1":"Agent Based Models","lvl4":"Agents","lvl3":"ABM components","lvl2":"ABMs"},"type":"lvl4","url":"/l06-01-abm-concepts#agents","position":14},{"hierarchy":{"lvl1":"Agent Based Models","lvl4":"Agents","lvl3":"ABM components","lvl2":"ABMs"},"content":"Have state at discrete time steps t (state is value of properties, some properties might be fixed)\n\nAlways aware of its own state\n\nAutonomous: can make a decisions independent of other agents\n\nReactive: can respond to changes in environment or state of other agents\n\nProactive: can behave in a way to achieve a goal\n\nCommunicate: can make some attributes visible to other agents\n\n","type":"content","url":"/l06-01-abm-concepts#agents","position":15},{"hierarchy":{"lvl1":"Agent Based Models","lvl4":"Environments","lvl3":"ABM components","lvl2":"ABMs"},"type":"lvl4","url":"/l06-01-abm-concepts#environments","position":16},{"hierarchy":{"lvl1":"Agent Based Models","lvl4":"Environments","lvl3":"ABM components","lvl2":"ABMs"},"content":"One of two types\n\nNatural Environments: biophysical landscapes and settings\n\nArtifical environments: classrooms, economic markets, parks, transportation streets, buildings, etc.\n\nAgents reside within an environment\n\nProperties of environment can be fixed (size, dimensions) or varying (weather, congestion, unused capacity)\n\nAgents can observe and potentially respond to properties of the environment\n\n","type":"content","url":"/l06-01-abm-concepts#environments","position":17},{"hierarchy":{"lvl1":"Agent Based Models","lvl4":"Rules","lvl3":"ABM components","lvl2":"ABMs"},"type":"lvl4","url":"/l06-01-abm-concepts#rules","position":18},{"hierarchy":{"lvl1":"Agent Based Models","lvl4":"Rules","lvl3":"ABM components","lvl2":"ABMs"},"content":"Rules are the key feature that makes ABMs dynamic\n\nTypes of rules:\n\nInter-agent: how agents communicate and respond to one another (e.g. information spread)\n\nAgent-environment rules: How an agent responds to an environment (e.g. avoid park if raining), or how an agent’s decisions and behaviors impact environment (e.g. more cars => more pollution)\n\nIntra-environmental rules: cause and effect mechanisms within the environment (e.g. more rain => more vegetation)\n\n","type":"content","url":"/l06-01-abm-concepts#rules","position":19},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"ABMs in Julia"},"type":"lvl2","url":"/l06-01-abm-concepts#abms-in-julia","position":20},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"ABMs in Julia"},"content":"We need a way to represent these three components in Julia\n\nAgents: represent as a Julia struct\n\nStruct fields record agent properties\n\nOur custom agent type can have methods that ascribe behavior to agents\n\nEnvironments: either explicitly as Julia struct or implicitly in the update rules\n\nRules: julia functions\n\nKey function is step! which will allow our agents to make decisions and have the environment and agent properties update in response\n\n","type":"content","url":"/l06-01-abm-concepts#abms-in-julia","position":21},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"Schelling Segregation Model"},"type":"lvl2","url":"/l06-01-abm-concepts#schelling-segregation-model","position":22},{"hierarchy":{"lvl1":"Agent Based Models","lvl2":"Schelling Segregation Model"},"content":"We will work on learning how to represent our agents, rules, and environment in Julia\n\nTo make that discussion more concrete, it will be helpful to have a model to implement\n\nThe “hello world” of ABMs may just be the Schelling segregation model\n\nReferences include \n\nQuantEcon and \n\nAgents.jl tutorial\n\n","type":"content","url":"/l06-01-abm-concepts#schelling-segregation-model","position":23},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Schelling’s Work","lvl2":"Schelling Segregation Model"},"type":"lvl3","url":"/l06-01-abm-concepts#schellings-work","position":24},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"Schelling’s Work","lvl2":"Schelling Segregation Model"},"content":"Thomas Schelling won a nobel price in economics for his study of racial segregation\n\nAt the heart of his study, was a model proposed in 1969 for how racial segregation can occur in urban areas\n\nOne theme of this work (and ABMs in general) is that local interactions (like decisions of individual agents) can lead to surprising aggregate results\n\n","type":"content","url":"/l06-01-abm-concepts#schellings-work","position":25},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"The Model","lvl2":"Schelling Segregation Model"},"type":"lvl3","url":"/l06-01-abm-concepts#the-model","position":26},{"hierarchy":{"lvl1":"Agent Based Models","lvl3":"The Model","lvl2":"Schelling Segregation Model"},"content":"Environment: 25x25 grid of single family dwellings\n\nAgents with properties:\n\nlocation (x,y) coordinate for current home\n\ntype: orange or blue. Fixed over time. 250 of each\n\nhappiness: 0 if less than N of neighbors are of same type, 1 otherwise\n\nRules:\n\nAgents choose to move to unoccupied grid point if unhappy\n\nNote neighbors for a particular cell are the the 8 other cells surrounding the cell of interest. Corner or edge cells have less than 8 neighbors","type":"content","url":"/l06-01-abm-concepts#the-model","position":27},{"hierarchy":{"lvl1":"Julia Skills"},"type":"lvl1","url":"/l06-02-julia-skills","position":0},{"hierarchy":{"lvl1":"Julia Skills"},"content":"Computational Analysis of Social Complexity\n\nSpencer Lyon\n\nPrerequisites\n\nNone\n\nOutcomes\n\nRemember how to define custom structs in Julia\n\nLearn to use the Julia REPL\n\nLearn how to create a custom Julia environment for a project\n\n","type":"content","url":"/l06-02-julia-skills","position":1},{"hierarchy":{"lvl1":"Julia Skills","lvl2":"ABMs in Julia"},"type":"lvl2","url":"/l06-02-julia-skills#abms-in-julia","position":2},{"hierarchy":{"lvl1":"Julia Skills","lvl2":"ABMs in Julia"},"content":"We will be defining agent based models in Julia\n\nDo do this, we need to remember/review a few concepts:\n\nTypes in Julia\n\nEvaluating code in REPL\n\nPackages and environments\n\n","type":"content","url":"/l06-02-julia-skills#abms-in-julia","position":3},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"Review: Julia Types","lvl2":"ABMs in Julia"},"type":"lvl3","url":"/l06-02-julia-skills#review-julia-types","position":4},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"Review: Julia Types","lvl2":"ABMs in Julia"},"content":"Recall how to define a composite type in Juliamutable struct NAME <: ABSTRACT_PARENT\n   FIELDS\nend\n\nNote including mutable allows us to change fields after creating an object (important for today’s application)\n\nWe can define new methods for an existing function for our type: isint(::NAME) = false\n\n","type":"content","url":"/l06-02-julia-skills#review-julia-types","position":5},{"hierarchy":{"lvl1":"Julia Skills","lvl2":"Running Julia Code"},"type":"lvl2","url":"/l06-02-julia-skills#running-julia-code","position":6},{"hierarchy":{"lvl1":"Julia Skills","lvl2":"Running Julia Code"},"content":"So far we’ve been running our Julia programs in Jupyter\n\nThis is an extra execution environment we added to our Julia installation\n\nThe default way to run Julia is in the REPL (Read-Eval-Print-Loop)\n\n","type":"content","url":"/l06-02-julia-skills#running-julia-code","position":7},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"REPL","lvl2":"Running Julia Code"},"type":"lvl3","url":"/l06-02-julia-skills#repl","position":8},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"REPL","lvl2":"Running Julia Code"},"content":"The REPL typically started either by typing julia in a terminal or by clicking on the Julia icon in your Applications list\n\nOnce staretd, we will see a prompt julia> where we can enter Julia code\n\nIf we enter code and press Enter, the REPL will evaluate the code and print the result\n\n","type":"content","url":"/l06-02-julia-skills#repl","position":9},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"REPL Modes","lvl2":"Running Julia Code"},"type":"lvl3","url":"/l06-02-julia-skills#repl-modes","position":10},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"REPL Modes","lvl2":"Running Julia Code"},"content":"The REPL has a few modes:\n\nDefault julia> mode: this is where we run Julia code and see output\n\nshell> mode: here we can interact with underlying shell/terminal. We activate shell mode with ;\n\nhelp?> mode: here we can get help on Julia functions. We activate help mode with ?\n\npkg> mode: here we can interact with Julia packages. We activate pkg mode with ]\n\n","type":"content","url":"/l06-02-julia-skills#repl-modes","position":11},{"hierarchy":{"lvl1":"Julia Skills","lvl2":"Packages"},"type":"lvl2","url":"/l06-02-julia-skills#packages","position":12},{"hierarchy":{"lvl1":"Julia Skills","lvl2":"Packages"},"content":"Julia code is structured into packages\n\nPackages fulfill a few roles\n\nAllow library developers to group+distributed related types and functions\n\nAllow application developers to create an isolated environment for an application\n\nSo far, we have used packages in the first context above: DataFrames.jl, Graphs.jl, and CSV.jl\n\nToday we will introduce the second one...\n\n","type":"content","url":"/l06-02-julia-skills#packages","position":13},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"Environments","lvl2":"Packages"},"type":"lvl3","url":"/l06-02-julia-skills#environments","position":14},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"Environments","lvl2":"Packages"},"content":"Any folder on your computer can be an environment for a Julia package/application\n\nIf a directory has a Project.toml file with appropriate contents, it is a Julia environment\n\nToday we will use htis feature to create an isolated environment for our ABM to make sure we have the right versions of the right packages installedm\n\n","type":"content","url":"/l06-02-julia-skills#environments","position":15},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"Create a new environment","lvl2":"Packages"},"type":"lvl3","url":"/l06-02-julia-skills#create-a-new-environment","position":16},{"hierarchy":{"lvl1":"Julia Skills","lvl3":"Create a new environment","lvl2":"Packages"},"content":"To create a new environment, you do the following:\n\nStart Julia in the directory you wish to make into an environment (or use shell mode -- ; -- to move there)\n\nEnter package mode by pressing ]\n\nEnter activate . to create a new environment in the current directory\n\nWhile in package mode use add XXX to add packages to the environment\n\nJulia will record all the packages you request as well as their versions\n\nOther users can then use ] activate . followed by instantiate to install the same packages and versions","type":"content","url":"/l06-02-julia-skills#create-a-new-environment","position":17},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl"},"type":"lvl1","url":"/l06-03-shelling-agents","position":0},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl"},"content":"Computational Analysis of Social Complexity\n\nPrerequisites\n\nJulia Basics\n\nJulia Types\n\nABM intro\n\nOutcomes\n\nImplement the Schelling segregation model using Agents.jl\n\nReferences\n\nAgents.jl tutorial\n\nQuantEcon lecture\n\n","type":"content","url":"/l06-03-shelling-agents","position":1},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl2":"Review Schelling Model"},"type":"lvl2","url":"/l06-03-shelling-agents#review-schelling-model","position":2},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl2":"Review Schelling Model"},"content":"Recall the components of the Schelling segregation model\n\nEnvironment: 25x25 grid of single family dwellings\n\nAgents with properties:\n\nlocation (x,y) coordinate for current home\n\ntype: orange or blue. Fixed over time. 250 of each\n\nhappiness: 0 if less than N of neighbors are of same type, 1 otherwise\n\nRules:\n\nAgents choose to move to unoccupied grid point if unhappy\n\nNote neighbors for a particular cell are the the 8 other cells surrounding the cell of interest. Corner or edge cells have less than 8 neighbors\n\n","type":"content","url":"/l06-03-shelling-agents#review-schelling-model","position":3},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl2":"Agents.jl"},"type":"lvl2","url":"/l06-03-shelling-agents#agents-jl","position":4},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl2":"Agents.jl"},"content":"We are now ready to get started implementing the Schelling segregation model in Julia\n\nWe’ll use the Agents.jl library, which is a very powerful ABM toolkit\n\nHere are some points of comparison between Agents.jl and other ABM software (from the Agents.jl website):\n\n","type":"content","url":"/l06-03-shelling-agents#agents-jl","position":5},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl2":"Schelling in Agents.jl"},"type":"lvl2","url":"/l06-03-shelling-agents#schelling-in-agents-jl","position":6},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl2":"Schelling in Agents.jl"},"content":"Let’s now implement the Schelling Segregation model in Agents.jl\n\nThe first thing we’ll need to do is define our Agent\n\nThe reccomended way to do this is to create a new Juila struct using the @agent macro provided by Agents.jl\n\nThe macro will ensure a few things:\n\nThe struct contains id and pos fields to keep track of the agent and its position\n\nThe struct is mutable so the position can be updated\n\nThe struct is a subtype of AbstractAgent so it can be used by other functions in Agents.jl\n\n# load up packages we need for this example... might take a few minutes\n# import Pkg\n# Pkg.activate(\".\")\n# Pkg.instantiate()\n\nusing Agents\n\n@agent struct SchellingAgent(GridAgent{2})\n    is_happy::Bool      # whether the agent is happy in its position. (true = happy)\n    group::Int          # The group of the agent, determines mood as it interacts with neighbors 0: blue, 1: orange\nend\n\nWe can see the complete structure of the ShellingAgent type using the dump function\n\ndump(SchellingAgent)\n\n","type":"content","url":"/l06-03-shelling-agents#schelling-in-agents-jl","position":7},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Schelling Environment","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#schelling-environment","position":8},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Schelling Environment","lvl2":"Schelling in Agents.jl"},"content":"Our Schelling environment will be one the built-in Agents.jl spaces\n\nWe’ll use GridSpace\n\nenvironment = GridSpaceSingle((25, 25); periodic = false)\n\n","type":"content","url":"/l06-03-shelling-agents#schelling-environment","position":9},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Rules","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#rules","position":10},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Rules","lvl2":"Schelling in Agents.jl"},"content":"The last part of our ABM that we need to specify is the rules for how agents respond to the environment and state\n\nOur rule is that agents will choose to move to an empty grid space if they have less than wanted_neighbors neighbors of the same group\n\nAgents.jl requires us to implement these rules in a method agent_step!(agent::SchellingAgent, model)\n\nWe’ll make use of a couple helper functions provided by Agents.jl:\n\nmove_agent_single!: move a single agent to an empty place in the environment. This modifies the pos field of the agent\n\nnearby_agents: return an array of all neighbors of a particular agent. This queries the pos field of all agents\n\nfunction agent_step!(agent::SchellingAgent, model)\n\twant = model.wanted_neighbors\n\thave = 0\n\tfor n in nearby_agents(agent, model)\n\t\tif n.group == agent.group\n\t\t\thave += 1\n\t\tend\n\tend\n\tagent.is_happy = have >= want\n\tif !agent.is_happy\n\t\tmove_agent_single!(agent, model)\n\tend\n\treturn\nend\n\n","type":"content","url":"/l06-03-shelling-agents#rules","position":11},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Combining Agents, Environment, and Rules","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#combining-agents-environment-and-rules","position":12},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Combining Agents, Environment, and Rules","lvl2":"Schelling in Agents.jl"},"content":"We now need to create an instance of the AgentBasedModel (or ABM for short) type\n\nTo construct our instance we need to specify our agent type, our environment, our rules (via agent_step! function), and any additional properties\n\nThese additional properties belong to the model, and can be thought of as parameters that should be calibrated\n\nIn our previous exposition we would have attached these to the environment\n\nproperties = Dict(:wanted_neighbors => 4)\nschelling = AgentBasedModel(SchellingAgent, environment; properties=properties, agent_step! = agent_step!)\n\n","type":"content","url":"/l06-03-shelling-agents#combining-agents-environment-and-rules","position":13},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Add Agents","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#add-agents","position":14},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Add Agents","lvl2":"Schelling in Agents.jl"},"content":"Now we have fully specified the behavior of our ABM, but we have a problem...\n\nWe don’t have any agents!!\n\nTo add agents, we’ll use the add_agent_single!(::SchellingAgent, model) function, which is provided by Agents.jl and knows how to place non-overlapping agents in our environment\n\nThis will set the pos field for our agents\n\nSo that we can run many experiements, we’ll actually create a helper function that will create a new model from scratch and add agents to it\n\nfunction init_schelling(;num_agents_per_group=250)\n\tenvironment = GridSpaceSingle((25, 25), periodic=false)\n\tproperties = Dict(:wanted_neighbors => 4)\n\tmodel = ABM(SchellingAgent, environment; properties=properties, agent_step! = agent_step!)\n\t\n\tid = 0\n\tfor group in 1:2, i in 1:num_agents_per_group\n\t\tagent = SchellingAgent(id+=1, (1, 1), false, group)\n\t\tadd_agent_single!(agent, model)\n\tend\n\tmodel\nend\n\nmodel = init_schelling()\n\n","type":"content","url":"/l06-03-shelling-agents#add-agents","position":15},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Running the Model","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#running-the-model","position":16},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Running the Model","lvl2":"Schelling in Agents.jl"},"content":"To run our model, we need to step forward in time\n\nWe do this using the step! function provided by Agents.jl\n\nThis function will iterate over all the agents in the model and call agent_step! for each of them\n\n# advance one step\nstep!(model)\n\n# advance three steps\nstep!(model, 3)\n\nWe can also use the run function\n\nThis function requires a model, agent_step! function, number of steps and array of agent property names to record\n\nThe output is a DataFrame with all the data\n\nmodel = init_schelling()\nadata = [:pos, :is_happy, :group]  # short for agent data\ndata, _ = run!(model, 10; adata)\ndata\n\n","type":"content","url":"/l06-03-shelling-agents#running-the-model","position":17},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Visualizing the Agents","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#visualizing-the-agents","position":18},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Visualizing the Agents","lvl2":"Schelling in Agents.jl"},"content":"One of the more instructive (and fun!) parts of agent based modeling is visualizing the data\n\nTo do this visualization we will use the abmplot function\n\nusing CairoMakie\n\nagent_color(a) = a.group == 1 ? :blue : :orange\nagent_marker(a) = a.group == 1 ? :circle : :rect\nfigure, _ = abmplot(model; agent_color, agent_marker, agent_size = 10)\nfigure # returning the figure displays it\n\n","type":"content","url":"/l06-03-shelling-agents#visualizing-the-agents","position":19},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Animating the Agents","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#animating-the-agents","position":20},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Animating the Agents","lvl2":"Schelling in Agents.jl"},"content":"We can also create a video that animates our agents moving throughout the system\n\nWe do this using the abmvideo function as follows\n\nmodel = init_schelling();\nabmvideo(\n    \"schelling.mp4\", model;\n    agent_color, agent_marker, agent_size = 10,\n    framerate = 4, frames = 20,\n    title = \"Schelling's segregation model\"\n)\n\n","type":"content","url":"/l06-03-shelling-agents#animating-the-agents","position":21},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Interactive Application","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#interactive-application","position":22},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Interactive Application","lvl2":"Schelling in Agents.jl"},"content":"Agents.jl also makes it very easy to create an interactive application for our model!\n\nWe can do this using the abmexploration function\n\nThis expects a single positional argument:\n\nmodel\n\nWe also have some keyword arguments\n\nparams: Dict mapping model parameters to range of values to test\n\nagent_color, agent_marker, agent_size: control marker color color, symbol, and size as before\n\nadata: Array of (agent_property, summary_func) tuples that specify which data to plot in separate charts\n\nalabels: What label to put on the plots for adata\n\nmodel = init_schelling()\nadata = [(:is_happy, sum)]\nalabels = [\"n_happy\"]\nparameter_range = Dict(:wanted_neighbors => 0:8)\nfigure, abmobs = abmexploration(\n    model;\n    adata, alabels,\n    agent_color, agent_marker, agent_size = 10,\n    params=parameter_range,\n)\nfigure\n\n","type":"content","url":"/l06-03-shelling-agents#interactive-application","position":23},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Run from REPL","lvl2":"Schelling in Agents.jl"},"type":"lvl3","url":"/l06-03-shelling-agents#run-from-repl","position":24},{"hierarchy":{"lvl1":"Schelling Model with Agents.jl","lvl3":"Run from REPL","lvl2":"Schelling in Agents.jl"},"content":"In order to use the interactive app we need to run the code from the Julia REPL (not inside vs code or jupyter)\n\nWe’ll do that now","type":"content","url":"/l06-03-shelling-agents#run-from-repl","position":25},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice"},"type":"lvl1","url":"/l07-01-abm-money","position":0},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice"},"content":"Computational Analysis of Social Complexity\n\nPrerequisites\n\nABM\n\nShelling segregation model\n\nOutcomes\n\nRecall key components of ABM\n\nReview additional example of ABM in Julia\n\nReferences\n\nhttps://​proceedings​.scipy​.org​/articles​/Majora​-7b98e3ed​-009​.pdf\n\nhttps://​juliadynamics​.github​.io​/Agents​.jl​/stable/\n\nhttps://​juliadynamics​.github​.io​/AgentsExampleZoo​.jl​/dev​/examples​/wealth​_distribution/\n\n","type":"content","url":"/l07-01-abm-money","position":1},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Agent Based Models"},"type":"lvl2","url":"/l07-01-abm-money#agent-based-models","position":2},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Agent Based Models"},"content":"Agent based models are...\n\nAn approximation to some complex system (a model)\n\nUsed in various fields including biology, epidemiology, economics, etc.\n\nComposed of three key elements: (1) Agents (2) Environment (3) Rules\n\n","type":"content","url":"/l07-01-abm-money#agent-based-models","position":3},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Shelling Segregation Model"},"type":"lvl2","url":"/l07-01-abm-money#shelling-segregation-model","position":4},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Shelling Segregation Model"},"content":"Recall the Schelling segregation model...\n\nAgents: individuals/families seeking a home in a neighborhood. Have a type and hapiness. Agents are happy if least N of their neighbors are same type\n\nEnvironment: grid of “lots” or homes where agents can live\n\nRules: All unhappy agents move to a new random home in the environment\n\n","type":"content","url":"/l07-01-abm-money#shelling-segregation-model","position":5},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Shelling Takeaways","lvl2":"Shelling Segregation Model"},"type":"lvl3","url":"/l07-01-abm-money#shelling-takeaways","position":6},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Shelling Takeaways","lvl2":"Shelling Segregation Model"},"content":"Very simplistic view of agents (people) and decision making criterion (rules)\n\nAgents only considered immediate neighbors when deciding to move (locality)\n\nSimplistic, local behavior led to stark aggregate results: segregation of neighborhoods into agent types\n\nAgents.jl implementation very straightforward: struct to represent agent, struct (Agents.jl defined) to represent environment, function to represent rules for single agent\n\n","type":"content","url":"/l07-01-abm-money#shelling-takeaways","position":7},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Plan today"},"type":"lvl2","url":"/l07-01-abm-money#plan-today","position":8},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Plan today"},"content":"See example of second model\n\nBreak into groups and study example models from the \n\nAgents.jl model zoo\n\nPresent model your group studied\n\n","type":"content","url":"/l07-01-abm-money#plan-today","position":9},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Money Model"},"type":"lvl2","url":"/l07-01-abm-money#money-model","position":10},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Money Model"},"content":"Agents:\n\nN Agents\n\nAll start with 1.0 wealth\n\nEnvironment: none -- they just exist ;)\n\nRules:\n\nIf agent has at least 1.0 wealth, gives 1.0 wealth to another agent\n\nIf agent has 0 wealth, does nothing\n\n","type":"content","url":"/l07-01-abm-money#money-model","position":11},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Agents","lvl2":"Money Model"},"type":"lvl3","url":"/l07-01-abm-money#agents","position":12},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Agents","lvl2":"Money Model"},"content":"\n\n# import Pkg\n# Pkg.activate(\".\")\n# Pkg.instantiate()\n\nusing Agents, Random, DataFrames\n\n@agent struct MoneyAgent(NoSpaceAgent)\n    wealth::Int\nend\n\n","type":"content","url":"/l07-01-abm-money#agents","position":13},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Rules","lvl2":"Money Model"},"type":"lvl3","url":"/l07-01-abm-money#rules","position":14},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Rules","lvl2":"Money Model"},"content":"\n\nfunction agent_step!(agent, model)\n    if agent.wealth == 0\n        return\n    end\n    recipient = random_agent(model)\n    agent.wealth -= 1\n    recipient.wealth += 1\nend\n\n","type":"content","url":"/l07-01-abm-money#rules","position":15},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Model","lvl2":"Money Model"},"type":"lvl3","url":"/l07-01-abm-money#model","position":16},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Model","lvl2":"Money Model"},"content":"\n\nfunction money_model(; N = 100)\n    m = ABM(MoneyAgent; agent_step!)\n    for _ in 1:N\n        add_agent!(m, 1)\n    end\n    return m\nend\n\nmoney_model()\n\n","type":"content","url":"/l07-01-abm-money#model","position":17},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Simulation","lvl2":"Money Model"},"type":"lvl3","url":"/l07-01-abm-money#simulation","position":18},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl3":"Simulation","lvl2":"Money Model"},"content":"\n\nm = money_model(N=2000)\nadata = [:wealth]\ndf, _ = run!(m, 10; adata)\n\nusing CairoMakie\n\nhist(\n    filter(x -> x.time == 10, df).wealth;\n    bins = collect(0:9),\n    color = cgrad(:viridis)[28:28:256],\n)\n\nAbove we see the famous “power law” pattern\n\nThis is a common result in many areas of economics: that activity or wealth is concentrated in the very few (the 1%)\n\nReferences: \n\nGabaix (2016), \n\nhttps://​www​.sciencedirect​.com​/science​/article​/abs​/pii​/S0378437197002173, \n\nhttps://​www​.theguardian​.com​/commentisfree​/2011​/nov​/11​/occupy​-movement​-wealth​-power​-law​-distribution\n\n","type":"content","url":"/l07-01-abm-money#simulation","position":19},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Money Model with space"},"type":"lvl2","url":"/l07-01-abm-money#money-model-with-space","position":20},{"hierarchy":{"lvl1":"Agent Based Models Review/Practice","lvl2":"Money Model with space"},"content":"Let’s expand our environment to a 2d-grid and only allow sharing wealth amongst neighbors\n\nChanges relative to previous setup:\n\nAgents now have a position\n\nEnvironment is 2d GridSpace\n\nAgents randomly pick a neighbor to give money to\n\n@agent struct WealthInSpace(GridAgent{2})\n    wealth::Int\nend\n\nfunction agent_step!(agent::WealthInSpace, model)\n    if agent.wealth == 0\n        return\n    end\n\n    recipient = rand(collect(nearby_agents(agent, model)))\n    agent.wealth -= 1\n    recipient.wealth += 1\nend\n\nfunction money_model_2d(; dims = (25, 25))\n    space = GridSpace(dims, periodic = true)\n    model = ABM(WealthInSpace, space; scheduler = Schedulers.Randomly(), agent_step! = agent_step!)\n\n    fill_space!(model, 1)\n    return model\nend\n\nm2d = money_model_2d()\nadata2d = [:wealth, :pos]\ndf2d, _ = run!(m2d, 10; adata=adata2d)\n\nhist(\n    filter(x -> x.time == 10, df2d).wealth;\n    bins = collect(0:9),\n    color = cgrad(:viridis)[28:28:256],\n)\n\nStill a power law... very pervasive!\n\nfunction make_heatmap(model, df, T=maximum(df.step))\n    df_T = filter(x -> x.time == T, df)\n\n    x = combine(groupby(df_T, :pos), :wealth => sum)\n    arr = zeros(Int, size(getfield(model, :space)))\n\n    for r in eachrow(x)\n        arr[r.pos...] += r.wealth_sum\n    end\n\n    figure = Figure(; size = (600, 450))\n    hmap_l = figure[1, 1] = Axis(figure, title=\"T= $T\")\n    hmap = heatmap!(hmap_l, arr; colormap = cgrad(:default))\n    cbar = figure[1, 2] = Colorbar(figure, hmap; width = 30)\n    return figure\nend\n\nfor t in 0:10\n    display(make_heatmap(m2d, df2d, t))\nend","type":"content","url":"/l07-01-abm-money#money-model-with-space","position":21},{"hierarchy":{"lvl1":"Game Theory 1"},"type":"lvl1","url":"/l08-01-game-theory-intro","position":0},{"hierarchy":{"lvl1":"Game Theory 1"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nJulia basics\n\nOutcomes\n\nUnderstand the basic structure of a Game\n\nBe able to identify any Nash Equilibria in pure strategies for a normal form game\n\nUnderstand how normal form and extensive form games are related\n\nReferences\n\nEasley and Kleinberg chapter 6\n\n","type":"content","url":"/l08-01-game-theory-intro","position":1},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Introduction"},"type":"lvl2","url":"/l08-01-game-theory-intro#introduction","position":2},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Introduction"},"content":"Computational social science analyzes the connectedness of natural, social, and technological systems\n\nGraph theory (networks) has helped us understand how the structure of relationships influence outcomes\n\nWe now turn to how behaviors, incentives, and strategies influence choices (and thus outcomes)\n\nThe study of how entities make strategic choices in settings where outcomes depend on individual choices and the choices of others is called game theory\n\nGame theory is a very rich topic at the intersection of mathematics and economics\n\nWe will study key concepts, but will not cover them in detail or exhaustively\n\n","type":"content","url":"/l08-01-game-theory-intro#introduction","position":3},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"What is a Game?"},"type":"lvl2","url":"/l08-01-game-theory-intro#what-is-a-game","position":4},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"What is a Game?"},"content":"A game is a description of a strategic environment composed of three elements:\n\nA finite set of N players\n\nFor each player i, a set of feasible actions S_i\n\nDefine \\Sigma = \\times_i S_i as action space  and \\sigma as typical element\n\nFor each player i, a payoff function p_i:\\Sigma \\rightarrow \\mathbb{R}\n\nTo help with notation we’ll focus on two player games (N=2)\n\nWe’ll also start by considering that each player has a discrete set of actions (WLOG call them 1 \\dots M_i for player i)\n\nBasic concepts and definitions can be naturally extended to cases where N>2\n\n","type":"content","url":"/l08-01-game-theory-intro#what-is-a-game","position":5},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Example: Prisoner’s Dilemma"},"type":"lvl2","url":"/l08-01-game-theory-intro#example-prisoners-dilemma","position":6},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Example: Prisoner’s Dilemma"},"content":"A very famous example of a game is called the prisoner’s dilemma\n\nStory: two robbery suspects brought in for questioning\n\nInvestigator immediately separates them and gives both the same deal\n\nIf you confess and your partner doesn’t, you go free and he gets 10 years. If you both confess you each get 4 years, and if neither confesses you each get 1 year.\n\nThe payoffs for this game can be summarized as follows:\n\n\nEach table entry has two items\n\nIn terms of our definition of a game we have...\n\nN = 2 players\n\nStrategys: S_i = \\{\\text{not confess}, \\text{ confess} \\} for both players\n\nPayoffs p_i as specified in the table\n\n","type":"content","url":"/l08-01-game-theory-intro#example-prisoners-dilemma","position":7},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Payoffs as matrices","lvl2":"Example: Prisoner’s Dilemma"},"type":"lvl3","url":"/l08-01-game-theory-intro#payoffs-as-matrices","position":8},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Payoffs as matrices","lvl2":"Example: Prisoner’s Dilemma"},"content":"A common representation of payoffs for a single player is a matrix called the payoff matrix P_i \\in \\mathbb{R}^{M_1 \\times M_2}\n\nThe row i, column j element gives the payoff when player i chooses the ith action in S_1 and player j chooses the jth action in S_2\n\nFor the Prisoner’s Dilemma game above, we have\n\npd_p1 = [-1 -10; 0 -4]\npd_p2 = [-1 0 ;-10 -4]\n\n","type":"content","url":"/l08-01-game-theory-intro#payoffs-as-matrices","position":9},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Best Responses","lvl2":"Example: Prisoner’s Dilemma"},"type":"lvl3","url":"/l08-01-game-theory-intro#best-responses","position":10},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Best Responses","lvl2":"Example: Prisoner’s Dilemma"},"content":"What would happen in the Prisoner’s dilemma game?\n\nYou may think that these partners in crime would like to stick together and get a total of 1 year each by not confessing\n\nHowever, that doesn’t happen\n\nThe investigator knows game theory and rigged the game against them...\n\n","type":"content","url":"/l08-01-game-theory-intro#best-responses","position":11},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"What should Suspect 1 Do?","lvl3":"Best Responses","lvl2":"Example: Prisoner’s Dilemma"},"type":"lvl4","url":"/l08-01-game-theory-intro#what-should-suspect-1-do","position":12},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"What should Suspect 1 Do?","lvl3":"Best Responses","lvl2":"Example: Prisoner’s Dilemma"},"content":"Let’s consider suspect 1\n\npd_p1\n\nSuppose suspect 1 believes suspect 2 will not confess\n\nSuspect 1 now faces the first column of pd_p1 and sees he’s better of confessing and getting 0 years instead of -1\n\nWhat if supsect 1 believes suspect 2 will confess?\n\n1 now faces second column and prefers -4 to a -10, so he still chooses to confess\n\nIn either case, suspect 1’s best response is to confess\n\nBecause confess is always a best response, we call it a dominating strategy (in this strictly dominating because it is always strictly better than not confess)\n\n","type":"content","url":"/l08-01-game-theory-intro#what-should-suspect-1-do","position":13},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"How about Suspect 2?","lvl3":"Best Responses","lvl2":"Example: Prisoner’s Dilemma"},"type":"lvl4","url":"/l08-01-game-theory-intro#how-about-suspect-2","position":14},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"How about Suspect 2?","lvl3":"Best Responses","lvl2":"Example: Prisoner’s Dilemma"},"content":"If we look closely as supsect 2’s payoffs we see his game is symmetric to suspect 1’s:\n\npd_p2\n\nNo matter what suspect 1 chooses, suspect 2’s best response is to confess\n\nThe rational outcome is that both players confess and spend 4 years together in prison\n\n","type":"content","url":"/l08-01-game-theory-intro#how-about-suspect-2","position":15},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Nash Equilibrium"},"type":"lvl2","url":"/l08-01-game-theory-intro#nash-equilibrium","position":16},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Nash Equilibrium"},"content":"How did this happen? How is it a rational outcome i.e. an equilibrium?\n\nA famous concept in game theory is called Nash equilbirum (after famous economist John Nash)\n\nDefinition: A strategy \\sigma is a Nash equilibrium if \\sigma_i is a best response to \\sigma_{-i} (everyone else’s actions)\n\nIntuition: A strategy is an Nash equilibrium if after taking into account every one else’s strategies, each player does not want to change their own\n\n","type":"content","url":"/l08-01-game-theory-intro#nash-equilibrium","position":17},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Computing Nash Equilibria","lvl2":"Nash Equilibrium"},"type":"lvl3","url":"/l08-01-game-theory-intro#computing-nash-equilibria","position":18},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Computing Nash Equilibria","lvl2":"Nash Equilibrium"},"content":"There are various algorithms that we can use for computing Nash equilibria\n\nFow now we will utilize the implementation of these algorithms in the GameTheory.jl package\n\nLet’s load it up and create a version of our prisoner’s dilemma game:\n\n# import Pkg; Pkg.add(\"GameTheory\")\n\nusing GameTheory\n\np1 = Player(pd_p1)\n\nGameTheory.jl requires that payoff matrices are always specified from the perspective of the current player\n\nThis means that we need to “reorient” suspect 2’s payoffs such that his actions are noted on the rows\n\nBecuase this is a symmetric game, suspect 2’s payoffs from suspect 2’s perspective looks exactly the same as suspect 1’s payoffs from suspect 1’s perspective\n\nWe can construct our NormalFormGame with two copies of the p1 player above\n\npd_g = NormalFormGame([p1, p1])\n\nWe can now ask GameTheory.jl to compute the nash Equilibria for us\n\nWe’ll use the pure_nash function to do this (we’ll talk about what “pure” means soon)\n\npd_eq = pure_nash(pd_g)\n\nAs we said before, the only equilibrium outcome to this game is that they both confess\n\nWe can see the payoffs each player gets in equilibrium by “indexing” into the game using the strategy array\n\nThe two expressions below are equivalent in this case\n\npd_g[pd_eq[1]...]\n\n# ↑ Equivalent to ↓\npd_g[2, 2]\n\n","type":"content","url":"/l08-01-game-theory-intro#computing-nash-equilibria","position":19},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Non symmetric games"},"type":"lvl2","url":"/l08-01-game-theory-intro#non-symmetric-games","position":20},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Non symmetric games"},"content":"Not all games are symmetric like the prisoner’s dilemma\n\nConsider the following game\n\nTwo players (firms) and two strategies each (sell low price or upscale goods)\n\n60% of total spending comes from people who prefer low prices\n\nFirm 1 more popular, so when they compete in same segment, firm 1 gets 80% of market\n\nBelow you find the payoff matrix in units of “% of total possible profit”\n\n\n","type":"content","url":"/l08-01-game-theory-intro#non-symmetric-games","position":21},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Strategies","lvl2":"Non symmetric games"},"type":"lvl3","url":"/l08-01-game-theory-intro#strategies","position":22},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Strategies","lvl2":"Non symmetric games"},"content":"Firm 1 has a dominant strategy: low-priced. They will always play this strategy\n\nFirm 2 is less clear:\n\nIf firm 1 were to choose the upscale market, they would be better off choosing low-priced\n\nhowever, when firm 1 chooses low-priced, firm 2 best response is upscale\n\nHow to find equilbirum?\n\n","type":"content","url":"/l08-01-game-theory-intro#strategies","position":23},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"Iterated Deletion of Dominated Strategies","lvl3":"Strategies","lvl2":"Non symmetric games"},"type":"lvl4","url":"/l08-01-game-theory-intro#iterated-deletion-of-dominated-strategies","position":24},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"Iterated Deletion of Dominated Strategies","lvl3":"Strategies","lvl2":"Non symmetric games"},"content":"An algorithm that can help find the solution to this game is called iterated deletion of dominated strategies\n\nThe algorithm proceeds as follows:\n\nSet iteration n = 0\n\nLet S_i^n be set of remaining actions for player i on iteration n. Start S_i^0 = S_i\n\nOn iteration n, for each player i remove from S_i^n any strategies that are dominated by other strategies in S_i^n (taking into account S_{-i}^n). Call surviving strategies S_i^{n+1}\n\nRepeat for all players i\n\nRepeat until one of two conditions is met:\n\nEach player has only one remaining strategy: |S_i^{n+1}| = 1 \\forall i -- this is NE\n\nOne or more players has an empty strategy set\n\n","type":"content","url":"/l08-01-game-theory-intro#iterated-deletion-of-dominated-strategies","position":25},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"Application to Marketing Game","lvl3":"Strategies","lvl2":"Non symmetric games"},"type":"lvl4","url":"/l08-01-game-theory-intro#application-to-marketing-game","position":26},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"Application to Marketing Game","lvl3":"Strategies","lvl2":"Non symmetric games"},"content":"Applying this algorithm we start with S_1^0 = \\{1, 2\\} \\; S_2^0 = \\{1, 2\\}\n\nWe see form firm 1 it is optimal to play strategy 1 for any choice of firm 2, which causes us to delete 2. Now we have S_1^1 = \\{1 \\} \\; S_2^1 = \\{1, 2\\}\n\nNow firm 2 takes into account that 1 will play 1 -- only best response is to play 2 and we get S_1^2 = \\{1 \\} \\; S_2^2 = \\{2\\}\n\nWe are done!\n\nThe unique Nash Equilibrium is for firm 1 to take the low-price segment and firm 2 to take the upscale segment\n“”\"\n\n","type":"content","url":"/l08-01-game-theory-intro#application-to-marketing-game","position":27},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"Exercise","lvl3":"Strategies","lvl2":"Non symmetric games"},"type":"lvl4","url":"/l08-01-game-theory-intro#exercise","position":28},{"hierarchy":{"lvl1":"Game Theory 1","lvl4":"Exercise","lvl3":"Strategies","lvl2":"Non symmetric games"},"content":"Construct the Marketing Game using GameTheory.jl\n\nVerify that the only pure strategy nash equilibrium is [1, 2]\n\nHINT: don’t forget to write player 2’s payoffs from player 2’s perspective!\n\np1_market = Player([0.0 0; 0 0])\np2_market = Player([0.0 0; 0 0])\ng_market = NormalFormGame([p1_market, p2_market])\n\n","type":"content","url":"/l08-01-game-theory-intro#exercise","position":29},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Matching Pennies"},"type":"lvl2","url":"/l08-01-game-theory-intro#matching-pennies","position":30},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Matching Pennies"},"content":"Consider the payoff matrices for another famous game called Matching Pennies\n\npennies_p1 = [-1 1; 1 -1]\npennies_p2 = [1 -1; -1 1]\n\npennies_p1, pennies_p2\n\nQuestion: how many players are there?\n\nHow many strategies does player 1 have? Player 2?\n\n","type":"content","url":"/l08-01-game-theory-intro#matching-pennies","position":31},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"More Questions","lvl2":"Matching Pennies"},"type":"lvl3","url":"/l08-01-game-theory-intro#more-questions","position":32},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"More Questions","lvl2":"Matching Pennies"},"content":"Does player 1 have a dominating strategy?\n\nHow about player 2?\n\nWhat is player 1’s best response when 2 chooses T? What about when 2 chooses H?\n\n","type":"content","url":"/l08-01-game-theory-intro#more-questions","position":33},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Pure Strategies","lvl2":"Matching Pennies"},"type":"lvl3","url":"/l08-01-game-theory-intro#pure-strategies","position":34},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Pure Strategies","lvl2":"Matching Pennies"},"content":"Choosing a strategy outright is called playing a pure strategy\n\nNeither player will always choose H or T no matter what the other player does\n\nWe can say that there is no Nash Equilibrium in pure strategies\n\nHowever, for all games we will consider (and most games in general) there is always a Nash equilibrium...\n\n","type":"content","url":"/l08-01-game-theory-intro#pure-strategies","position":35},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Mixed Strategies","lvl2":"Matching Pennies"},"type":"lvl3","url":"/l08-01-game-theory-intro#mixed-strategies","position":36},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Mixed Strategies","lvl2":"Matching Pennies"},"content":"Sometimes players will not be able to play pure strategies in eqiulibrium\n\nIn these cases they will need to randomize their behavior\n\nA mixed strategy is a probability distribution over strategies\n\nFor example, in the matching pennies game, a mixed strategy is to play H with probabilty 0.5 and T with probability 0.5\n\nIt turns out that both players playing this mixed strategy is the unique Nash Equilibrium of the matching pennies game\n\n","type":"content","url":"/l08-01-game-theory-intro#mixed-strategies","position":37},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Mixed Strategies with GameTheory.jl"},"type":"lvl2","url":"/l08-01-game-theory-intro#mixed-strategies-with-gametheory-jl","position":38},{"hierarchy":{"lvl1":"Game Theory 1","lvl2":"Mixed Strategies with GameTheory.jl"},"content":"GameTheory.jl can compute mixed strategy nash equilibria for us\n\nTo do that we’ll use the support_enumeration method (support enumeration is the name of an algorithm for computing all NE of a game, in pure or mixed strategies)\n\n","type":"content","url":"/l08-01-game-theory-intro#mixed-strategies-with-gametheory-jl","position":39},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Bimatrix","lvl2":"Mixed Strategies with GameTheory.jl"},"type":"lvl3","url":"/l08-01-game-theory-intro#bimatrix","position":40},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Bimatrix","lvl2":"Mixed Strategies with GameTheory.jl"},"content":"Before we have GameTheory compute our mixed strategy NE, we’ll show one other way to create a NormalFormGame -- with a payoff bimatrix\n\nFor an N player game with N_i strategies for each player, a bimatrix is an N_1 \\times N_2 \\times \\cdots \\times N_N \\times N array of payoffs\n\nFor our game, we need a 2x2x2 array\n\nlast 2 represents 2 players\n\nfirst two 2’s represent 2 actions per player\n\npennies_bimatrix = zeros(2, 2, 2)\npennies_bimatrix[1, 1, :] = [-1, 1]\npennies_bimatrix[1, 2, :]  = [1, -1]\npennies_bimatrix[2, 1, :] = [1, -1]\npennies_bimatrix[2, 2, :] = [-1, 1]\npennies_g = NormalFormGame(pennies_bimatrix)\n\nNotice how when using a bimatrix we can directly read the cells of the normal form game\n\nThe (H,H) cell is in position [1,1] and has payoffs [-1, 1]\n\nThe (T, H) cell is in position [2, 1] and has payoffs [1, -1]\n\netc.\n\nThis can make it easier to specify payoffs because we don’t have to worry about “player N payoffs from player N’s perspective”\n\nsupport_enumeration(pennies_g)\n\n","type":"content","url":"/l08-01-game-theory-intro#bimatrix","position":41},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Exercise","lvl2":"Mixed Strategies with GameTheory.jl"},"type":"lvl3","url":"/l08-01-game-theory-intro#exercise-1","position":42},{"hierarchy":{"lvl1":"Game Theory 1","lvl3":"Exercise","lvl2":"Mixed Strategies with GameTheory.jl"},"content":"Try support_enumeration with the other two games we’ve worked with\n\nWhat does it give you with the prisoner’s dilemma?\n\nWhat does it give you with the marketing game?\n\n# TODO: your code AND explanation here","type":"content","url":"/l08-01-game-theory-intro#exercise-1","position":43},{"hierarchy":{"lvl1":"Network Traffic with Game Theory"},"type":"lvl1","url":"/l08-02-network-traffic","position":0},{"hierarchy":{"lvl1":"Network Traffic with Game Theory"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nNetworks\n\nGame Theory\n\nOutcomes\n\nRepresent network traffic weighted DiGraph\n\nAnalyze equilibrium network outcomes using the concept of Nash Equilibirum\n\nUnderstand Braes’ paradox\n\nLearn about the concept of social welfare and a social planners\n\nReferences\n\nEasley and Kleinberg chapter 8\n\n","type":"content","url":"/l08-02-network-traffic","position":1},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"Congestion"},"type":"lvl2","url":"/l08-02-network-traffic#congestion","position":2},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"Congestion"},"content":"We regularly use physical networks of all kinds\n\nPower grids\n\nThe internet\n\nStreets\n\nRailroads\n\nWhat happens when the networks get congested?\n\nTypically -- flow across the network slows down\n\nToday we’ll study how game theoretic ideas are helpful when analyzing how a network with finite capacity or increasing costs\n\n","type":"content","url":"/l08-02-network-traffic#congestion","position":3},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"A Traffic Network"},"type":"lvl2","url":"/l08-02-network-traffic#a-traffic-network","position":4},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"A Traffic Network"},"content":"We’ll start by considering a traffic network\n\nThe figure caption has extra detail -- so be sure to read it!\n\n\nWe’ll write up some helper Julia functions that will let us create and visualize the traffic network for arbitrary values of x\n\nusing Graphs, SimpleWeightedGraphs, GraphPlot, Interact\n\nfunction traffic_graph1(x)\n    A = [\n        0 0 x/100 45;\n        0 0 0 0;\n        0 45 0 0;\n        0 x/100 0 0\n        ]\n    SimpleWeightedDiGraph(A)\nend\n\nfunction plot_traffic_graph(g::SimpleWeightedDiGraph)\n    locs_x = [1.0, 3, 2, 2]\n    locs_y = [1.0, 1, 0, 2]\n    labels = collect('A':'Z')[1:nv(g)]\n    gplot(g, locs_x, locs_y, nodelabel=labels, edgelabel=weight.(edges(g)))\nend\n\n@manipulate for x=10:10:4000\n    plot_traffic_graph(traffic_graph1(x))\nend\n\nPlay around with the slider and watch the weights on our graph change\n\n","type":"content","url":"/l08-02-network-traffic#a-traffic-network","position":5},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"The Game","lvl2":"A Traffic Network"},"type":"lvl3","url":"/l08-02-network-traffic#the-game","position":6},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"The Game","lvl2":"A Traffic Network"},"content":"Now suppose, as indicated in the figure caption, that we have 4,000 drivers that need to commute from A to B in the morning\n\nIf all take the upper route (A-C-B) we get a total time of 40 + 45 = 85 minutes\n\nIf all take the lower route (A-D-B) we get a total time of 40 + 45 = 85 minutes\n\nIf, however, they evenly divide we get a total time of 20 + 45 = 65 minutes\n\n","type":"content","url":"/l08-02-network-traffic#the-game","position":7},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Equilibrium","lvl2":"A Traffic Network"},"type":"lvl3","url":"/l08-02-network-traffic#equilibrium","position":8},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Equilibrium","lvl2":"A Traffic Network"},"content":"Recall that for a set of strategies (here driving paths) to be a Nash Equilibrium, each player’s strategy must be a best response to the strategy of all other players\n\nWe’ll argue that the only NE of this commuting game is that 2,000 drivers take (A-C-B) and 2,000 take (A-D-B) and everyone takes 65 mintues to commute\n\n","type":"content","url":"/l08-02-network-traffic#equilibrium","position":9},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Exercise","lvl2":"A Traffic Network"},"type":"lvl3","url":"/l08-02-network-traffic#exercise","position":10},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Exercise","lvl2":"A Traffic Network"},"content":"Show that this strategy (2,000 drivers take (A-C-B) and 2,000 take (A-D-B)) is indeed a Nash equilibrium\n\nTo do this recognize that the game is symmetric for all drivers\n\nThen, argue that if 3,999 drivers are following that strategy, the best response for the last driver is also to follow the strategy\n\nYour work HERE!\n\n","type":"content","url":"/l08-02-network-traffic#exercise","position":11},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Discussion","lvl2":"A Traffic Network"},"type":"lvl3","url":"/l08-02-network-traffic#discussion","position":12},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Discussion","lvl2":"A Traffic Network"},"content":"Note a powerful outcome here -- without any coordination by a central authority, drivers  will automatically balance perfectly in equilibrium\n\nThe only assumptions we made were:\n\nDrivers want to minimize driving time\n\nDrivers are allowed to respond to the decisions of others\n\nThe first assumption is very plausable -- nobody wants to sit in more traffic than necessary\n\nThe second highlights a key facet of our modern society...\n\nInformation availability (here decisions of other drivers) can (and does!) lead to optimal outcomes without the need for further regulation or policing\n\n","type":"content","url":"/l08-02-network-traffic#discussion","position":13},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"Adding a “warp tunnel”"},"type":"lvl2","url":"/l08-02-network-traffic#adding-a-warp-tunnel","position":14},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"Adding a “warp tunnel”"},"content":"Now suppose that we modify the network and add a new edge between C-D that has zero cost\n\nEffectively we add a wormhole that connects C to D\n\nfunction traffic_graph2(x)\n    G = traffic_graph1(x)\n    # need to add an edge with minimal weight so it shows up in plot\n    add_edge!(G, 3, 4, 1e-16)\n    G\nend\n\n@manipulate for x2=10:10:4000\n    plot_traffic_graph(traffic_graph2(x2))\nend\n\n","type":"content","url":"/l08-02-network-traffic#adding-a-warp-tunnel","position":15},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Exercise","lvl2":"Adding a “warp tunnel”"},"type":"lvl3","url":"/l08-02-network-traffic#exercise-1","position":16},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Exercise","lvl2":"Adding a “warp tunnel”"},"content":"Is a 50/50 split of traffic still a Nash equilibrium in this case?\n\nWhy or why not?\n\nIs all 4,000 drivers doing (A-C-D-B) a Nash equilibrium?\n\nWhy or why not?\n\n","type":"content","url":"/l08-02-network-traffic#exercise-1","position":17},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Braes’ Paradox","lvl2":"Adding a “warp tunnel”"},"type":"lvl3","url":"/l08-02-network-traffic#braes-paradox","position":18},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Braes’ Paradox","lvl2":"Adding a “warp tunnel”"},"content":"In the previous exercise, we saw a rather startling result...\n\nDoing a network “upgrade” -- adding a wormhole connecting C and D -- resulted in a worse equilibrium outcome for everyone!\n\nThe equilbirum driving time is now 80 mintues for all drivers instead of 65 minutes (which was the case before the wormhole)\n\nThis is known as Braes’ paradox\n\n","type":"content","url":"/l08-02-network-traffic#braes-paradox","position":19},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Follow ups","lvl2":"Adding a “warp tunnel”"},"type":"lvl3","url":"/l08-02-network-traffic#follow-ups","position":20},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Follow ups","lvl2":"Adding a “warp tunnel”"},"content":"Braes’ paradox was the starting point for a large body of research on using game theory to analyze network traffic\n\nSome questions that have been asked are:\n\nHow much larger can equilibirum travel time increase after a network upgrade?\n\nHow can network upgrade be designed to be resilient to Braes’ paradox?\n\n","type":"content","url":"/l08-02-network-traffic#follow-ups","position":21},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"Social Welfare"},"type":"lvl2","url":"/l08-02-network-traffic#social-welfare","position":22},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl2":"Social Welfare"},"content":"Many economic models are composed of individual actors who make autonomous decisions and have autonomous payoffs\n\nWe’ve been studying some of these settings using tools from game theory, focusing on the individual perspective\n\nOur notion of equilibrium is dependent on no individual wanting to change strategy in response to other strategies\n\nAnother form of analysis works at the macro level -- we analyze the total payoff for all agents (i.e. sum of payoffs)\n\nWe call this aggregate payoff social welfare\n\n","type":"content","url":"/l08-02-network-traffic#social-welfare","position":23},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"The Social Planner","lvl2":"Social Welfare"},"type":"lvl3","url":"/l08-02-network-traffic#the-social-planner","position":24},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"The Social Planner","lvl2":"Social Welfare"},"content":"In an economic model, someone who seeks to maximize social welfare is called a social planner\n\nA social planner is given the authority to make decisions for all agents\n\nIn our traffic model, a social planner would choose to ignore the wormhole and have 1/2 the drivers take A-C-B and the other half take A-D-B\n\nIn this case everyone would be better off with a cost of 65 minutes instead of the equilibrium 80 minutes\n\n","type":"content","url":"/l08-02-network-traffic#the-social-planner","position":25},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Cost of Freedom","lvl2":"Social Welfare"},"type":"lvl3","url":"/l08-02-network-traffic#cost-of-freedom","position":26},{"hierarchy":{"lvl1":"Network Traffic with Game Theory","lvl3":"Cost of Freedom","lvl2":"Social Welfare"},"content":"Question: in a generic traffic model, how much worse can the equilibrium outcome be than the social optimium?\n\nIn our example,\n\nOptimal social welfare is 4000 * 65 = $(4000*65)\n\nEquilibrium social welfare is 4000 * 80 = $(4000*80)\n\nA change of $(4000*15)\n\nTo answer this question for a general traffic model, we need to be able to compute the equilibrium for a generic traffic model\n\nWe may study this next week, or perhaps even on your homework 😉","type":"content","url":"/l08-02-network-traffic#cost-of-freedom","position":27},{"hierarchy":{"lvl1":"More Game Theory"},"type":"lvl1","url":"/l09-01-mixed-strategies","position":0},{"hierarchy":{"lvl1":"More Game Theory"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nIntro to Game Theory\n\nOutcomes\n\nKnow how to solve for a Nash Equilibrium in mixed strategies\n\nUnderstand the concepts of a repeated game and be able to reason about equilibria in such games\n\nUnderstand how normal form and extensive form games are related\n\nReferences\n\nEasley and Kleinberg chapter 6\n\n","type":"content","url":"/l09-01-mixed-strategies","position":1},{"hierarchy":{"lvl1":"More Game Theory","lvl2":"Review: Matching Pennies"},"type":"lvl2","url":"/l09-01-mixed-strategies#review-matching-pennies","position":2},{"hierarchy":{"lvl1":"More Game Theory","lvl2":"Review: Matching Pennies"},"content":"Recall the matching pennies game we studied last week\n\n","type":"content","url":"/l09-01-mixed-strategies#review-matching-pennies","position":3},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Properties","lvl2":"Review: Matching Pennies"},"type":"lvl3","url":"/l09-01-mixed-strategies#properties","position":4},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Properties","lvl2":"Review: Matching Pennies"},"content":"Matching pennies is an example of a wider class of games\n\nSome of its properties are that it is\n\nZero-sum: sum of payoffs from all players is always 0. One player loses, the other wins\n\nNo Nash Equilibrium in Pure Strategies\n\nb/c no NE in PS, beneficial to introduce randomness into actions\n\nZero sum games are very common\n\nExample: D-Day in WW2. US could have landed in France on Normandy or Calais. Germany could have put bulk of defenses at one of these places. Outcome largely depended on US ability to \n\ntrick Germany into putting defenses at Calais\n\n","type":"content","url":"/l09-01-mixed-strategies#properties","position":5},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Mixed Strategies","lvl2":"Review: Matching Pennies"},"type":"lvl3","url":"/l09-01-mixed-strategies#mixed-strategies","position":6},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Mixed Strategies","lvl2":"Review: Matching Pennies"},"content":"A pure strategy is a selection of a single action from set of possible strategies \\sigma_i  \\in S_i\n\nA mixed strategy is a probability distribution over the set of possible strategies \\sigma_i \\in \\triangle(S_i) \\subseteq [0,1]^{M_i}\n\nPure strategy is a special case of a mixed strategy -- a degenerate distribution\n\n","type":"content","url":"/l09-01-mixed-strategies#mixed-strategies","position":7},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Expected Payoffs","lvl2":"Review: Matching Pennies"},"type":"lvl3","url":"/l09-01-mixed-strategies#expected-payoffs","position":8},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Expected Payoffs","lvl2":"Review: Matching Pennies"},"content":"When dealing with pure strategies we could determine payoffs by reading off an appropriate value from the payoff matrix\n\nWith mixed strategies we have to deal with expected payoffs\n\n","type":"content","url":"/l09-01-mixed-strategies#expected-payoffs","position":9},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"P1  Payoffs in Matching Pennies","lvl2":"Review: Matching Pennies"},"type":"lvl3","url":"/l09-01-mixed-strategies#p1-payoffs-in-matching-pennies","position":10},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"P1  Payoffs in Matching Pennies","lvl2":"Review: Matching Pennies"},"content":"Suppose you are player 1 in matching pennies\n\nSuppose further that P2 is following a strategy to play H with probability q and T with probability (1-q)\n\nThe expected payoffs from each of P1’s pure strategies are:\n\nPlays H: p_1(H | q): -1 \\cdot q + 1 \\cdot (1-q) = 1 - 2q\n\nPlays T: p_1(T | q): 1 \\cdot 1 - 1 \\cdot (1-q) = 2q - 1\n\n","type":"content","url":"/l09-01-mixed-strategies#p1-payoffs-in-matching-pennies","position":11},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Indifference","lvl2":"Review: Matching Pennies"},"type":"lvl3","url":"/l09-01-mixed-strategies#indifference","position":12},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Indifference","lvl2":"Review: Matching Pennies"},"content":"Argument: it cannot be optimal for P2 to follow the (q, 1-q) strategy unless it makes P1 indifferent about playing H or T\n\nWhy? Suppose p_1(H | q) > p_1(T | q). Then P1 will always pick H, which would in turn make P2 want to change behavior to always play H\n\nSimlar logic applies if p_1(T | q) > p_1(H | q)\n\nSo, it must be that p_1(T | q) = p_1(H | q)\n\nThis means 1 - 2q = 2q - 1 \\Longrightarrow q = \\frac{1}{2}\n\n","type":"content","url":"/l09-01-mixed-strategies#indifference","position":13},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Comment","lvl2":"Review: Matching Pennies"},"type":"lvl3","url":"/l09-01-mixed-strategies#comment","position":14},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Comment","lvl2":"Review: Matching Pennies"},"content":"Notice that we used P1’s expected payoffs to determine the mixed strategy for P2\n\nThere are two major themes here\n\nTo derive optimal behavior for one player, you must consider impact of that player’s decisions on the rewards to other players\n\nP2’s ability to commit to following the (q, 1-q) both allowed P1 to reason about payoffs AND made P1 indifferent about his/her own choice. Commitment is a major theme in advanced game theory, and one we’ll revisit later in the course when we talk about blockchains and smart contracts\n\n","type":"content","url":"/l09-01-mixed-strategies#comment","position":15},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"P1’s action","lvl2":"Review: Matching Pennies"},"type":"lvl3","url":"/l09-01-mixed-strategies#p1s-action","position":16},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"P1’s action","lvl2":"Review: Matching Pennies"},"content":"We now know (based on P1s expected payoffs) that P2 will do 50/50 split between H and T\n\nNow consider game from P2’s perspective, taking as given that P1 will be playing H with probability p and T with probability 1-p\n\nExercise: use P2’s expected payoffs under the (p, 1-p) strategy for P1 to determine the value of p. What is that value? does it surprise you? Why or Why not?\n\n","type":"content","url":"/l09-01-mixed-strategies#p1s-action","position":17},{"hierarchy":{"lvl1":"More Game Theory","lvl2":"Asymmetry: NFL play choice"},"type":"lvl2","url":"/l09-01-mixed-strategies#asymmetry-nfl-play-choice","position":18},{"hierarchy":{"lvl1":"More Game Theory","lvl2":"Asymmetry: NFL play choice"},"content":"Matching pennies is a particularly simple example of a zero-sum game with no equilibira in pure strategies\n\nThe symmetry in the payoff matrix led to a “boring” outcome\n\nLet’s consider another example that doesn’t have this symmetry\n\nConsider an american football game\n\nEach play\n\nthe offense can choose to call a run or pass play\n\nThe defense can choose to focus play call on defending run or defending pass\n\nThe payoff for offensive team is how many yards they gain\n\nPayoff for defense is always negative and equal to “-” yards gained by offense\n\nPayoffs based on play calls are given below\n\n","type":"content","url":"/l09-01-mixed-strategies#asymmetry-nfl-play-choice","position":19},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Exercise","lvl2":"Asymmetry: NFL play choice"},"type":"lvl3","url":"/l09-01-mixed-strategies#exercise","position":20},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Exercise","lvl2":"Asymmetry: NFL play choice"},"content":"Given what we’ve learned about mixed strategies, determine the equilibrium probability that the defense chooses to defend the pass (call this q) and the equilibrium probability that the offense chooses a pass play (call this p)\n\n","type":"content","url":"/l09-01-mixed-strategies#exercise","position":21},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Comments","lvl2":"Asymmetry: NFL play choice"},"type":"lvl3","url":"/l09-01-mixed-strategies#comments","position":22},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Comments","lvl2":"Asymmetry: NFL play choice"},"content":"In the run/pass example we see that the higher payoff option for the offense is to pass, but that they choose it less than the lower payoff run option\n\nWhy?\n\nThe main idea is that the threat of a successful pass play causes the defense to choose to defend the pass more than 50% of the time.\n\nThe offense takes that strategy as given, and realizes they are better off running more than 1/2 the time\n\nAny deviation by the offense to pass more often than the p you computed would cause the defense to always defend the pass\n\nThis would result in a strictly worse expected payoff for the offense\n\n","type":"content","url":"/l09-01-mixed-strategies#comments","position":23},{"hierarchy":{"lvl1":"More Game Theory","lvl2":"Dynamic Games"},"type":"lvl2","url":"/l09-01-mixed-strategies#dynamic-games","position":24},{"hierarchy":{"lvl1":"More Game Theory","lvl2":"Dynamic Games"},"content":"So far the games we’ve studied have all been static\n\nBy this we mean each participant makes exactly one choice at exactly the same time\n\nGame theory is far more rich than this!\n\nWe now introduce the concept of a dynamic game\n\nOur treatement will focus on non-simultaneous decisions\n\nStudy of repeated games is beyond our scope for now\n\n","type":"content","url":"/l09-01-mixed-strategies#dynamic-games","position":25},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Example: Firm Advertising","lvl2":"Dynamic Games"},"type":"lvl3","url":"/l09-01-mixed-strategies#example-firm-advertising","position":26},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Example: Firm Advertising","lvl2":"Dynamic Games"},"content":"Consider a game with two firms (1 and 2) and two new markets (A and B)\n\nMarket A has a total of 12 profit to be gained and market B has total of 6 profit\n\nBoth firms have to choose which market to advertise to\n\nFirm 1 gets to choose first\n\nBecause of the earlier choice, firm 1 has “first mover advantage”\n\nIf firms choose same market, firm 1 gets 2/3 of potential profit and firm 2 gets 1/3\n\nIf they advertise to separate markets, each firm gets all potential profit in their chosen market\n\n","type":"content","url":"/l09-01-mixed-strategies#example-firm-advertising","position":27},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Extensive form","lvl2":"Dynamic Games"},"type":"lvl3","url":"/l09-01-mixed-strategies#extensive-form","position":28},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Extensive form","lvl2":"Dynamic Games"},"content":"We can represent the game we just described as a tree (or directed graph, if you prefer)\n\nEach node represents a decision point and each branch represents a particular action being chosen\n\nThe game tree for the advertising game is given below\n\n]\n\n","type":"content","url":"/l09-01-mixed-strategies#extensive-form","position":29},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Equilibrium via Game Tree","lvl2":"Dynamic Games"},"type":"lvl3","url":"/l09-01-mixed-strategies#equilibrium-via-game-tree","position":30},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Equilibrium via Game Tree","lvl2":"Dynamic Games"},"content":"We can use the extensive form representation of a dynamic game to determine the equilibrium outcome\n\nThe approach for doing this is to start at the bottom of the tree and work our way up\n\nIn this example we first start with Player 2 and determine what decision should be made at each decision node:\n\nIf on branch from P1 choosing A, P2 shoudl choose B because p_2(B|A) > p_2(A|A) (6 > 4)\n\nIf on branch from P1 choosing B, P2 shoudl choose A because p_2(A|B) > p_2(B|B) (12 > 2)\n\nNow that we know what P2 will do at each node, we go up the tree to P1s decision\n\nP1 knows P2 will choose the opposite of P1’s choice\n\nSo P1 realizes that p_1(A) > p1_(B) (12 > 6), so P1 chooses A\n\nThe equilibrium of this game is (A, B)\n\n","type":"content","url":"/l09-01-mixed-strategies#equilibrium-via-game-tree","position":31},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Example: Market Entry","lvl2":"Dynamic Games"},"type":"lvl3","url":"/l09-01-mixed-strategies#example-market-entry","position":32},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Example: Market Entry","lvl2":"Dynamic Games"},"content":"Let’s consider a similar game\n\nPlayer 2 is now an incumbant (already existing firm) and player 1 is a startup deciding to enter the market or stay out\n\nIf P1 chooses to stay out (S), the game ends and P2 is happy\n\nIf P1 chooses to enter (E), P2 can choose to retaliate or cooperate\n\nPayoffs are given in game tree below\n\n","type":"content","url":"/l09-01-mixed-strategies#example-market-entry","position":33},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Exercise","lvl2":"Dynamic Games"},"type":"lvl3","url":"/l09-01-mixed-strategies#exercise-1","position":34},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Exercise","lvl2":"Dynamic Games"},"content":"Apply the logic outlined above when styding advertising game to determine the equilibrium outcome of the Market Entry Game\n\nWhat does Player 1 choose to do?\n\nDoes player 2 have to make a choice? If so, what is it?\n\n","type":"content","url":"/l09-01-mixed-strategies#exercise-1","position":35},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Comparison to Normal Form","lvl2":"Dynamic Games"},"type":"lvl3","url":"/l09-01-mixed-strategies#comparison-to-normal-form","position":36},{"hierarchy":{"lvl1":"More Game Theory","lvl3":"Comparison to Normal Form","lvl2":"Dynamic Games"},"content":"Consider the market entry game in normal form\n\nNotice that in this normal form game there are two NE in pure strategies:\n\n(S, R): An equilibrium that didn’t come up in the dynamic game, because P1 got to move first and chose to enter\n\n(E, C): the equilibrim we’ve already seen\n\nKey idea: taking into account timing may change equilibrium outcomes\n\nIf P2 had the ability to commit to retaliate, then perhaps P1 would choose to stay out\n\nAgain commitment is a key concept in Game theory\n\nBook talks about “if P2 could commit to having a computer play its strategies”\n\nThis is not just a hypothetical if -- smart contracts make it possible and enforce it!","type":"content","url":"/l09-01-mixed-strategies#comparison-to-normal-form","position":37},{"hierarchy":{"lvl1":"Auctions"},"type":"lvl1","url":"/l09-03-auctions","position":0},{"hierarchy":{"lvl1":"Auctions"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nGame Theory\n\nOutcomes\n\nKnow the 4 main types of auctions\n\nUnderstand concept of individual valuation of an item or an outcome\n\nUnderstand why truth-telling in auctions is optimal\n\nReferences\n\nEasley and Kleinberg chapter 9\n\n","type":"content","url":"/l09-03-auctions","position":1},{"hierarchy":{"lvl1":"Auctions","lvl2":"Intro"},"type":"lvl2","url":"/l09-03-auctions#intro","position":2},{"hierarchy":{"lvl1":"Auctions","lvl2":"Intro"},"content":"An auction is a special type of economic market between a seller and many buyers\n\nThe seller has an item or outcome that -- presumably -- the buyers want\n\nRules are established for how buyers indicate their willingness to pay\n\n","type":"content","url":"/l09-03-auctions#intro","position":3},{"hierarchy":{"lvl1":"Auctions","lvl3":"Why/When auctions?","lvl2":"Intro"},"type":"lvl3","url":"/l09-03-auctions#why-when-auctions","position":4},{"hierarchy":{"lvl1":"Auctions","lvl3":"Why/When auctions?","lvl2":"Intro"},"content":"When would auctions be applicable?\n\nIn a typical buyer-seller scenario usually the value of the good for one party is known\n\nThe buyer knows costs of creating the good and posts a reasonable price\n\nThe seller knows how much they are willing to pay and will purchase if affordable\n\nAn auction is to be used when the valuation of the good is either private information (I, a buyer, don’t want seller to know how badly I want the good) or is unknown\n\n","type":"content","url":"/l09-03-auctions#why-when-auctions","position":5},{"hierarchy":{"lvl1":"Auctions","lvl2":"Types of Auctions"},"type":"lvl2","url":"/l09-03-auctions#types-of-auctions","position":6},{"hierarchy":{"lvl1":"Auctions","lvl2":"Types of Auctions"},"content":"First price, ascending (English): what we see on TV. Auctioneer calls out higher and higher prices, bidders indicate willinness to pay, terminates when nobody outbids current highest bid\n\nDescending-bid (Dutch): Price starts high and falls, bidders are quiet until somebody says they’ll buy at current price, auction ends and bidder pays that price\n\nFirst price, sealed-bid: everyone writes down their bid secretly and submits at the same time, highest bidder wins and pays price they wrote\n\nSecond price, sealed-bid (Vickrey): everyone writes down their bid secretly and submits at the same time, highest bidder wins and pays price of second highest bidder\n\n","type":"content","url":"/l09-03-auctions#types-of-auctions","position":7},{"hierarchy":{"lvl1":"Auctions","lvl3":"Equivalence","lvl2":"Types of Auctions"},"type":"lvl3","url":"/l09-03-auctions#equivalence","position":8},{"hierarchy":{"lvl1":"Auctions","lvl3":"Equivalence","lvl2":"Types of Auctions"},"content":"Because auctions are useful in settings with unknown valuations, we often think about how the rules of an auction lead to revealing information\n\nIt turns out that of the 4 types of auctions we just described, only two information patterns emerge:\n\nDescending bid and first-price sealed auction: In this case nobody learns anything about buyers willingness to pay until we see the highest bidder’s price and auction ends. We only ever learn the highest bidder’s bid\n\nAscending and second-price sealed: We see which buyers are willing to purchase at low prices, auction ends when one person has outbid the rest, if auction increments slowly this will be at the maximum price for second place bidder. In either case we learn second highest bidder’s price, which is paid by the highest bidder (we don’t get to see the highest bidder’s valuation)\n\nFor this reason, we’ll study the two forms of sealed-bid auctions\n\n","type":"content","url":"/l09-03-auctions#equivalence","position":9},{"hierarchy":{"lvl1":"Auctions","lvl2":"Second price, sealed bid auction"},"type":"lvl2","url":"/l09-03-auctions#second-price-sealed-bid-auction","position":10},{"hierarchy":{"lvl1":"Auctions","lvl2":"Second price, sealed bid auction"},"content":"Let’s set up the second price, sealed-bid auction as a game\n\nSuppose there are N bidders (each is a player)\n\nBidder i strategy is to bid an amount b_i, which is a function of that bidder’s true valuation v_i\n\nPayoffs to player i with valuation v_i and bid b_i are:\n\n0: if b_i is not highest bid\n\nequal to v_i - b_k: if b_i is highest bid and second higest bid is b_k\n\nTies go to bidder with lower “index” i wins over k if b_i = b_k and i < k\n\n","type":"content","url":"/l09-03-auctions#second-price-sealed-bid-auction","position":11},{"hierarchy":{"lvl1":"Auctions","lvl3":"Truth telling","lvl2":"Second price, sealed bid auction"},"type":"lvl3","url":"/l09-03-auctions#truth-telling","position":12},{"hierarchy":{"lvl1":"Auctions","lvl3":"Truth telling","lvl2":"Second price, sealed bid auction"},"content":"Claim: in a sealed-bid second price auction, it is a dominant strategy for each bidder to choose b_i = v_i\n\nTo prove this we need to consider possible outcomes if b_i <v_i or if b_i > v_i\n\nCall b_i' the bid above valuation (b_i' > v_i) and b_i'' the bid below valuation (b_i'' < v_i). Also let b_k represent the second highest bid\n\nSuppose bidder i chooses to bid b_i'' < v_i\n\nCase v_i, b_i'' < b_k: lose auction with payoff 0\n\nCase v_i, b_i'' > b_k: win auction with payoff v_i - b_k\n\nCase v_i > b_k and b_i'' < b_k: b_k'' loses auction gets. Bidding b_i = v_i would have won for payoff v_i - b_k \\ge 0. So, bidding to low can’t help, but can hurt\n\nSuppose bidder i chooses to bid b_i' > v_i\n\nCase v_i, b_i' < b_k: payoff 0\n\nCase v_i, b_i' > b_k: payoff v_i - b_k\n\nCase v_i < b_k and b_i' > b_k: b_i' now wins auction and gets payoff v_i - b_k \\le 0. Truthful bid would lose and get payoff 0. Bidding high can’t help, but can hurt\n\nSo, in sealed-bid second price auction it is always optimal to bid true value\n\n","type":"content","url":"/l09-03-auctions#truth-telling","position":13},{"hierarchy":{"lvl1":"Auctions","lvl2":"First price, sealed bid auction"},"type":"lvl2","url":"/l09-03-auctions#first-price-sealed-bid-auction","position":14},{"hierarchy":{"lvl1":"Auctions","lvl2":"First price, sealed bid auction"},"content":"Same notation players, valuations, and bids\n\nPayoffs are now:\n\n0 if b_i not highest\n\nequal to v_i - b_i if b_i is highest\n\nNote, bidding true value is not optimal -- you would always get 0 payoff\n\nWhat is optimal then?\n\nOptimal behavior is to “shade” bid a bit lower than true value\n\nHow much lower depends on interaction between not bidding too close to true value (b/c that diminishes your payoffs) and not bidding too low (b/c you risk losing an otherwise profitable win).\n\nActually solving for this tradeoff is complex!\n\n","type":"content","url":"/l09-03-auctions#first-price-sealed-bid-auction","position":15},{"hierarchy":{"lvl1":"Auctions","lvl3":"Considerations","lvl2":"First price, sealed bid auction"},"type":"lvl3","url":"/l09-03-auctions#considerations","position":16},{"hierarchy":{"lvl1":"Auctions","lvl3":"Considerations","lvl2":"First price, sealed bid auction"},"content":"What factors might influence how much you shade your bid?\n\nNumber of other bidders: with many bidders, shading becomes more risky (more people that might outbid you) so you tend to bid higher\n\nDistribution of bidder values: understanding how valuation of other bidders is distributed might allow you to shade more\n\n","type":"content","url":"/l09-03-auctions#considerations","position":17},{"hierarchy":{"lvl1":"Auctions","lvl3":"Outcomes","lvl2":"First price, sealed bid auction"},"type":"lvl3","url":"/l09-03-auctions#outcomes","position":18},{"hierarchy":{"lvl1":"Auctions","lvl3":"Outcomes","lvl2":"First price, sealed bid auction"},"content":"For now, we will not discuss how to compute optimal bids in first-price auctions\n\nInstead we will talk about some outcomes:\n\nThe Revelation Principle: in order to derive optimal bids, we use a framework that considers small deviations to v_i instead of b_i. We assert that the expected payoff for using a a strategy derived from a value v_i is at least as high as the expected payoff for a strategy derived from any other value v\n\nRevenue equivalence: the expected payoff to the seller is exactly the same for both first and second price auctions, when bidders follow equilibrium strategies\n\n","type":"content","url":"/l09-03-auctions#outcomes","position":19},{"hierarchy":{"lvl1":"Auctions","lvl2":"Twists"},"type":"lvl2","url":"/l09-03-auctions#twists","position":20},{"hierarchy":{"lvl1":"Auctions","lvl2":"Twists"},"content":"There are some twists to the auction setup we’ve described\n\nOne is the notion of an “all-pay auction”\n\nIn an all-pay auction only the highest bidder wins, but all bidders must pay their bid\n\nTurns out, this style of auction also satisfies the revenue equivalence principle (under equilibrium bidding, expected seller revenue is same as in sealed first and sealed second price bid)\n\nAuction markets on blockchain\n\nImplementing auctions via smart contracts has interesting implications\n\nTransfer of ownership can be settled immediately and trustlessly\n\nParticipating in an auction is permisionless (anyone can be a seller or buyer)\n\nConditions for resales can be set (i.e. original seller gets x% of all subsequent sales)  -- now we have a repeated, dynamic game!\n\nOnwership rights can be verified and open up\n\nThis scratches the surface of some economic implications of a class of assets called NFTs or non-fungible tokens\n\n     ","type":"content","url":"/l09-03-auctions#twists","position":21},{"hierarchy":{"lvl1":"Intro to Blockchains"},"type":"lvl1","url":"/l11-01-intro-to-blockchains","position":0},{"hierarchy":{"lvl1":"Intro to Blockchains"},"content":"Computational Analysis of Social Complexity\n\nFall 2025, Spencer Lyon\n\nPrerequisites\n\nNone!\n\nOutcomes\n\nCreate and utilize a ledger for recording financial transactions\n\nCompare the difference between current payment systems (credit cards) and blockchain based systems\n\nUnderstand the use of private/public key pairs in cryptographic security\n\nLearn the core ideas behind the proof of work consensus mechanism\n\nReferences\n\nhttps://​www​.forbes​.com​/advisor​/credit​-cards​/credit​-card​-processing​-how​-it​-works/\n\nhttps://​www​.youtube​.com​/watch​?v​=​bBC​-nXj3Ng4\n\nhttps://​medium​.com​/certik​/the​-blockchain​-trilemma​-decentralized​-scalable​-and​-secure​-e9d8c41a87b3\n\n","type":"content","url":"/l11-01-intro-to-blockchains","position":1},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Intro"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#intro","position":2},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Intro"},"content":"We now begin studying blockchains\n\nThis is an exciting new technology that combines aspects of computer science, cryptography, and economics to build a new set of “financial rails” for the 21st century\n\nWe will learn about the following concepts:\n\nFinancial ledgers\n\nPublic/private keys\n\nDecentralized ledgers and consensus\n\nThe Bitcoin network\n\n","type":"content","url":"/l11-01-intro-to-blockchains#intro","position":3},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Ledgers"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#ledgers","position":4},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Ledgers"},"content":"A financial ledger is a log or record of financial transactions\n\nLedgers are typically used by accountants to record the transaction of a business and produce financial statements\n\nBalance sheet: assets and liabilities of a company (stocks)\n\nIncome statement: revenues and expenses for a company in a given time frame (flows)\n\nCash flow statement: summary of cash (or cash equivalents) entering or leaving a company in a given time frame\n\nThere are rules that govern how every transaction should be recorded on a company’s general ledger\n\nThey are known as the generally accepted accounting principles (GAAP)\n\n","type":"content","url":"/l11-01-intro-to-blockchains#ledgers","position":5},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Simple Example","lvl2":"Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#simple-example","position":6},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Simple Example","lvl2":"Ledgers"},"content":"We won’t teach the rules of double entry book keeping or worry about categorizing transactions as credits or debits here\n\nInstead we’ll consider a simplified ledger that illustrates the main idea\n\nSuppose there are 4 friends: Alice, Bob, Charlie, and Darla\n\nThese friends frequently do things together, like go out for meals, attend concerts or movies, etc.\n\nTo simplify paying for all these excursions, they often “cover” one another with the expectation that the costs will be repayed\n\nInstead of immediately repaying with cash, the friends instead decide to use a ledger...\n\n","type":"content","url":"/l11-01-intro-to-blockchains#simple-example","position":7},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"... Example continued","lvl2":"Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#id-example-continued","position":8},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"... Example continued","lvl2":"Ledgers"},"content":"Suppose Alice and Bob go out for lunch, and Bob pays for Alice’s meal\n\nLedger entry: Alice owes Bob 20 USD\n\nThen Bob and Charlie go to a concert and Charlie buys the tickets\n\nLedger entry: Bob owes Charlie 40 USD\n\nThen Darla and Charlie go out to dinner and Darla buys Charlie’s meal:\n\nLeger Entry: Charlie owes Darla 30 USD\n\nFinally Darla and Alice see a movie and Alice buys the tickets\n\nLedger Entry: Darla owes Alice 10 USD\n\n","type":"content","url":"/l11-01-intro-to-blockchains#id-example-continued","position":9},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"... Example continued","lvl2":"Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#id-example-continued-1","position":10},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"... Example continued","lvl2":"Ledgers"},"content":"Then at the end of the month, the friends get together and add up how much each person paid compared to how much each person owes:\n\nAlice owes 10 USD\n\nBob owes 20 USD\n\nCharlie gets 10 USD\n\nDarla gets 20 USD\n\nOnce this tally has been computed, everyone who owes money puts their owed amount into the pot and everyone who should gets money takes it\n\n","type":"content","url":"/l11-01-intro-to-blockchains#id-example-continued-1","position":11},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Comments","lvl2":"Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#comments","position":12},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Comments","lvl2":"Ledgers"},"content":"Two comments\n\nWhile we don’t always think about ledgers, they are active all around us in our everday purchases\n\nIn this example, we used a communal ledger\n\nAnyone, at any time, can add an entry\n\nThis works well for a group of close friends where there is mutual trust\n\nBut what if there isn’t trust?...\n\nWe will discuss each of these in turn\n\n","type":"content","url":"/l11-01-intro-to-blockchains#comments","position":13},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Everyday Ledgers"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#everyday-ledgers","position":14},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Everyday Ledgers"},"content":"What happens when you use your credit card to purchase Mickey Ears from the World of Disney store in Disney Springs?\n\nA lot!\n\nThere are a few key players involved:\n\nYou\n\nDisney\n\nDisney’s bank (the “acquiring bank”)\n\nThe credit card network (Visa, Mastercard, Discover, or Amex)\n\nYour bank (who issued you the credit card, also called “issuing bank”)\n\nThere are two main steps\n\nThe approval phase: involves all 5 key players\n\nThe settlment phase: involves everyone except you\n\n","type":"content","url":"/l11-01-intro-to-blockchains#everyday-ledgers","position":15},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Approval phase","lvl2":"Everyday Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#approval-phase","position":16},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Approval phase","lvl2":"Everyday Ledgers"},"content":"Here are the steps for the approval phase\n\nYou swipe (or tap/insert) credit card into store terminal\n\nDisney sends card information and transaction details to acquiring bank via internet\n\nAcquiring bank receives information and sends details to card network\n\nCard network routes information to cardholder’s bank\n\nThe issuing bank receives the information and checks the card details (like card number and CVV code) to make sure the transaction is not fraudulent. The bank also ensures the cardholder is in good standing and has enough remaining credit to cover the purchase (or has sufficient funds to cover the transaction if using a debit card)\n\nThe issuing bank sends a response back via the card network to the acquiring bank indicating success or failure\n\nThe response is received at the Disney’s credit card machine or terminal. If all the credentials in Step 5 check out, the transaction will be approved. Otherwise, it will show a message like “denied” on the machine. The cardholder sees this information right away and finishes the transaction while the response code is stored on the Disney’s machine for stage two of processing.\n\nNOTE: ledgers are kept at each intermediary: disney records transaction response and details, issuing bank records how much you owe them for purchase, merchant bank records how much they need to collect on Disney’s behalf...\n\n","type":"content","url":"/l11-01-intro-to-blockchains#approval-phase","position":17},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Settlement Phase","lvl2":"Everyday Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#settlement-phase","position":18},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Settlement Phase","lvl2":"Everyday Ledgers"},"content":"Typically at store close (or on a schedule) Disney will send all transactions from that day to their bank\n\nAcquiring bank will confirm each authorization (using transaction response code provided by Disney) and use the card network to request funds from cardholders’ issuing banks\n\nIssuing bank pays acquiring bank via card network. Card network debits issuing bank and credits acquiring bank — purely middle man\n\nIssuing bank keeps track of transaction on your statement and collects from you at the end of the month\n\n","type":"content","url":"/l11-01-intro-to-blockchains#settlement-phase","position":19},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Comments","lvl2":"Everyday Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#comments-1","position":20},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Comments","lvl2":"Everyday Ledgers"},"content":"There is a lot of complexity here:\n\nMany counterparties and intermediaries\n\nDelayed finality\n\nRecord keeping for amount of credit used/remaining\n\nReporting at many levels\n\nProvides a great service\n\nWe don’t always need to carry cash\n\nWe can spend based on credit\n\nCan spend in digital places like online ordering (how would you send cash to amazon?)\n\nNot that efficient\n\nEach intermediary typically takes a fee\n\n","type":"content","url":"/l11-01-intro-to-blockchains#comments-1","position":21},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Trust in Ledgers"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#trust-in-ledgers","position":22},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Trust in Ledgers"},"content":"Let’s return to the example  with Alice, Bob, Charlie, and Darla\n\nSuppose there is a falling out between Alice and Bob and Bob decides to add an entry to the leger: Alice owes Bob 100 USD\n\nGiven what we described so far, there is nothing in our ledger system that prevents this\n\nWhen settling the tally in the ledger at the end of the month, Bob would get 100 USD  and Alice would have to pay 100 USD\n\nHow can we avoid this? Digital signatures\n\n","type":"content","url":"/l11-01-intro-to-blockchains#trust-in-ledgers","position":23},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Signatures","lvl2":"Trust in Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#signatures","position":24},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Signatures","lvl2":"Trust in Ledgers"},"content":"In the existing monetary system, signatures are often required on checks or other forms of payment\n\nThey are (supposedly) unique to each individual, but never change\n\nThere are also forms of digital signatures with the following properties\n\nThey cannot easily be forged\n\nThey vary based on the document being signed\n\nIt can be easily and unambiguously verified or proven that a specific entity signed the transaction\n\n","type":"content","url":"/l11-01-intro-to-blockchains#signatures","position":25},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Public/Private keys","lvl2":"Trust in Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#public-private-keys","position":26},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Public/Private keys","lvl2":"Trust in Ledgers"},"content":"How does this digital signature work?\n\nEach participant in our digital, communal ledger will need to have a public/private key pair\n\nAll participants know the public 1/2 of everyone’s key\n\nHowever, the private part is kept secret and is only known to the specific person\n\nWhen a transaction or message is added to the ledger, it must be signed by the counterparty\n\nsign(::Transaction, ::PrivateKey)::Signature\n\nAll other participants on the ledger can use the signer’s public key to verify that the message did indeed come from the signer:\n\nverify(::Signature, ::PublicKey)::Bool\n\nIn the example where Bob creates a fraudulent transaction, he would have to use Alice’s private key before other ledger members accept it\n\nBecause Bob doesn’t have Alice’s private key, he cannot do this, so the faulty transaction would be rejected\n\n","type":"content","url":"/l11-01-intro-to-blockchains#public-private-keys","position":27},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Comment","lvl2":"Trust in Ledgers"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#comment","position":28},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Comment","lvl2":"Trust in Ledgers"},"content":"Now we have found a workaround for the fradulent transaction issue, greatly lowering the need for trust\n\nHowever, we still trust that everyone will actually pay cash when it is time to settle up\n\nCan we get rid of that too?\n\n","type":"content","url":"/l11-01-intro-to-blockchains#comment","position":29},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Trustless ledgers"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#trustless-ledgers","position":30},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Trustless ledgers"},"content":"Suppose now that we extend the ledger in the following way\n\nAll participants must “deposit” funds before they can participate\n\nFor a transaction to be valid it must be signed/verified and must require that the users running balance is not less than their total “deposits”\n\nAt any time users can withdraw their funds to get back the dollars equal to their running balance (may be different from depoits)\n\nExample:\n\nLedger entries\n\nCharlie deposits 100 USD: accepted (balance 100 USD)\n\nCharlie owes Bob 50 USD: accepted (balance 50 USD)\n\nCharlie owes Alice 50 USD: accepted (balance 0 USD)\n\nCharlie owes Darla 20 USD: rejected  (balance already 0 USD, would be negative if accepted)\n\nNow we no longer need to trust anyone -- would be willing to let anyone in the world use this system\n\n","type":"content","url":"/l11-01-intro-to-blockchains#trustless-ledgers","position":31},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Cryptocurrencies"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#cryptocurrencies","position":32},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Cryptocurrencies"},"content":"If everyone in the world was using this ledger system to record earnings, borrowing, and spending -- we wouldn’t need dollars\n\nIf we don’t need dollars, we might as well call the values something different\n\nLet’s use LD (ledger dollars) insead of USD for entries in the ledger\n\nWe’ll talk about how new LD enter the system later, but for now assume we can convert USD to LD before depositing\n\nNOTE: as an economist I’m obligated to say that (1) I don’t believe we necessarily want a system without dollars and (2) it is not the intention of (most) cryptocurrency projects to replace the dollar\n\n","type":"content","url":"/l11-01-intro-to-blockchains#cryptocurrencies","position":33},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Ledger Dollars","lvl2":"Cryptocurrencies"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#ledger-dollars","position":34},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Ledger Dollars","lvl2":"Cryptocurrencies"},"content":"Now our ledger dollars exist only on our digital ledger\n\nBut, because everyone we know is using them, they are every bit as real as USD backed by the US government\n\nThe value of one LD is entirely determined by the ledger itself\n\nWe can say that the cryptocurrency is the ledger\n\n","type":"content","url":"/l11-01-intro-to-blockchains#ledger-dollars","position":35},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Centralization"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#centralization","position":36},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Centralization"},"content":"We now have a cryptocurrency -- the ledger dollar (LD)\n\nThe LD has value based on the ledger\n\nBut how to people add entries to the ledger\n\nSo far we have just called it public and said anyone can add an entry\n\nAn obvious choice would be a website or app\n\n... but that requires all users of the system to trust the servers/company providing the app!\n\nWe can do better...\n\n","type":"content","url":"/l11-01-intro-to-blockchains#centralization","position":37},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Decentralization <==> Trustless public ledger","lvl2":"Centralization"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#decentralization-trustless-public-ledger","position":38},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Decentralization <==> Trustless public ledger","lvl2":"Centralization"},"content":"In order to have a completely trustless peer-to-peer payment system (what Bitcoin was designed to be) we need to remove the centralized ledger operator\n\nHow do we do this?\n\nThis is a pretty technical question, explained beautifully in a \n\nYouTube video by 3blue1brown\n\nWe’ll watch that video together and then discuss\n\n\n\n","type":"content","url":"/l11-01-intro-to-blockchains#decentralization-trustless-public-ledger","position":39},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Blockchains: key components","lvl2":"Centralization"},"type":"lvl3","url":"/l11-01-intro-to-blockchains#blockchains-key-components","position":40},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl3":"Blockchains: key components","lvl2":"Centralization"},"content":"There are a few other key components that were mentioned in the video, but should be written here also\n\nWhy “blockchain”?\n\nTransactions are bundled together and submitted as a block\n\nThe ledger is a sequence of blocks\n\nTo validate a block, miners have to compute the hash of (1) the transactions in the block AND (2) the hash of the previous block\n\nThis dependence on the previous block gives the notion of a chain of blocks\n\nConsensus mechanism\n\nFor Bitcoin it is called “proof of work”\n\nExchanges physical resources (hardware and energy) for security\n\nMost popular modern alternative is “proof of stake” -- provides security through economic constraints and incentives. We’ll study this next time\n\nBlockchain trilemma:\n\nHypothesis is blockchains can have at most 2 of the following\n\nDecentralization: no central points of trust or failure\n\nSecurity: ability to operate as intended in the face of attacks\n\nScalability: ability to handle large volume of transactions\n\nBitcoin chose security and decentralization, but suffers in area of scalability\n\nBeing challenged by new technologies/blockchains including Solana, Avalanche, Cardano, Algorand, etc...\n\n","type":"content","url":"/l11-01-intro-to-blockchains#blockchains-key-components","position":41},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Blockchain Properties"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#blockchain-properties","position":42},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Blockchain Properties"},"content":"Global: blockchains are not limited geographically. If you can connect, you can use them wherever you happen to be in the world\n\nTrustless: users of the blockchain don’t have to trust any single person or entity in order to use the blockchain\n\nPermissionless: nobody can grant or deny access to the blockchain. If you have an internet connection and are able to connect to any node on the chain, you can use it exactly like all other users\n\n","type":"content","url":"/l11-01-intro-to-blockchains#blockchain-properties","position":43},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Blockchain Applications"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#blockchain-applications","position":44},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Blockchain Applications"},"content":"Peer-to-peer payments: original use case and purpose of bitcoin blockchain\n\nDigital art via NFTs\n\nGloabal healthcare records\n\nractionalized real-estate via tokenization\n\nVideo games and pay to earn\n\nInfrastructure\n\nDecentralized Finance\n\nCloud computing and \n\nfile storage\n\n... many, many others\n\n","type":"content","url":"/l11-01-intro-to-blockchains#blockchain-applications","position":45},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Summary"},"type":"lvl2","url":"/l11-01-intro-to-blockchains#summary","position":46},{"hierarchy":{"lvl1":"Intro to Blockchains","lvl2":"Summary"},"content":"Today we started by talking about the importance of financial ledgers\n\nWe discussed how ledgers are used in the current financial system\n\nWe then talked about the key components of a blockchain (distributed ledger) including public/private key cryptography, decentralization, consensus mechanisms\n\nWe then spent a few minutes talking about key properties of blockchains and some current applications of blockchain technology\n\nNext week we will learn about blockchain 2.0\n\nKey idea is what if we could store more than just transactions in the ledger\n\nWhat if we could store and execute computer programs?","type":"content","url":"/l11-01-intro-to-blockchains#summary","position":47},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0"},"type":"lvl1","url":"/l12-01-ethereum","position":0},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0"},"content":"Computational Analysis of Social Complexity\n\nPrerequisites\n\nIntro to Blockchain\n\nOutcomes\n\nRecall the key components of the Bitcoin blockchain\n\nUnderstand the key features of a finite state machine\n\nUnderstand the additional features Ethereum brought to the blockchain\n\nCode up some sample smart contracts\n\nReferences\n\nhttps://​ethereum​.org​/en​/whitepaper/\n\nhttps://​coinsul​.io​/learn​/smartContracts\n\nhttps://​coinsul​.io​/learn​/erc20\n\n","type":"content","url":"/l12-01-ethereum","position":1},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Review: What is a Blockchain?"},"type":"lvl2","url":"/l12-01-ethereum#review-what-is-a-blockchain","position":2},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Review: What is a Blockchain?"},"content":"A blockchain is a decentralized distributed ledger:\n\nLedger: record of financial transactions\n\nDistributed: full copies of ledger stored on multiple (all) nodes\n\nDecentralized: no single point of trust, control, or failure for network\n\n","type":"content","url":"/l12-01-ethereum#review-what-is-a-blockchain","position":3},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Review: How does the Blockchain Work?","lvl2":"Review: What is a Blockchain?"},"type":"lvl3","url":"/l12-01-ethereum#review-how-does-the-blockchain-work","position":4},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Review: How does the Blockchain Work?","lvl2":"Review: What is a Blockchain?"},"content":"Users construct and sign transactions\n\nTransactions are submitted to nodes (miners)\n\nNodes collect a bundle of transactions and order them in a block and do:\n\nvalidate all transactions in block (check fund ownership and verify signatures)\n\nFind a proof of work (PoW -- key to block that produces hash with special properties)\n\nFirst miner to find PoW gets BTC as reward, other nodes verify and all move to next block\n\nHash of block N-1 included in header of block N (any change to historical blocks changes all subsequent hashes and invalidates blocks) ==> block chain/\n\n","type":"content","url":"/l12-01-ethereum#review-how-does-the-blockchain-work","position":5},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Review: Properties of Blockchain","lvl2":"Review: What is a Blockchain?"},"type":"lvl3","url":"/l12-01-ethereum#review-properties-of-blockchain","position":6},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Review: Properties of Blockchain","lvl2":"Review: What is a Blockchain?"},"content":"Trustless: to interact with another individual, I don’t have to trust them or any third party\n\nPermissionless: permission can neither be granted nor revoked -- everyone on equal footing in terms of access\n\nCensorship resistant: cannot be manipulated or controlled by any single party or entity\n\n","type":"content","url":"/l12-01-ethereum#review-properties-of-blockchain","position":7},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Review: Issues with Blockchain","lvl2":"Review: What is a Blockchain?"},"type":"lvl3","url":"/l12-01-ethereum#review-issues-with-blockchain","position":8},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Review: Issues with Blockchain","lvl2":"Review: What is a Blockchain?"},"content":"PoW requires extreme amount of computation => uses energy => damages environment\n\nLimited throughput\n\n“Just” a ledger -- can only record transfers of BTC (with minor caveat)\n\n","type":"content","url":"/l12-01-ethereum#review-issues-with-blockchain","position":9},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Key Abstraction: State Machine"},"type":"lvl2","url":"/l12-01-ethereum#key-abstraction-state-machine","position":10},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Key Abstraction: State Machine"},"content":"In computer science, a state machine is a system with the following properties\n\nCan be in exactly one of a (usually finite) number of “states” at any time\n\nResponds to inputs to transition from one state to another state\n\nLet\n\ns: state of the machine at some moment\n\ntx: an arbitrary set of inputs\n\ns' the “next” state after processing tx\n\nf(s, tx) = s': the transition function that produces the next state (s') given current state (s) and an input set (tx)\n\n","type":"content","url":"/l12-01-ethereum#key-abstraction-state-machine","position":11},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Bitcoin as State Machine","lvl2":"Key Abstraction: State Machine"},"type":"lvl3","url":"/l12-01-ethereum#bitcoin-as-state-machine","position":12},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Bitcoin as State Machine","lvl2":"Key Abstraction: State Machine"},"content":"In a blockchain the state is a record of all unspent transaction outputs (UTXO)\n\nUTXO is a more accurate technical term for “account balance”. It is ok to think of this as the number of coins held by an account, or their balance\n\nEach transaction modifies the state of the blockchain\n\nExample: Leslie gives Ron 2.3 BTC results in0xLeslie000 -= 2.3 BTC\n0xRon000000 += 2.3 BTC\n\nThe Bitcoin blockchain (mostly) limits the transition function f to modify account balances\n\n","type":"content","url":"/l12-01-ethereum#bitcoin-as-state-machine","position":13},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Ethereum’s Innovation","lvl2":"Key Abstraction: State Machine"},"type":"lvl3","url":"/l12-01-ethereum#ethereums-innovation","position":14},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Ethereum’s Innovation","lvl2":"Key Abstraction: State Machine"},"content":"The Ethereum project took the concept of treating the blockchain as a state machine one step further...\n\nEach node runs a turing complete virtual machine (called Ethereum virtual machine or EVM)\n\nTransactions can now encode arbitrary logic, expressed in bytecode of EVM\n\nCode executed during transaction validation stage\n\nMakes validation slightly more expensive, but cost is negligible compared to proof of work (which Ethereum still uses)\n\n","type":"content","url":"/l12-01-ethereum#ethereums-innovation","position":15},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Ethereum: History"},"type":"lvl2","url":"/l12-01-ethereum#ethereum-history","position":16},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Ethereum: History"},"content":"Whitepaper published in 2013\n\nOfficially launched in 2015\n\nFounded by Vitalik Buterin (considered main founder), Anthony Di Iorio, Charles Hoskinson, Mihai Alisie & Amir Chetrit\n\nMany more people have worked on it\n\nHas been 2nd largest (only bitcoin is biggest) in terms of Market cap for many years\n\nIt’s coin (Ether, short ETH) is largest cryptocurrency based on settlement value (bigger than BTC)\n\nHome blockchain to many, many projects\n\nIf you add up market cap for ETH + all tokens built on Ethereum, about same size as BTC\n\n","type":"content","url":"/l12-01-ethereum#ethereum-history","position":17},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Smart Contracts"},"type":"lvl2","url":"/l12-01-ethereum#smart-contracts","position":18},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Smart Contracts"},"content":"The ethereum blockchain has two types of accounts\n\nExternally owned accounts: these are simlar to bitcoin accounts. Held/controlled by a public/private key\n\nContract Accounts (smart contracts): can hold ETH and interact with other accounts, but controlled by contract code\n\nContracts can receive messages and execute pre-defined instructions in response\n\n","type":"content","url":"/l12-01-ethereum#smart-contracts","position":19},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"What is a Smart Contract?","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#what-is-a-smart-contract","position":20},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"What is a Smart Contract?","lvl2":"Smart Contracts"},"content":"I like to think of smart contracts like classes in a programming language\n\nLike a class, a smart contract\n\nHas a name\n\nCan be “instantiated” (deployed)\n\nHas properties or data\n\nHas actions/functions that act on properties and/or externally supplied data at call time\n\nUnlike a class, a smart contract\n\nIs immutable -- once it is deployed it can’t be changed\n\nIs payable (can receive tokens/coins)\n\nIs stored on the blockchain -- the code defining the contract’s functionality is stored as the data field on the contract account\n\n","type":"content","url":"/l12-01-ethereum#what-is-a-smart-contract","position":21},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Safe Online Purchase","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#example-safe-online-purchase","position":22},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Safe Online Purchase","lvl2":"Smart Contracts"},"content":"Suppose you would like to purchase a one-of-a-kind piece of art  directly from the artist\n\nBecause of COVID-19, this artist does all transactions online\n\nTo make the purchase you must trust\n\nThe payment provider used by the artist\n\nThe artist to actually deliver the artwork\n\nThe typicaly flow of events is:\n\nYou identify good and send money to payment provider\n\nPayment provider gives confirmation of receipt to artist\n\nArtist sends you the goods\n\n(not necessarily in this order), artist gets money from payment provider\n\n","type":"content","url":"/l12-01-ethereum#example-safe-online-purchase","position":23},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"... continued","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#id-continued","position":24},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"... continued","lvl2":"Smart Contracts"},"content":"Because of the high price, this is a pretty risky transaction\n\nYou and the artist agree upon an altered system for how this works\n\nWhen you decide to purchase an item both you and the artist must send 2x the sale price to the payment provider\n\nOnce the payment provider gets all the funds and seller has shipped product, the following happens\n\nBuyer confirms reciept of goods\n\nPayment provider sends 1/2 buyer’s money to buyer\n\nPayment provider sends 1/2 buyer’s money + all of seller’s money to seller\n\nThe idea is that both parties have an incentive to complete the transaction, or they forfeit their deposits\n\nThis is an improvement, but you now have a lot of trust in payment provider...\n\n","type":"content","url":"/l12-01-ethereum#id-continued","position":25},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"... continued","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#id-continued-1","position":26},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"... continued","lvl2":"Smart Contracts"},"content":"You and the artist happen to be programmers\n\nTo help the payment provider, you give them the following Julia code to help them fulfill their duties:\n\n@enum PurchaseState begin\n    Created\n    Locked\n    Released\n    Inactive\nend\n\nmutable struct Purchase\n    value::Number\n    buyer_deposit::Int\n    seller_deposit::Int\n    good::Any\n    state::PurchaseState\n    \n    # start a new purchase -- initiated\n    # by buyer sending an initial deposit for the item\n    function Purchase(buyer_deposit::Int)\n        new(buyer_deposit/2, buyer_deposit, 0, nothing, Created)\n    end\nend\n\n\"\"\"\nSeller confirms purchase and sends their deposit.\n\nTo be called only by seller!\n\"\"\"\nfunction confirm_purchase(p::Purchase, seller_deposit::Int, good::Any)\n    # TODO: how to guarantee seller is caller??\n    \n    # make sure deposits match\n    @assert seller_deposit == p.buyer_deposit\n\n    # make sure state is created\n    @assert p.state == Created\n        \n    # record deposit\n    p.seller_deposit = seller_deposit\n    \n    # send good\n    p.good = good\n    \n    # note that the state is locked\n    p.state = Locked\nend\n\n\"\"\"\nBuyer confirms receipt\n\nTo be called only by buyer!\n\"\"\"\nfunction confirm_receipt(p::Purchase)\n    # TODO: how to make sure buyer is caller??\n    # assert state is locked\n    @assert p.state == Locked\n    \n    # mark good as having been released\n    p.state = Released\n     \n    # send 1/2 deposit back to buyer\n    p.buyer_deposit -= p.value\nend\n\n\"\"\"\nSeller gets payment\n\nTo be called only be seller!\n\"\"\"\nfunction finalize_transaction(p::Purchase)\n    # TODO: how to make sure seller is caller\n    @assert p.state == Released\n    \n    # send rest of buyer's deposit and all of seller's deposit\n    p.buyer_deposit -= p.value\n    p.seller_deposit -= 2*p.value\n    \n    p.state = Inactive\nend\n\n","type":"content","url":"/l12-01-ethereum#id-continued-1","position":27},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"... continued","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#id-continued-2","position":28},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"... continued","lvl2":"Smart Contracts"},"content":"Let’s try out our code\n\nSuppose the purchase price is $2,000\n\nThe item is a very nice string\n\n# start transaction with buyer sending $4000 = 2x price\nitem = \"Happy Holidays!\"\np = Purchase(4000)\np\n\nconfirm_purchase(p, 4000, item)\np\n\nconfirm_receipt(p)\np\n\nfinalize_transaction(p)\np\n\n","type":"content","url":"/l12-01-ethereum#id-continued-2","position":29},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"...comments","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#id-comments","position":30},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"...comments","lvl2":"Smart Contracts"},"content":"This code will help the payment provider make sure no operations happen out of sequence\n\nIf seller tries to finalize before buyer confirms receipt, code will throw an error\n\nIf buyer tries to confirm receipt before seller provides good and deposit, code errors\n\nIf anyone tries to do their transaction twice, error\n\nThere are still problems with the code\n\nSome function should only be called by either seller or buyer, but we haven’t enforced that\n\nCode doesn’t actualy do value transfer, still trust payment provider to do steps as we agreed\n\n","type":"content","url":"/l12-01-ethereum#id-comments","position":31},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Purchase as Smart Contract","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#example-purchase-as-smart-contract","position":32},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Purchase as Smart Contract","lvl2":"Smart Contracts"},"content":"Let’s revisit this example, but using a Smart Contract\n\nTo do this we will load up a contract I wrote into the \n\nremix IDE and experiment with it\n\nTo follow along\n\nOpen remix using the link above\n\nOn the home screen fin the “Gist” button under “Load From”. Copy/paste the following gist id 3be12cd254272ab7a06e4859a37a8ba2\n\nThen watch as we work through the contract interactions\n\n","type":"content","url":"/l12-01-ethereum#example-purchase-as-smart-contract","position":33},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Notes","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#notes","position":34},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Notes","lvl2":"Smart Contracts"},"content":"Each time we call a function, we did an ethereum transaction\n\nA transaction receipt contains the hash of the transaction and other helpful information\n\n","type":"content","url":"/l12-01-ethereum#notes","position":35},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Transaction Data: Events","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#transaction-data-events","position":36},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Transaction Data: Events","lvl2":"Smart Contracts"},"content":"Contract method calls are transactions with:\n\nCaller (msg.sender)\n\nAmount of Ether (msg.value)\n\nInputs (function arguments)\n\nOutputs (return values)\n\nEmitted Events\n\nThe Event system in Ethereum works as follows:\n\nEvent types are defined within the SmartContract (see event keyword in our Purchase.sol contract)\n\nThey have a name, and some parameters\n\nWhen contract methods are called, events are emitted\n\nEmitted events are collected/reported alongside transaction\n\n","type":"content","url":"/l12-01-ethereum#transaction-data-events","position":37},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Transaction Data: Logs","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#transaction-data-logs","position":38},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Transaction Data: Logs","lvl2":"Smart Contracts"},"content":"Whenever an event is emitted from a smart contract, a log is created -- there is a 1:1 relationship between emit and log\n\nThese logs are the main visibility window into the data passing through contracts\n\nA team building a smart contract system with web-based interface will\n\nWrite contracts to fulfill goals\n\nCraft event types to capture key semantic data\n\nEmit events in the smart contract methods\n\nProcess the logs that are generated as users interact with platform\n\nDisplay the processed log data in front-end\n\n","type":"content","url":"/l12-01-ethereum#transaction-data-logs","position":39},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Uniswap V3 Info","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#example-uniswap-v3-info","position":40},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Uniswap V3 Info","lvl2":"Smart Contracts"},"content":"Uniswap is a smart contract system that allows users to swap one ethereum based token for another one\n\nWe’ll learn much more about it in a future lecture\n\nFor now we’ll focus only on a specific type of event: \n\nSwap\n\nThe Swap event is emitted every time a trade or swap happens on the platform\n\nThe Uniswap team uses it to track things like:\n\nTotal number of swaps\n\nSwaps per user\n\nSwaps per coin\n\nPrice of coins\n\nVolume in USD of swaps\n\nEtc.\n\nWe can see all this processed log data on their data dashboard: \n\nhttps://​info​.uniswap​.org​/​#/\n\n","type":"content","url":"/l12-01-ethereum#example-uniswap-v3-info","position":41},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Data Sources","lvl2":"Smart Contracts"},"type":"lvl3","url":"/l12-01-ethereum#data-sources","position":42},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Data Sources","lvl2":"Smart Contracts"},"content":"There are a few ways to get log data from Ethereum transactions:\n\nAccess via Ethereum node. This is the ultimate source of truth and will let you access everything. It is also the most technical\n\nTheGraph: project that allows developers to specify how to process log data and creates a data marketplace for data indexing/access\n\nOther, private APIs: \n\nBitquery, \n\netherscan, \n\ncoingecko, \n\ncoinmarketcap, many more...\n\n","type":"content","url":"/l12-01-ethereum#data-sources","position":43},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Additional Examples"},"type":"lvl2","url":"/l12-01-ethereum#additional-examples","position":44},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Additional Examples"},"content":"Below we have a couple more examples of what smart contracts could be used for\n\nThis barely scratches the surface of what is possible\n\nThe goal is to whet your appetite to learn more\n\n","type":"content","url":"/l12-01-ethereum#additional-examples","position":45},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Escrow","lvl2":"Additional Examples"},"type":"lvl3","url":"/l12-01-ethereum#example-escrow","position":46},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Escrow","lvl2":"Additional Examples"},"content":"Suppose you want to purchase a home\n\nThe typical flow of events includes:\n\nBuyer makes an offer\n\nSeller accepts\n\nBuyer deposits some money into an escrow account\n\nInsepections are done, contracts drafted\n\nSeller gives title (ownership) to escrow agent\n\nBuyer gives escrow agent rest of down payment + closing costs + mortgage information\n\nWhen (and only when) escrow agent has both title from seller and all funds from buyer, makes swap to transfer funds to seller and title to buyer\n\n","type":"content","url":"/l12-01-ethereum#example-escrow","position":47},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Escrow in contracts","lvl2":"Additional Examples"},"type":"lvl3","url":"/l12-01-ethereum#escrow-in-contracts","position":48},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Escrow in contracts","lvl2":"Additional Examples"},"content":"The escrow example is simlar to our purcahse example\n\nHere the good is a home, represented by the title\n\nThe best scenario is that the title is tokenized and represented on the blockchain\n\nIn this case the contract would need to:\n\nCollect escrow funds from buyer\n\nCollect title from seller\n\nCollect closing costs/downpayment/mortgage info from buyer\n\nSwap title for funds\n\nThe benefit of doing this on the blockchain is that there is no need for an Escrow agent (who takes a fee) or trust between parties.\n\n","type":"content","url":"/l12-01-ethereum#escrow-in-contracts","position":49},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Lending and Borrowing","lvl2":"Additional Examples"},"type":"lvl3","url":"/l12-01-ethereum#example-lending-and-borrowing","position":50},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl3":"Example: Lending and Borrowing","lvl2":"Additional Examples"},"content":"Suppose you hold some ETH, don’t want to sell it, but need USD to pay bills\n\nYou could take our ETH to a smart contract based lending platform like Aave or compound and deposit as collateral\n\nThen you can borrow up to 1/2 the value of your collateral as USDC\n\nYou can take USDC back to your exchange and get USD\n\nPay your bills using the USD\n\nWhen you get a paycheck you go pack to exchange to convert USD -> USDC\n\nThen you go back to Aave, repay your loan, and reclaim your ETH\n\nAmazing thing is:\n\nYou don’t need credit or permisison to get loan (your ETH is your credit)\n\nYou don’t have to apply\n\nyou don’t need to know your lenders\n\nSystem protects lenders -- if your collateral falls in value, Aave’s smart contracts will sell it to pay back lenders\n\nNo fixed duration/terms/due date\n\n","type":"content","url":"/l12-01-ethereum#example-lending-and-borrowing","position":51},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Looking Ahead"},"type":"lvl2","url":"/l12-01-ethereum#looking-ahead","position":52},{"hierarchy":{"lvl1":"Ethereum: Blockchain 2.0","lvl2":"Looking Ahead"},"content":"The possibilities with smart contracts on a blockchain are vast and still being explored\n\nWe have two more weeks of class\n\nStudy decentralized finance, the ERC-20 token standard, and Dapps like Uniswap/Curve/Aave\n\nStudy the ERC-721 token standard, NFTs, and Dapps like OpenSea/TopShots/Rarible/AxieInfinity","type":"content","url":"/l12-01-ethereum#looking-ahead","position":53},{"hierarchy":{"lvl1":"Defi: Money Legos"},"type":"lvl1","url":"/l12-02-defi","position":0},{"hierarchy":{"lvl1":"Defi: Money Legos"},"content":"Computational Analysis of Social Complexity\n\nPrerequisites\n\nIntro to Blockchain\n\nEthereum: Blockchain 2.0\n\nOutcomes\n\nBe familiar with most popular Defi protocols\n\nUnderstand how the ERC-20 token standard is central to Defi\n\nUnderstand how Uniswap enables decentralized liquidity and token swapping\n\nSee example of data that can be analyzed for Uniswap\n\nReferences\n\nhttps://​ethereum​.org​/en​/developers​/docs​/standards​/tokens​/erc​-20/\n\nhttps://​defipulse​.com\n\ncontracts​/token​/ERC20​/IERC20​.sol\n\nhttps://​ethereum​.org​/en​/developers​/docs​/web2​-vs​-web3/\n\nhttps://​www​.freecodecamp​.org​/news​/what​-is​-web3/\n\nhttps://​coinsul​.io​/learn​/liquidity\n\nhttps://​coinsul​.io​/learn​/erc20\n\nhttps://​coinsul​.io​/learn​/amm\n\n","type":"content","url":"/l12-02-defi","position":1},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Decenralized Finance"},"type":"lvl2","url":"/l12-02-defi#decenralized-finance","position":2},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Decenralized Finance"},"content":"One of the largest and growing applications of the Smart Contract system is known as Decentralized Finance, or Defi\n\nDecentralized Finance (Defi) is\n\nDefinition: Financial primitives built on a decentralized blockchain\n\nOne of the key applications of blockchain technology to date\n\nOver $40B in funds locked in Defi smart contracts (\n\nsource)\n\n","type":"content","url":"/l12-02-defi#decenralized-finance","position":3},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Defi Primitives","lvl2":"Decenralized Finance"},"type":"lvl3","url":"/l12-02-defi#defi-primitives","position":4},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Defi Primitives","lvl2":"Decenralized Finance"},"content":"Some of the primitives offered by Defi applications include:\n\nCollateralized borrowing and lending: \n\nCompound and \n\nAAVE\n\nSwapping or exchanging tokens: \n\nUniswap, \n\nSushiSwap, and \n\nCurve\n\nMarket making or liquidity providing: same list as swapping/exchanging\n\nInsurance: \n\nNexus Mutual\n\nIndex Funds: \n\nTokenSets and \n\nBalancer\n\n","type":"content","url":"/l12-02-defi#defi-primitives","position":5},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Money Legos","lvl2":"Decenralized Finance"},"type":"lvl3","url":"/l12-02-defi#money-legos","position":6},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Money Legos","lvl2":"Decenralized Finance"},"content":"Defi inherits the primary features of a blockchain: trustless, permisionless, censorship-resistant\n\nDefi protocols also have additional features:\n\n“Blind” interoperability\n\nLoosely coupled, but tightly integrated\n\nThese are possible because of the ERC-20 token standard\n\n","type":"content","url":"/l12-02-defi#money-legos","position":7},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"ERC-20","lvl2":"Decenralized Finance"},"type":"lvl3","url":"/l12-02-defi#erc-20","position":8},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"ERC-20","lvl2":"Decenralized Finance"},"content":"The ERC-20 token standard is an interface describing functionality (methods) and data (events) that make a smart contract a “token”\n\nThe Methods that are required include:// Info\nfunction name() public view returns (string)\nfunction symbol() public view returns (string)\nfunction decimals() public view returns (uint8)\nfunction totalSupply() public view returns (uint256)\n\n// functionality\nfunction balanceOf(address _owner) public view returns (uint256 balance)\nfunction transfer(address _to, uint256 _value) public returns (bool success)\nfunction transferFrom(address _from, address _to, uint256 _value) public returns (bool success)\nfunction approve(address _spender, uint256 _value) public returns (bool success)\nfunction allowance(address _owner, address _spender) public view returns (uint256 remaining)\n\nThe events areevent Transfer(address indexed _from, address indexed _to, uint256 _value)\nevent Approval(address indexed _owner, address indexed _spender, uint256 _value)\n\n","type":"content","url":"/l12-02-defi#erc-20","position":9},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Why ERC-20?","lvl2":"Decenralized Finance"},"type":"lvl3","url":"/l12-02-defi#why-erc-20","position":10},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Why ERC-20?","lvl2":"Decenralized Finance"},"content":"Applications can be built against the ERC-20 standard instead of specific tokens\n\nAllows applications to work for all tokens that implement the standard\n\nExtends to tokens that don’t even exist when the application is created\n\nAllows using outputs of one application as input for another\n\nExample: Deposit \n\nUSDC on AAVE as a lender --> receive \n\naUSDC\n\nDeposit aUSDC to \n\nCurve as a liquidity provider --> receive \n\nCurve LP tokens\n\nDeposit Curve LP tokens into \n\nConvexFinance to increase LP revenues --> receive Convex Reward tokens\n\n","type":"content","url":"/l12-02-defi#why-erc-20","position":11},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Web3"},"type":"lvl2","url":"/l12-02-defi#web3","position":12},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Web3"},"content":"A common term you will hear in the ethereum space is “Web3”\n\nWhat is Web3?\n\nWeb3 means applications with a web-based frontend, backed at least in part by blockchain on the backend\n\nKey components to web3\n\nUser wallet for authentication/authorization (public/private key pair)\n\nStorage of application assets on decentralized file system like IPFS\n\nKey logic and payment settling happens on chain\n\n","type":"content","url":"/l12-02-defi#web3","position":13},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Why Web3?","lvl2":"Web3"},"type":"lvl3","url":"/l12-02-defi#why-web3","position":14},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Why Web3?","lvl2":"Web3"},"content":"Inherit benefits of blockchain: global, permissionless, trustless, etc.\n\nAvoid the “middle man” of the internet: companies like Google, FaceBook, Amazon tracking your activity and making money on it\n\nEnables content creators to truly own their content (see next week’s discussion on NFTs)\n\n","type":"content","url":"/l12-02-defi#why-web3","position":15},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Dapps","lvl2":"Web3"},"type":"lvl3","url":"/l12-02-defi#dapps","position":16},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Dapps","lvl2":"Web3"},"content":"Another common piece of jargon is “Dapp”\n\nDapp stands for “decentralized app”\n\nA Dapp is a system of coordinating smart contracts and web3 enabled user interface tied together to create an application like experience\n\nTo build a Dapp you need a frontend (web) talent and backend talent just like building an app...\n\nAdditionally Dapp builders also need to be able to create smart contracts\n\n","type":"content","url":"/l12-02-defi#dapps","position":17},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Popular Dapps","lvl2":"Web3"},"type":"lvl3","url":"/l12-02-defi#popular-dapps","position":18},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Popular Dapps","lvl2":"Web3"},"content":"There are many popular Dapps\n\nAll the examples listed earlier in the lecture as examples of Defi platforms are examples of Dapps\n\nThere are other, non Defi dapps also:\n\nNFTS: \n\nOpenSea, \n\nRarible, \n\nLooksRare\n\nCloud services: \n\nFilecoin, \n\nDfinity\n\nGaming: \n\nSandbox, \n\nAxieInfinity, \n\nGala\n\n","type":"content","url":"/l12-02-defi#popular-dapps","position":19},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Example: Uniswap V2"},"type":"lvl2","url":"/l12-02-defi#example-uniswap-v2","position":20},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Example: Uniswap V2"},"content":"Let’s dive in to see in a bit more detail how the Uniswap Platform works\n\nTwo types of users:\n\nTraders: go to site, input token they want to sell (token A) + amount to sell + token to buy (token B) ==> platform figures out trade and gives quote for amount of token B user will receive\n\nLiquidity Providers: supply a pair of assets they own to a liquidity pool. These assets allow users to trade from one asset in the pool to the other\n\n","type":"content","url":"/l12-02-defi#example-uniswap-v2","position":21},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"How Trading Works","lvl2":"Example: Uniswap V2"},"type":"lvl3","url":"/l12-02-defi#how-trading-works","position":22},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"How Trading Works","lvl2":"Example: Uniswap V2"},"content":"In a traditional exchange (not Uniswap), trades are executed based off an order book\n\nSellers of an asset post a sale price and amount\n\nBuyers post a purchase price and amount\n\nWhenever there is “overlap” on seller and buyer orders, a trade happens\n\nUniswap takes a different approach and is called an Automated Market Maker (AMM)\n\nLiqudity providers deposit 2 tokens into a pool (anyone can deposit into any pool)\n\nA bonding curve is used to quote prices of one token in terms of the other (e.g. tokenA/tokenB or tokenB/tokenA)\n\nTraders interact directly with the pool\n\nTokens they are selling are deposited into pool\n\nTokens they are buying are withdrawn\n\nA logic layer called the “router” allows one token to be swapped for any other one, even if there is not a pool between the two tokens\n\ne.g. CRV -> GRT might go through path CRV -> ETH -> USDC -> GRT\n\nThree pools are used CRV - ETH, ETH - USDC, USDC - GRT\n\nTerm AMM comes from fact that smart contract + mathematical equation execute trades automatically without any central entity needing to organize order book or execute trades\n\n","type":"content","url":"/l12-02-defi#how-trading-works","position":23},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Constant Product AMM: x * y = k","lvl2":"Example: Uniswap V2"},"type":"lvl3","url":"/l12-02-defi#constant-product-amm-x-y-k","position":24},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Constant Product AMM: x * y = k","lvl2":"Example: Uniswap V2"},"content":"We spoke of a bonding curve that is used by the pool’s smart contract to quote prices\n\nThis is a key mathematical equation that dictates how reserves enter or leave the pool\n\nThe bonding curve used in Uniswap (and others like SushiSwap) is called the constant product rule: x * y = k\n\nFrom the \n\nuniswap V2 docs\n\nPairs act as automated market makers, standing ready to accept one token for the other as long as the “constant product” formula is preserved. This formula, most simply expressed as x * y = k, states that trades must not change the product (k) of a pair’s reserve balances (x and y). Because k remains unchanged from the reference frame of a trade, it is often referred to as the invariant. This formula has the desirable property that larger trades (relative to reserves) execute at exponentially worse rates than smaller ones.\n\nMore reading: \n\nhttps://​coinsul​.io​/learn​/amm\n\n","type":"content","url":"/l12-02-defi#constant-product-amm-x-y-k","position":25},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise","lvl2":"Example: Uniswap V2"},"type":"lvl3","url":"/l12-02-defi#exercise","position":26},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise","lvl2":"Example: Uniswap V2"},"content":"Consider a liquidity pool between assets WBTC and USDC\n\nWe’ll refer to reserves of WBTC as x\n\nUSDC reserves are y\n\nSuppose that before a trade we have x = 25 and y=600_000\n\nThis implies that k = x * y =  6_000_000\n\nA trader comes and wants to sell 0.5 WBTC\n\nAfter the trade we have x1 = x0 + 0.5 = 25.5 units of WBTC in the pool\n\nQuestion: What amount of USDC will the trader get out of the pool? Use the code cell below to figure it out...\n\n# TODO: your code here\n\n\n","type":"content","url":"/l12-02-defi#exercise","position":27},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Comment","lvl2":"Example: Uniswap V2"},"type":"lvl3","url":"/l12-02-defi#comment","position":28},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Comment","lvl2":"Example: Uniswap V2"},"content":"See how the x * y = k bonding curve was used to quote prices\n\nIn our example we were able to say that the price of 0.5 WBTC (in terms of USDC) was the answer you computed\n\n\n\n","type":"content","url":"/l12-02-defi#comment","position":29},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Adding/Removing Liquidity","lvl2":"Example: Uniswap V2"},"type":"lvl3","url":"/l12-02-defi#adding-removing-liquidity","position":30},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Adding/Removing Liquidity","lvl2":"Example: Uniswap V2"},"content":"The constant product rule states that x * y = k before and after each trade\n\nWhat about when liquidity is added or removed from the pool?\n\nIn order to add liquidity a liquidity provider must deposit equal value of both tokens\n\nValue is computed in terms of the current reserves: for each unit of token B that is deposited, a user must have x/y units of token A\n\nWhen liquidity is added, k is increased\n\nWhen liquidity is removed the liquidity provider gets their share of the current reserves and k decreases\n\n","type":"content","url":"/l12-02-defi#adding-removing-liquidity","position":31},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Example: Analyzing Uniswap Pools","lvl2":"Example: Uniswap V2"},"type":"lvl3","url":"/l12-02-defi#example-analyzing-uniswap-pools","position":32},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Example: Analyzing Uniswap Pools","lvl2":"Example: Uniswap V2"},"content":"Let’s do some data analysis on Uniswap Pools\n\nBelow I have some code that will read in data on all pools with at least $75,000 total value\n\nThe data come from a service called “The Graph”\n\nI also have some helper functions that you might find helpful for your analysis\n\nWe’ll seek to answer some questions:\n\nHow many pools have at least 1/2 million in asssets?\n\nWhich token appears in the most pools?\n\nIs it possible to swap from any token in these pools to any other token? (i.e. is the graph connected)?\n\nWhat’s the cheapest possible way to trade from one token to another?\n\n","type":"content","url":"/l12-02-defi#example-analyzing-uniswap-pools","position":33},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Data","lvl2":"Example: Uniswap V2"},"type":"lvl3","url":"/l12-02-defi#data","position":34},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Data","lvl2":"Example: Uniswap V2"},"content":"Let’s first gather some data\n\n# import Pkg\n# Pkg.activate(\".\")\n# Pkg.add([\"HTTP\", \"JSON3\", \"StructTypes\", \"Graphs\", \"DataFrames\", \"GraphPlot\"])\n# Pkg.instantiate()\n\nusing HTTP, JSON3, StructTypes\n\nstruct Token\n    id::String\n    name::String\n    symbol::String\n    decimals::Int\nend\n\n\nstruct Pool\n    id::String\n    inputTokens::Vector{Token}\n    inputTokenBalances::Vector{String}\n    totalValueLockedUSD::Float64\n    inputTokenWeights::Vector{String}\nend\n\nstruct Pairs\n    liquidityPools::Vector{Pool}\nend\n\nstruct Response\n    data::Pairs\nend\n\n# Set up JSON3 to unpack API results into our structs\nStructTypes.StructType(::Union{Type{Pool},Type{Token},Type{Response},Type{Pairs}}) = StructTypes.Struct()\n\nfunction request_uniswap_pool_data(api_key, first::Int=1000, skip::Int=0)\n    query = \"\"\"\n    {\n      liquidityPools(\n        first: $first\n        skip: $skip\n        orderBy: id\n        orderDirection: desc\n        where: {totalValueLockedUSD_gt: 75000}\n      ) {\n        id\n        inputTokens {\n          id\n          name\n          symbol\n          decimals\n        }\n        inputTokenBalances\n        totalValueLockedUSD\n        inputTokenWeights\n      }\n    }\n    \"\"\"\n    url = \"https://gateway.thegraph.com/api/$(api_key)/subgraphs/id/77jZ9KWeyi3CJ96zkkj5s1CojKPHt6XJKjLFzsDCd8Fd\"\n    # url = \"https://api.thegraph.com/subgraphs/name/uniswap/uniswap-v2\"\n    headers = Dict(\"content-type\" => \"application/json\")\n    body = Dict(\"query\" => query)\n    res = HTTP.post(url, headers, JSON3.write(body))\n    data = JSON3.read(String(res.body), Response, parsequoted=true)\n    return data.data.liquidityPools\nend\n\n\nusing JLD2\n\nfunction load_pool_data(api_key)\n    if isfile(\"all_pools.jld2\")\n        return jldopen(\"all_pools.jld2\") do file\n           file[\"all_pools\"]\n        end\n    end\n\n    # we are limited to 1000 pools per request, so we will\n    # paginate through the request until we have collected\n    # all pools with at least 50k in holdings\n    done = false\n    n = 0\n    all_pools = Pool[]\n    while !done\n        new_pools = request_uniswap_pool_data(api_key, 1000, n)\n        @show n += length(new_pools)\n        if length(new_pools) < 1000\n            done = true\n        end\n        all_pools = vcat(all_pools, new_pools)\n    end\n\n    jldsave(\"all_pools.jld2\"; all_pools)\n\n    all_pools\nend\n\n\nall_pools = load_pool_data(\"MISSING_API_KEY\");\n\n## the code below constructs `tokens`, which is a Vector of unique tokens\ntokens_set = Set{Token}()\nfor p in all_pools, t in p.inputTokens\n    push!(tokens_set, t)\nend\ntokens = collect(tokens_set);\n\n## the code below constructs `token_ids` which maps from\n# a `Token` to a\ntoken_ids = Dict(zip(tokens, 1:length(tokens)));\n\ntoken_ids\n\nfunction pools_for_token(t::Token)\n    bools = [t in p.inputTokens for p in all_pools]\n    return all_pools[bools]\nend\n\nfunction pools_for_token(sym::String)\n    bools = [t.symbol == sym for t in tokens]\n    token = tokens[bools]\n    @assert length(token) == 1 \"More than one token for this symbol\"\n    return pools_for_token(token[1])\nend\n\npools_for_token(\"WBTC\")\n\n","type":"content","url":"/l12-02-defi#data","position":35},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Exercise 1"},"type":"lvl2","url":"/l12-02-defi#exercise-1","position":36},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl2":"Exercise 1"},"content":"Using the routines above, construct a Simple (un-weighted, undirected) graph of tokens.\n\nThe number of nodes should be the number of tokens\n\nThe number of edges should be the number of pools\n\nAn edge should form between two nodes if we have a pool for those two tokens\n\n# Your code here\nusing Graphs\n\n","type":"content","url":"/l12-02-defi#exercise-1","position":37},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise 2","lvl2":"Exercise 1"},"type":"lvl3","url":"/l12-02-defi#exercise-2","position":38},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise 2","lvl2":"Exercise 1"},"content":"Using GraphPlot package, visualize the graph.\n\nSee if you can add node labels, perhaps only to the few nodes with the most edges.\n\n# your code here\nusing GraphPlot\n\n","type":"content","url":"/l12-02-defi#exercise-2","position":39},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise 3","lvl2":"Exercise 1"},"type":"lvl3","url":"/l12-02-defi#exercise-3","position":40},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise 3","lvl2":"Exercise 1"},"content":"Using the graph you just constructed, answer the following questions\n\nIs is possible to trade from any token to any other token?\n\nWhat is the overall clustering coefficient?\n\nWhich token is part of the most pools? What is the local clustering coefficient for this pool?\n\n# Your code here\n\n","type":"content","url":"/l12-02-defi#exercise-3","position":41},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise 4","lvl2":"Exercise 1"},"type":"lvl3","url":"/l12-02-defi#exercise-4","position":42},{"hierarchy":{"lvl1":"Defi: Money Legos","lvl3":"Exercise 4","lvl2":"Exercise 1"},"content":"We mentioned above that trades can take place between any two assets, even if there isn’t a pool for that pair\n\nThis happens through order routing\n\nIn practice, this is implemented as a smart contract called the router\n\nBecause we have studied network theory, we could implement a router by solving a shortest path problem...\n\nYour task here is to use the graph you formed before to compute the cheapest path possible to trade from the MKR token to the SAND token.\n\nMeaning, assuming we sell 10 MKR, which path through these pools gives the most possible units of SAND?\n\nA few tips:\n\nYou will have to use the x*y=k rule we learned earlier at each pool\n\nYou should never visit any pool more than once -- that would be a cycle and would be wasteful\n\n# Your code here","type":"content","url":"/l12-02-defi#exercise-4","position":43},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain"},"type":"lvl1","url":"/l12-03-nft","position":0},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain"},"content":"Computational Analysis of Social Complexity\n\nPrerequisites\n\nIntro to Blockchain\n\nEthereum: Blockchain 2.0\n\nOutcomes\n\nBe able to explain meaning of NFT to a friend or family member\n\nUnderstand the key features of the ERC-721 token standard\n\nSee example of creating an NFT commemorating completion of course (free for you to claim ;) )\n\nReferences\n\nhttps://medium.com/\n\nhttps://​www​.youtube​.com​/watch​?v​=​z8MCevWETm4\n\nhttps://​eips​.ethereum​.org​/EIPS​/eip​-721\n\nhttps://​docs​.openzeppelin​.com​/contracts​/2​.x​/api​/token​/erc721\n\nhttps://solana.com/\n\n","type":"content","url":"/l12-03-nft","position":1},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Review: Blockchains, Smart contracts, and Tokens"},"type":"lvl2","url":"/l12-03-nft#review-blockchains-smart-contracts-and-tokens","position":2},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Review: Blockchains, Smart contracts, and Tokens"},"content":"Over the past few weeks we have learned quite a bit about blockchains and cryptocurrencies\n\nA blockchain is a decentralized, distributed, public ledger\n\nPrimary application: means of recording transfers of crypto currencies (e.g. BTC)\n\nSmart contracts as we know them today started with Ethereum\n\nUse blockchain to store and execute computer programs\n\nTransactions can now include arbitrary logic... unlimited potential\n\nStandards have been developed to allow smart contracts to interact with one another\n\nThe ERC-20 standard defines what it means to be a “token”\n\nThere are tens of thousands of tokens on the Ethereum blockchain today (Nov. 2021)\n\nWe learned about decentralized finance (Defi)\n\nSystems of smart contracts implement financial primitives (trading, borrowing, insurance, lending, etc.)\n\nThese systems operate on the ERC-20 standard, making them composable and flexible\n\nOpportunity to define arbitrary financial systems (“I wrote financial systems into existence” Hamilton)\n\nToday we learn about NFTs...\n\n","type":"content","url":"/l12-03-nft#review-blockchains-smart-contracts-and-tokens","position":3},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"NFT??"},"type":"lvl2","url":"/l12-03-nft#nft","position":4},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"NFT??"},"content":"NFT stands for non-fungible token\n\nTwo parts:\n\nToken: we learned last week... backed by smart contract and can be secured/transferred on blockchain\n\nNon-fungible: adjective meaning cannot be copied, divided, or replaced with something else\n\nRecall what the USDC token looks like:\n\nname: USD Coin\n\nsymbol: USDC\n\ndecimals: 6\n\nThis means if you hold one unit of USDC, you have $1e-6\n\nAny single unit of USDC is equivalent to any other unit of USDC. You don’t care which one you have\n\n","type":"content","url":"/l12-03-nft#nft","position":5},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl3":"NFT Use Cases","lvl2":"NFT??"},"type":"lvl3","url":"/l12-03-nft#nft-use-cases","position":6},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl3":"NFT Use Cases","lvl2":"NFT??"},"content":"Art and collectables: \n\nMusic by Timbaland, \n\nOpenSea Gallary, \n\nAudius\n\nVideo Games: \n\nSandbox, \n\nAxieInfinity, \n\nDecentraland, \n\nGala, \n\nGods Unchained\n\nOwnership of Physical goods: car title, home deed, company ownership\n\nDefi applications: \n\nLiquidity provider positions on Uniswap V3, smart wallets, debt positions\n\n","type":"content","url":"/l12-03-nft#nft-use-cases","position":7},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl3":"History","lvl2":"NFT??"},"type":"lvl3","url":"/l12-03-nft#history","position":8},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl3":"History","lvl2":"NFT??"},"content":"There is some debate about where the idea of NFTs on a blockchain originated\n\nSome key projects that really brought them into the mainstream are:\n\nCryptopunks (early 2017): 10,000 unique, but randomly generated “pixel art” images\n\nCryptokitties (late 2017): buy, breed, sell game that nearly brought ethereum to a halt (caused increase in gas limit)\n\nCreation of many NFT platforms (2018-present): \n\nOpenSea, \n\nSuperRare, \n\nRarible, \n\nSolanart\n\nERC-721 (January 2018): proposal for formal standard/interface for NFTs on Ethereum\n\nBeeple’s NFT (early 2021): images from 5,000 days that sold for over $69 million in auction at Christie’s auction house\n\n","type":"content","url":"/l12-03-nft#history","position":9},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"How it works: ERC-721"},"type":"lvl2","url":"/l12-03-nft#how-it-works-erc-721","position":10},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"How it works: ERC-721"},"content":"The key interface for describing NFTs\n\nToo much code to put here, so we’ll review key concepts and then check out \n\nOpenZeppelin docs on ERC-721\n\nKey ideas:\n\nNFTS are grouped into a single smart contract and referenced by an integer index tokenId\n\nEach NFT is owned by exactly one address\n\nThe address (owner) must approve any other entity to be able to handle the NFT\n\nOnly one non-owner can be approved at a time\n\nIf recipient of transfer doesn’t know how to handle NFTs, could be lost forever... use safeTransferFrom instead of transferFrom\n\nExtensions to core standard allow associating metadata (images, files, movies, etc.) with token\n\n","type":"content","url":"/l12-03-nft#how-it-works-erc-721","position":11},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl3":"High Gas fees","lvl2":"How it works: ERC-721"},"type":"lvl3","url":"/l12-03-nft#high-gas-fees","position":12},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl3":"High Gas fees","lvl2":"How it works: ERC-721"},"content":"At times the ethereum blockchain can be very busy\n\nThis results in high transaction (gas) fees for doing operations like deploying an ERC-721 smart contract or selling/transferring NFTs\n\nThere are a few ways around this issue:\n\nWait for gas fees to be low\n\nUse a “Layer 2” solution (see next slide)\n\nUse an alternative blockchain: Solana is currently (Nov. 2022) next most popular for NFTs (see subsequent slides)\n\n","type":"content","url":"/l12-03-nft#high-gas-fees","position":13},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Layer 2"},"type":"lvl2","url":"/l12-03-nft#layer-2","position":14},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Layer 2"},"content":"The popularity of the Ethereum blockchain has led to high demand for block space and, as a result, high gas fees\n\nSince Ethereum was publically released there have been various advancements in theory and technology that allow for lower fees and higher throughput, without sacrificing security\n\nOptimistic Rollups\n\nZero knowledge proofs\n\nZero knowledge rollups:\n\nProof of Stake: no need for high mining reward to offset costs for miners’ energy use\n\nVideo here\n\nDisclaimer: this space is new and rapdily evolving -- I don’t have all the answers!\n\n","type":"content","url":"/l12-03-nft#layer-2","position":15},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Alternative Chains: Solana"},"type":"lvl2","url":"/l12-03-nft#alternative-chains-solana","position":16},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Alternative Chains: Solana"},"content":"Solana is branded as an “ethereum killer”\n\nIt is an entirely new blockchain built on cutting edge technology\n\nCurrently at over 2000 TPS, but with the capacity to increase that at least two orders of magnitude\n\nTransaction costs are very low (average less than $0.00025)\n\nActively growing ecosystem of dapps in the Defi and NFT spaces\n\n","type":"content","url":"/l12-03-nft#alternative-chains-solana","position":17},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Example: Class Badge"},"type":"lvl2","url":"/l12-03-nft#example-class-badge","position":18},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Example: Class Badge"},"content":"Let’s close out our semester by creating some NFTs!\n\nGoal: create an NFT signifying successful completion of the course\n\nSteps:\n\nGoogle Chrome (or \n\nBrave)\n\nMetamask wallet browser extension: \n\nhttps://​chrome​.google​.com​/webstore​/detail​/metamask​/nkbihfbeogaeaoehlefnkodbefgpgknn​?hl​=en\n\nAdd polygon network: \n\nhttps://​docs​.polygon​.technology​/docs​/develop​/metamask​/config​-polygon​-on​-metamask/\n\nCreate logo: \n\nhttps://looka.com/\n\nCreate NFT on \n\nopensea\n\nClaim NFT after we create it (or send me your wallet address and I’ll send it to you!)\n\n","type":"content","url":"/l12-03-nft#example-class-badge","position":19},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Summary"},"type":"lvl2","url":"/l12-03-nft#summary","position":20},{"hierarchy":{"lvl1":"NFTs: Asset Ownership on Blockchain","lvl2":"Summary"},"content":"Thank you for being in the class\n\nSecond run through -- some rough edges, but hopefully we all learned\n\nJulia is a very powerful and growing language for doing computational work -- it will be worthwhile in your data careers to know it\n\nTopics such as graph theory, game theory, and auction theory can be applied in a wide variety of settings\n\nBlockchain is a perfect blend of those tools + programming + data\n\nPrime example of applied “computational analysis of social complexity”\n\nIf blockchain succeeds at becoming infrastructure for future finance + web (not a given, but very possible) you will benefit from being “early”","type":"content","url":"/l12-03-nft#summary","position":21}]}