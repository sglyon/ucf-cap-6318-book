{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; TD learning\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Statistics and Probability\n",
    "- Dynamic Programming\n",
    "- Reinforcement Learning Introduction\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the meaning of the $Q(s, a)$ function\n",
    "- Understand the concept of a temporal difference\n",
    "- Apply temporal differences to form an RL algorithm (Sarsa)\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 4-6\n",
    "- [Stokey and Lucas (1989)](https://www.jstor.org/stable/j.ctvjnrt76) Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Reminder: Dynamic programming\n",
    "\n",
    "- Let's begin by recalling what we know about dynamic programming\n",
    "- Recall the cake eating problem:\n",
    "    - Time is discrete\n",
    "    - $\\beta$ is discount factor\n",
    "    - Size of cake is $\\bar{x}$\n",
    "    - Consumption of cake in period $t$ is $c_t$\n",
    "    - Utility function $u: \\mathbb{R} \\rightarrow \\mathbb{R}$ maps from consumption today into happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sequential Problem\n",
    "\n",
    "- Objective\n",
    "$$\\begin{aligned}\n",
    "\\max_{c_t} &\\sum_{t=0}^{\\infty} \\beta^t u(c_t) \\\\\n",
    "\\text{subject to } \\quad & \\sum_{t=0}^{\\infty} c_t \\le \\bar{x} \\\\\n",
    "& c_t \\ge 0 \\quad \\forall t\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Need to solve for *infinite* sequence $\\{c_t\\}_t$\n",
    "- Or..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Value Function\n",
    "\n",
    "- We can set up a value function $$v(\\bar{x}) \\equiv \\sum_{t=0}^{\\infty} \\beta^t u(c_t)$$\n",
    "- $v(\\bar{x})$ is the total *value* the consumer places on having a cake of size $\\bar{x}$\n",
    "- Decompose $v$ into two steps: first period + later periods $$v(\\bar{x}) = \\underbrace{u(c_0)}_{\\text{flow utility}} + \\underbrace{\\beta \\sum_{t=1}^{\\infty} \\beta^{t-1} u(c_t)}_{\\text{continuation utility}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Recursive Formulation\n",
    "\n",
    "- Note: continuation utility depends on $x_{t+1} = x_t - c_t$\n",
    "- Use this observation to write $v: \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ recursively:\n",
    "$$\\begin{aligned}\n",
    "  v(x_t) &= \\max_{0 \\leq c_t  \\leq x} \\underbrace{u(c_t)}_{\\text{flow utility}} + \\underbrace{\\beta v(x_t - c_t)}_{\\text{continuation value}}\n",
    "\\end{aligned}$$\n",
    "- This is known as the **Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Solution to Recursive Problem\n",
    "\n",
    "- A solution to the dynamic program consists of two functions:\n",
    "    1. **Value function** $v^*(x): \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ -- value of beginning period with $x$ cake remaining\n",
    "    2. **Policy function** $c^*(x): \\mathbb{R}^+ \\rightarrow [0, x]$ -- optimal level of consumption with $x$ cake remaining\n",
    "- Under certain regularity conditions (which we assume), the recursive problem (and its solution) is equivalent to the sequential problem we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Connection to RL\n",
    "\n",
    "- How does this connect to RL?\n",
    "- (S, A, R) pattern for RL is very closely related to recursive formulation of dynamic programming\n",
    "    - $S_t \\Longrightarrow x_t$\n",
    "    - $A_t \\Longrightarrow c_t$ \n",
    "    - $R_t \\Longrightarrow u(c_t)$\n",
    "- Expressing $v(x) = \\text{flow utility} + \\text{ continuation value}$ is like repeating (S, A, R) sequence many times\n",
    "- Baseline algorithm for solving DP problem (VFI) is quite similar to how basic RL algorithms work\n",
    "    - Start with guess for value, make decision, update guess, repeat..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## TD-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Baseline Assumptions\n",
    "\n",
    "- Let state space $\\mathcal{S}$ and action space $\\mathcal{A}$ be discrete\n",
    "- Let $S_t \\in \\mathcal{S}$  represent state at time $t$\n",
    "- Let $A_t \\in \\mathcal{A}(S_t)$ represent action at time $t$\n",
    "- Let $R_{t+1} \\in \\mathcal{R} \\subseteq \\mathbb{R}$ represent reward at time $t+1$\n",
    "- Let state transitions satisfy the Markov property such that \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& p(s', r | s, a) = \\text{Prob}(S_{t+1}=s', R_{t+1}=r | S_{t} = s, A_{t} = 1) \\\\\n",
    "\\text{ where } \\quad & \\sum_{s' \\in \\mathcal{S}} \\sum_{R \\in \\mathcal{R}} p(s', r | s, a) = 1 \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(S) \\\\\n",
    "\\text{ and } \\quad & p(s', r | s, a) \\ge = 0 \\quad \\forall s, s' \\in \\mathcal{S}, a \\in \\mathcal{A}(s), r \\in \\mathcal{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Note *Markov* means that probability for $S_{t+1}, R_{t+1}$ only depends on $S_t, A_t$ and not and $S_i, A_i, R_i$ where $i < t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### State Value Function\n",
    "\n",
    "- Let $v^*(s)$ be the optimal value of being in state $s$ (called *state value function*)\n",
    "- We write this as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^*(s) &= \\max_{a \\in \\mathcal{A}(s)} E \\left[R_{t+1} + \\beta v^*(S_{t+1}) | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\max_{a \\in \\mathcal{A}(s)} \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta v^*(s') \\right]\n",
    "\\end{aligned}$$\n",
    "- Should be familiar from our dynamic programming studies\n",
    "- Note expectation around the flow utility term $R_{t+1}$, leaving room for that reward to be stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Action Value Function\n",
    "\n",
    "- We can also write an *action value function*\n",
    "- Let $q^*(s, a)$ be the optimal value of being in state $s$ and choosing action $a$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q^*(s, a) &= E \\left[R_{t+1} + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(S_{t+1}, a') | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(s', a') \\right]\n",
    "\\end{aligned}$$\n",
    "- Notice max operator is now *inside* the expectation and applied to future decision $a'$\n",
    "- The function $q^*(s, a)$ is more general than $v^*(s)$: $$v^*(s) = \\max_{a \\in \\mathcal{A}(s)} q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Acting with $q^*$\n",
    "\n",
    "- Goal of RL is to learn to make decisions that maximize $\\sum \\beta^t R_t$\n",
    "- Knowing $q^*(s, a)$ tells us maximium value of being in state $s$ and choosing $a$\n",
    "- If we *knew* $q^*$, acting optimally would be easy: $$a^*(s) = \\text{argmax}_{a\n",
    "           \\in \\mathcal{A}(s)} q^*(s, a)$$\n",
    "- However, we rarely if ever *know* $q^*$, so we must approximate it\n",
    "- We will let $Q(s, a)$ represent our *approximation* of $q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Approximating $q^*$\n",
    "\n",
    "- The goal of TD learning is to find an accurate approximation $Q(s, a)$ such that $Q(s, a) \\approx q^*(s, a) \\; \\forall s, a)$\n",
    "- There are many RL algorithms that seek to do this\n",
    "- We'll focus on two:\n",
    "    - Sarsa\n",
    "    - Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Temporal Differences\n",
    "\n",
    "\n",
    "- The Bellman equaition for our approximation $Q(s, a)$ is $$Q(s,a) = E[R' + \\beta \\max_{a'} Q(s', a') | s,a]$$\n",
    "- Suppose that we interacted with environment and have in hand $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "- Now we plug these into the Bellman by:\n",
    "    1. Using the form of the Bellman\n",
    "    2. But drop $E$ and $\\max$ because we already know the transition that did occur from $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "    3. Change $=$ an $\\approx$ because this isn't evaluating full Bellman\n",
    "$$Q(S_t,A_t) \\approx R_{t+1} + \\beta  Q(S_{t+1}, A_{t+1})$$\n",
    "- The difference between the left and right and sides is known as a temporal difference: $$TD(0)(Q) \\equiv R_{t+1} + \\beta Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$$\n",
    "\n",
    "> There are extensions to the temporal difference that allow for multiple time periods. The 0 in $TD(0)$ indicates that this is *one-step* TD learning. See Chapters 7 and 12 of Sutton/Barto for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Learning using TD(0)\n",
    "\n",
    "- We can use temporal differences to *improve* our approximation $Q$:\n",
    "- Let $Q_t(s, a)$ represent our approximation at the start of period $t$\n",
    "- Similar to gradient descent methods, we will take a step from $Q_t$ in a direction that improves its accuracy\n",
    "- To improve accuracy we step in direction of $TD(0)(Q_t)$ (using step size $\\alpha$): \n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{t+1}(S_t, A_t) &= Q_t(S_t, A_t) + \\alpha TD(0)(Q_t) \\\\\n",
    "&= Q_t(S_t, A_t) + \\alpha \\left[R_{t+1} + \\beta Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t, A_t) \\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sarsa Algorithm\n",
    "\n",
    "- The Sarsa algorithm applies the update rule we just described\n",
    "- The algorithm is summarized by Barto and Sutton as follows (section 6.4)\n",
    "\n",
    "![sarsa_barto_sutton.png](./sarsa_barto_sutton.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ:\n",
    "    def __init__(self):\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "    \n",
    "    def __call__(self, s, a):\n",
    "        return self.Q[(s.observable_state(), a)]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        s, a = k\n",
    "        self.Q[(s.observable_state(), a)] = v\n",
    "        \n",
    "    def get_greedy(self, s, A_s):\n",
    "        vals = [self(s, a) for a in A_s]\n",
    "        max_val = max(vals)\n",
    "        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sarsa(object):\n",
    "    def __init__(self, environment, epsilon=0.9, alpha=0.1, beta=1.0):\n",
    "        self.env = environment\n",
    "        self.Q = TabularQ()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.restart_episode()    \n",
    "\n",
    "    def restart_episode(self):\n",
    "        # These will be (S, A) in our notation. Need to initialize\n",
    "        self.s = self.env.reset()\n",
    "        self.a = self.act(self.s, self.env.enumerate_options(self.s))        \n",
    "\n",
    "    def get_greedy(self, s, A_s):\n",
    "        return self.Q.get_greedy(s, A_s)\n",
    "    \n",
    "    def act(self, s, A_s):\n",
    "        if random.random() > self.epsilon:\n",
    "            return random.choice(A_s)\n",
    "        return self.get_greedy(s, A_s)\n",
    "\n",
    "    def done(self, s=None) -> bool:\n",
    "        return self.env.done(s if s else self.s)\n",
    "    \n",
    "    def step(self):\n",
    "        # first take the step (s, a)\n",
    "        s, a = self.s, self.a\n",
    "        sp, r = self.env.step(s, a)\n",
    "        \n",
    "        if self.done(sp):\n",
    "            # game is over\n",
    "            self.s = sp\n",
    "            return\n",
    "        \n",
    "        # then use policy to compute ap\n",
    "        A_sp = self.env.enumerate_options(sp)\n",
    "        ap = self.act(sp, A_sp)\n",
    "        \n",
    "        # now we know S-A-R-S'-A' -- ready to do update\n",
    "        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n",
    "        Q[(s, a)] = Q(s, a) + α * (r + β * Q(sp, ap) - Q(s, a))\n",
    "        \n",
    "        # step forward in time\n",
    "        self.s = sp \n",
    "        self.a = ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: Farkle\n",
    "\n",
    "- In a separate video we implemented the dice game farkle\n",
    "- We'll re-use that code as an environment for `Sarsa` algorithm\n",
    "- For a review of farkle, see video\n",
    "- Today we'll approach it like the RL algorihtm will: \n",
    "    - A stochastic environment that sends states, a list of possible actions, and rewards\n",
    "    - We will *not* specialize based on rules of game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Farkle env\n",
    "\n",
    "- Let's wrap farkle code into environment `Sarsa` expects\n",
    "- Need a few key methods:\n",
    "    - `reset() -> State`\n",
    "    - `enumerate_options(state) -> List[Action]`\n",
    "    - `step(s: State, a: Action) -> Tuple[State, Reward]`\n",
    "    - `done(s:State) -> bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import State, Action, RandomFarklePlayer, FarklePlayer, STOP, BANKRUPT\n",
    "from typing import List, Tuple\n",
    "\n",
    "class FarkleEnv:    \n",
    "    # first, some helper methods\n",
    "    def __init__(\n",
    "            self, \n",
    "            opponent: FarklePlayer=RandomFarklePlayer(), \n",
    "            points_to_win=10_000, \n",
    "            verbose: bool = False\n",
    "        ):\n",
    "        self.points_to_win = points_to_win\n",
    "        self.opponent = opponent\n",
    "        self.n_players = 2\n",
    "        self._state = State(self.n_players)\n",
    "        self._history: List[Tuple[State, Action]] = []\n",
    "\n",
    "    @property\n",
    "    def state(self) -> State:\n",
    "        return self._state\n",
    "\n",
    "    def set_state(self, action: Action, new_state: State):\n",
    "        self._history.append((self.state, action))\n",
    "        self._state = new_state\n",
    "    \n",
    "    def opponent_turn(self, s: State) -> State:\n",
    "        choices = s.enumerate_options()         \n",
    "        action = self.opponent.act(s, choices)\n",
    "        sp = s.step(action)\n",
    "\n",
    "        # check if player chose to stop\n",
    "        if sp.current_player != 1:\n",
    "            return sp\n",
    "\n",
    "        # Player didn't stop, but still their turn. Call again\n",
    "        return self.opponent_turn(sp)\n",
    "        \n",
    "    # key methods needed\n",
    "    def done(self, state) -> bool:\n",
    "        return any(score > self.points_to_win for score in state.scores)\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = State(self.n_players)\n",
    "        self._history = []\n",
    "        return self.state.roll()\n",
    "\n",
    "    def step(self, s: State, a: Action) -> Tuple[State, int]:\n",
    "        sp = s.step(a)\n",
    "        r = 0\n",
    "        \n",
    "        # see if we ended\n",
    "        if sp.current_player != 0:\n",
    "            if a is STOP:  \n",
    "                # only score when we choose to stop\n",
    "                r = s.turn_sum\n",
    "            \n",
    "            # take opponent turn\n",
    "            sp = self.opponent_turn(sp)\n",
    "            \n",
    "        self.set_state(a, sp)\n",
    "        return sp, r\n",
    "    \n",
    "    def enumerate_options(self, s: State) -> List[Action]:\n",
    "        return self.state.enumerate_options(s.rolled_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Playing Farkle\n",
    "\n",
    "- Let's try it out!\n",
    "- We need to create an env, then pass it to sarsa\n",
    "- We'll also define a helper function to play a game for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FarkleEnv()\n",
    "sarsa = Sarsa(env, epsilon=0.9, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_game(algo):\n",
    "    algo.restart_episode()\n",
    "    while not algo.done():\n",
    "        algo.step()\n",
    "    return algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Round: 130. Score: [10350, 8400]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(sarsa)\n",
    "sarsa.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x117e72fd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [h[0].scores for h in env._history]\n",
    "plt.plot(scores)\n",
    "plt.legend([\"sarsa\", \"random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import HumanFarklePlayer\n",
    "env_human = FarkleEnv(opponent=HumanFarklePlayer(name=\"Spencer\"), points_to_win=2000)\n",
    "sarsa_human = Sarsa(env_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(score: [0, 0]) You have rolled:  [] ( [] )\n",
      "The current total you have at stake is: 0\n",
      "Your scoring options are as follows:\n",
      "\t0: roll\n",
      "\t1: stop\n",
      "Input not understood, try again!\n",
      "(score: [0, 0]) You have rolled:  [⚀, ⚁, ⚁, ⚁, ⚂, ⚃] ( [1, 2, 2, 2, 3, 4] )\n",
      "The current total you have at stake is: 0\n",
      "Your scoring options are as follows:\n",
      "\t0: Play 1 to score 100\n",
      "\t1: Play Three 2's to score 200\n",
      "(score: [0, 0]) You have rolled:  [⚀, ⚂, ⚃] ( [1, 3, 4] )\n",
      "The current total you have at stake is: 200\n",
      "Your scoring options are as follows:\n",
      "\t0: Play 1 to score 100\n",
      "\t1: roll\n",
      "\t2: stop\n",
      "(score: [0, 0]) You have rolled:  [⚂, ⚃] ( [3, 4] )\n",
      "The current total you have at stake is: 300\n",
      "Your scoring options are as follows:\n",
      "\t0: roll\n",
      "\t1: stop\n",
      "(score: [0, 300]) You have rolled:  [] ( [] )\n",
      "The current total you have at stake is: 0\n",
      "Your scoring options are as follows:\n",
      "\t0: roll\n",
      "\t1: stop\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n",
      "Input not understood, try again!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fx/_k7mnq0x0lg851kcg9vwn1dh0000gn/T/ipykernel_30837/2370101386.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msarsa_human\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fx/_k7mnq0x0lg851kcg9vwn1dh0000gn/T/ipykernel_30837/3752859420.py\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(algo)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestart_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fx/_k7mnq0x0lg851kcg9vwn1dh0000gn/T/ipykernel_30837/2983899727.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# first take the step (s, a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fx/_k7mnq0x0lg851kcg9vwn1dh0000gn/T/ipykernel_30837/1609751820.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# take opponent turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopponent_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fx/_k7mnq0x0lg851kcg9vwn1dh0000gn/T/ipykernel_30837/1609751820.py\u001b[0m in \u001b[0;36mopponent_turn\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopponent_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Teaching/NYU/ml_course/S2023/ECON-GA-4005-S2023/Handouts/L16_rl_2_tdlearning/farkle.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, choices)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Choose an integer to select a scoring option: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                 \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/css/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             )\n\u001b[0;32m-> 1006\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/css/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "play_game(sarsa_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_human.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Learning\n",
    "\n",
    "- Great! Our algorithm can play Farkle\n",
    "- But... it needs to play *many* games to learn how to play well\n",
    "- Let's let it play many more games to build up some intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def play_many_games(N):\n",
    "    terminal_states = []\n",
    "    print_skip = N // 10\n",
    "    for i in range(N):\n",
    "        play_game(sarsa) \n",
    "        terminal_states.append(sarsa.s)\n",
    "        if i % print_skip == 0:\n",
    "            print(f\"Done with {i}/{N} (len(Q) = {len(sarsa.Q.Q)})\")\n",
    "    return terminal_states\n",
    "\n",
    "# WARNING: this takes a *long time* and requires a lot of ram!\n",
    "# Only use on a computer with at least 32 GB ram\n",
    "# There are ways we could optimize this... such as only including\n",
    "# final score in `terminal_states` and dropping things like current_round\n",
    "# from the state\n",
    "sarsa_history = play_many_games(200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Did we learn?\n",
    "\n",
    "- Let's analyze the history and see if the algorithm learned with experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won = np.array([s.scores[0] > s.scores[1] for s in sarsa_history])\n",
    "game_idx = np.arange(len(won))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(game_idx, won.cumsum())\n",
    "ax.plot(game_idx, 0.5 * game_idx)\n",
    "plt.legend([\"sarsa\", \"E[random agent]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "apple-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
