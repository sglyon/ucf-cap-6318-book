{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074259b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; Q-Learning\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Statistics and Probability\n",
    "- Dynamic Programming\n",
    "- Reinforcement Learning Introduction\n",
    "- Reinforcement Learning Sarsa algorithm\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Know the difference between on policy and off policy learning\n",
    "- Learn the Q-learning algorithm for off policy TD based control\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 4-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571ba58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Recap\n",
    "\n",
    "- In the RL problem agent observes $S$, makes decision $A$, sees reward and next state $R$, $S'$ -- then process repeats $S, A, R, S', A', ...$\n",
    "- Sarsa uses a $(S, A, R, S', A')$ quintuple to learn $Q(s, a)$ that approximates $q^*(s, a)$\n",
    "- Notice Sarsa uses $\\epsilon$-greedy policy to propose $a'$ **AND** uses that $A'$ when updating $Q$\n",
    "![sarsa_barto_sutton.png](./sarsa_barto_sutton.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b107c26",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### On-policy vs off-policy methods\n",
    "\n",
    "- Because Sarsa uses the $Q$ about which it is learning to generate $A'$, it is known as an *on-policy* learning method\n",
    "- On policy: make decisions based on value (policy) function being learned\n",
    "- Alternative: follow any policy for proposing $A'$, but use the greedy policy derived from $Q$ when computing $TD(0)$...\n",
    "- This is what we'll explore today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037f04c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Q-learning\n",
    "\n",
    "- An early theoretical breakthrough in RL was the idea of off-policy learning\n",
    "- The Q-learning algorithm was the first off-policy control algorithm to be suggested\n",
    "- It allows the algorithm to make use of $S, A, R, S'$ transitions obtained from *any* source, and still learn an approximation $Q$ that converges to $q^*$ with probability 1\n",
    "- Convergence requires some conditions, most importantly that the transitions $S, A, R, S'$ *cover* the action space of $q^*$\n",
    "- *Coverage* means all (s, a) pairs that are optimal under $q^*$ must be visited by the $S, A, R, S'$ transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8407ccb",
   "metadata": {},
   "source": [
    "### Example: Self-Driving Car\n",
    "\n",
    "- Goal: train RL agent to safely drive vechicle\n",
    "- Sarsa method:\n",
    "    - Give control of vehicle over to Sarsa, so it can choose $A$ and observe implied $R$, $S'$ transitions\n",
    "- Off-policy:\n",
    "    - Let human expert driver drive vehicle in intended way\n",
    "    - Record $S, A, R, S'$ transitions visited by human driver\n",
    "    - Train RL agent based on data generated from human experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c08bc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### The Q-learning Algorithm\n",
    "\n",
    "![q-learning_barto_sutton.png](./q-learning_barto_sutton.png)\n",
    "\n",
    "- $A$ that are suggested are $\\epsilon$-greedy in $Q$\n",
    "- This is a *suggestion* for how to generate $A$, but anything else (including totally random) could be used\n",
    "- When updating the $t+1$ component of $TD(0)(Q)$ there is an explicit $max_{a'} Q(S', a')$ -- it is always *greedy*\n",
    "<!-- - By computing $TD(0)(Q)$ updates that are greedy in $Q$, Q-learning can converge to $q^*$ regardless of how $A$ are generated -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d59166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Q-learning Farkle\n",
    "\n",
    "- Let's implement the Q-learning algorithm to solve our farkle game\n",
    "- First, some code optimizations:\n",
    "    1. `farkle.py` has been updated to include a method `State.observable_state`\n",
    "        - This method returns a tuple containing only how many dice are rollable, sum collected in turn, and what rolled dice are showing\n",
    "        - Drops scores, round, etc.\n",
    "        - Implication -- we will have agent learn to maximize score each turn\n",
    "        - If agent scores high every turn, should be able to win game\n",
    "        - Loses ability to customize behavior based on \"stage\" of game (aggressive play to catch up, or conservative to maintain lead)\n",
    "    2. Creatd a `TablularQ` class below that uses this `State.observable_state` method\n",
    "        - Allows RL algorithms to not worry about `State.observable_state`\n",
    "    3. Remove history tracking from `FarkleEnv` (see `FarkleEnv` in farkle.py file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef8e141",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfff08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ:\n",
    "    def __init__(self, default_value= lambda: 0):\n",
    "        val = default_value if callable(default_value) else lambda x: defaul_value\n",
    "        self.Q = defaultdict(lambda: default_value)\n",
    "    \n",
    "    def __call__(self, s, a):\n",
    "        return self.Q[(s.observable_state(), a)]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        s, a = k\n",
    "        self.Q[(s.observable_state(), a)] = v\n",
    "        \n",
    "    def get_greedy(self, s, A_s):\n",
    "        vals = [self(s, a) for a in A_s]\n",
    "        max_val = max(vals)\n",
    "        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1266d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Q-learning implementation\n",
    "\n",
    "- We implement Q-learning in the `QLearning` class below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qlearning(object):\n",
    "    def __init__(self, environment, default_value=0, epsilon=0.9, alpha=0.1, beta=1.0):\n",
    "        self.env = environment\n",
    "        self.Q = TabularQ(default_value=default_value)\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.restart_episode()    \n",
    "\n",
    "    def restart_episode(self):\n",
    "        self.s = self.env.reset()\n",
    "\n",
    "    def get_greedy(self, s, A_s):\n",
    "        return self.Q.get_greedy(s, A_s)\n",
    "    \n",
    "    def generate_A(self, s, A_s):\n",
    "        if random.random() > self.epsilon:\n",
    "            return random.choice(A_s)\n",
    "        return self.get_greedy(s, A_s)\n",
    "\n",
    "    def done(self, s=None) -> bool:\n",
    "        return self.env.done(s if s else self.s)\n",
    "    \n",
    "    def step(self):\n",
    "        s = self.s\n",
    "        # first generate an A\n",
    "        A_s = self.env.enumerate_options(s)\n",
    "        a = self.generate_A(s, A_s)\n",
    "\n",
    "        # take step\n",
    "        sp, r = self.env.step(s, a)\n",
    "        \n",
    "        if self.done(sp):\n",
    "            # game is over\n",
    "            self.s = sp\n",
    "            return\n",
    "        \n",
    "        # get greedy a' based on Q and sp\n",
    "        A_sp = self.env.enumerate_options(sp)\n",
    "        ap = self.get_greedy(sp, A_sp)\n",
    "        \n",
    "        # Do TD update\n",
    "        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n",
    "        Q[(s, a)] = Q(s, a) + α * (r + β * Q(sp, ap) - Q(s, a))\n",
    "        \n",
    "        # step forward in time\n",
    "        self.s = sp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2350343",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Single Game test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import FarkleEnv, play_game, play_many_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(40)\n",
    "\n",
    "env = FarkleEnv(track_history=False)\n",
    "ql = Qlearning(env)\n",
    "play_game(ql)\n",
    "ql.s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f7178",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Longer training\n",
    "\n",
    "- Let's now let our qlearning algorithm train on 5,000 games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f86614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(42)  # reset seed for reproducibility\n",
    "qlearning_history = play_many_games(ql, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4e198",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_win_rate(history):\n",
    "    won = np.array([s.scores[0] > s.scores[1] for s in history])\n",
    "    game_idx = np.arange(len(won))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(game_idx, won.cumsum())\n",
    "    ax.plot(game_idx, 0.5 * game_idx)\n",
    "    plt.legend([\"algo\", \"E[random agent]\"])\n",
    "    print(f\"won {sum(won)}/{len(won)} games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855c9bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_win_rate(qlearning_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed9bf9",
   "metadata": {},
   "source": [
    "- Excellent! Our Q-learning algorithm seems to be doing quite a bit better than we would expect a random agent to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd76d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
