{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating AI Systems: Testing Agents at Scale\n",
    "\n",
    "> Computational Analysis of Social Complexity\n",
    ">\n",
    "> Fall 2025, Spencer Lyon\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Pydantic AI Agents and Tools\n",
    "- Python programming fundamentals\n",
    "- Basic understanding of testing concepts\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand why systematic evaluation is critical for AI systems\n",
    "- Identify when and what to evaluate in AI agents\n",
    "- Implement deterministic and LLM-based evaluators\n",
    "- Design evaluation datasets using code-first approaches\n",
    "- Analyze and compare evaluation results across experiments\n",
    "- Connect evaluation practices to production deployment concerns\n",
    "\n",
    "**References**\n",
    "\n",
    "- [Pydantic AI Evals Documentation](https://ai.pydantic.dev/evals/index.md)\n",
    "- [Pydantic Evals API Reference](https://ai.pydantic.dev/api/pydantic_evals/dataset/index.md)\n",
    "- Testing practices from software engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: The AI Testing Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Testing AI is Different\n",
    "\n",
    "- Traditional software testing: deterministic inputs → deterministic outputs\n",
    "  - If `add(2, 3)` returns `5` once, it always will\n",
    "  - Clear pass/fail criteria\n",
    "- AI systems: same input → potentially different outputs\n",
    "  - Ask an agent \"What's the capital of France?\" twice, might get:\n",
    "    - \"The capital of France is Paris.\"\n",
    "    - \"Paris is France's capital city.\"\n",
    "    - \"France's capital is Paris, known for the Eiffel Tower.\"\n",
    "  - All correct, but different!\n",
    "- **Key challenge**: How do we test something that's non-deterministic?\n",
    "\n",
    "### Motivating Scenario: The Support Bot Problem\n",
    "\n",
    "- You've built a customer support agent using Pydantic AI\n",
    "- It handles questions about product returns, shipping, and account issues\n",
    "- In development, it seems to work great on the examples you tried\n",
    "- You deploy to production and...\n",
    "  - Sometimes gives outdated return policy information\n",
    "  - Occasionally \"hallucinates\" shipping partners that don't exist\n",
    "  - Has trouble with edge cases you didn't think to test\n",
    "- **Question**: How could systematic evaluation have caught these issues?\n",
    "\n",
    "### What We'll Learn\n",
    "\n",
    "- We'll explore **Pydantic Evals**, a framework for testing AI systems\n",
    "- Three main components:\n",
    "  1. **Datasets**: Collections of test scenarios\n",
    "  2. **Evaluators**: Scoring mechanisms to check outputs\n",
    "  3. **Experiments**: Runs that combine datasets and evaluators\n",
    "- Think of it like unit testing for AI:\n",
    "  - Cases + Evaluators = individual unit tests\n",
    "  - Datasets = test suites\n",
    "  - Experiments = running your entire test suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import Dependencies\n",
    "\n",
    "We'll need Pydantic AI and the evals module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pydantic-ai if needed\n",
    "# %pip install pydantic-ai[anthropic,evals] pydantic-evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_evals import Dataset, Case\n",
    "from pydantic_evals.evaluators import (\n",
    "    Evaluator,\n",
    "    EvaluatorContext,\n",
    "    EvaluationReason,\n",
    "    EqualsExpected,\n",
    "    Contains,\n",
    "    IsInstance,\n",
    "    MaxDuration,\n",
    "    LLMJudge,\n",
    "    HasMatchingSpan,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API Keys\n",
    "\n",
    "Make sure you have your Anthropic API key set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API key should be set in environment\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-key-here\"\n",
    "\n",
    "# Verify it's set\n",
    "assert \"ANTHROPIC_API_KEY\" in os.environ, \"Please set ANTHROPIC_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts: The Evaluation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure: Cases, Datasets, and Experiments\n",
    "\n",
    "**Cases**: Individual test scenarios\n",
    "\n",
    "- Like a single unit test\n",
    "- Contains:\n",
    "  - `inputs`: Data you pass to the agent\n",
    "  - `expected_output`: (Optional) What you expect back\n",
    "  - `metadata`: (Optional) Context about this test\n",
    "  - `evaluators`: (Optional) Case-specific checks\n",
    "\n",
    "**Datasets**: Collections of cases\n",
    "\n",
    "- Like a test suite\n",
    "- Groups related test scenarios\n",
    "- Can have dataset-level evaluators that apply to all cases\n",
    "\n",
    "**Experiments**: Evaluation runs\n",
    "\n",
    "- Like running `pytest` or `julia test`\n",
    "- Executes your task function on all cases\n",
    "- Applies evaluators to score outputs\n",
    "- Generates reports with results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Simple Support Bot Dataset\n",
    "\n",
    "Let's build intuition with a concrete example.\n",
    "\n",
    "Suppose we want to test our support bot's ability to identify the user's intent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Intent Classification Tests\n",
      "Number of cases: 3\n"
     ]
    }
   ],
   "source": [
    "# Define test cases\n",
    "intent_dataset = Dataset[str, str](\n",
    "    name=\"Intent Classification Tests\",\n",
    "    cases=[\n",
    "        Case(\n",
    "            name=\"return_request\",\n",
    "            inputs=\"I want to return my order\",\n",
    "            expected_output=\"return\"\n",
    "        ),\n",
    "        Case(\n",
    "            name=\"shipping_status\",\n",
    "            inputs=\"Where is my package?\",\n",
    "            expected_output=\"shipping\"\n",
    "        ),\n",
    "        Case(\n",
    "            name=\"account_question\",\n",
    "            inputs=\"How do I reset my password?\",\n",
    "            expected_output=\"account\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {intent_dataset.name}\")\n",
    "print(f\"Number of cases: {len(intent_dataset.cases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the pattern**:\n",
    "\n",
    "- Each case tests one scenario\n",
    "- We specify what we expect\n",
    "- Cases are typed: `Dataset[str, str]` means string inputs → string outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Design Your Own Cases\n",
    "\n",
    "**Scenario**: You're building a sentiment analysis agent for product reviews.\n",
    "\n",
    "**Task**: Following the pattern above, create a `Dataset[str, str]` with 3-5 test cases for an agent that should classify product reviews as:\n",
    "- \"positive\"\n",
    "- \"negative\" \n",
    "- \"neutral\"\n",
    "\n",
    "Think about:\n",
    "\n",
    "- What review texts represent typical positive/negative/neutral cases?\n",
    "- What are edge cases? (e.g., mixed sentiments, sarcasm)\n",
    "- Make sure each Case has:\n",
    "  - A descriptive `name`\n",
    "  - `inputs` (the review text)\n",
    "  - `expected_output` (the sentiment label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 0 cases\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your code here\n",
    "# Create a sentiment analysis dataset following the pattern above\n",
    "\n",
    "sentiment_dataset = Dataset[str, str](\n",
    "    name=\"Product Sentiment Analysis\",\n",
    "    cases=[\n",
    "        # TODO: Add your test cases here\n",
    "        # Case(\n",
    "        #     name=\"clearly_positive\",\n",
    "        #     inputs=\"This product exceeded my expectations! ...\",\n",
    "        #     expected_output=\"positive\"\n",
    "        # ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Created dataset with {len(sentiment_dataset.cases)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators: How to Score Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Types of Evaluation\n",
    "\n",
    "**Deterministic Evaluators**: Code-based checks\n",
    "\n",
    "- Exact matches\n",
    "- Type checking\n",
    "- Format validation (email, phone number, URL)\n",
    "- PII detection\n",
    "- Regular expression matching\n",
    "\n",
    "**Non-Deterministic Evaluators**: Subjective assessment\n",
    "\n",
    "- LLM as judge\n",
    "- Human evaluation\n",
    "- Quality metrics (accuracy, relevance, helpfulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Evaluators\n",
    "\n",
    "Pydantic Evals provides several ready-made evaluators:\n",
    "\n",
    "#### 1. Exact Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator: EqualsExpected()\n"
     ]
    }
   ],
   "source": [
    "# Check if output equals expected value\n",
    "evaluator = EqualsExpected()\n",
    "\n",
    "print(f\"Evaluator: {evaluator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Type Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator: IsInstance(type_name='str')\n"
     ]
    }
   ],
   "source": [
    "# Ensure output is correct type\n",
    "evaluator = IsInstance('str')\n",
    "\n",
    "print(f\"Evaluator: {evaluator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Membership/Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator: Contains(value='return policy')\n"
     ]
    }
   ],
   "source": [
    "# Check if key phrase appears in output\n",
    "evaluator = Contains('return policy')\n",
    "\n",
    "print(f\"Evaluator: {evaluator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Performance Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator: MaxDuration(seconds=2.0)\n"
     ]
    }
   ],
   "source": [
    "# Ensure agent responds quickly enough\n",
    "evaluator = MaxDuration(seconds=2.0)  # 2 seconds max\n",
    "\n",
    "print(f\"Evaluator: {evaluator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Adding Evaluators to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 2 evaluators\n",
      "Dataset has 3 cases\n"
     ]
    }
   ],
   "source": [
    "intent_dataset_with_evals = Dataset[str, str](\n",
    "    name=\"Intent Classification Tests\",\n",
    "    cases=[\n",
    "        Case(\n",
    "            name=\"return_request\",\n",
    "            inputs=\"I want to return my order\",\n",
    "            expected_output=\"return\"\n",
    "        ),\n",
    "        Case(\n",
    "            name=\"shipping_status\",\n",
    "            inputs=\"Where is my package?\",\n",
    "            expected_output=\"shipping\"\n",
    "        ),\n",
    "        Case(\n",
    "            name=\"account_question\",\n",
    "            inputs=\"How do I reset my password?\",\n",
    "            expected_output=\"account\"\n",
    "        ),\n",
    "    ],\n",
    "    evaluators=[\n",
    "        EqualsExpected(),  # Applied to all cases\n",
    "        MaxDuration(seconds=2.0)  # Response time check\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Dataset has {len(intent_dataset_with_evals.evaluators)} evaluators\")\n",
    "print(f\"Dataset has {len(intent_dataset_with_evals.cases)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM as Judge: When Correctness is Subjective\n",
    "\n",
    "- Sometimes there's no single \"correct\" answer\n",
    "- Example: \"Write a friendly response to this complaint\"\n",
    "  - Many valid responses exist\n",
    "  - Hard to check with deterministic rules\n",
    "- Solution: Use another LLM to evaluate\n",
    "\n",
    "#### LLMJudge Evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created LLMJudge evaluator\n"
     ]
    }
   ],
   "source": [
    "judge = LLMJudge(\n",
    "    rubric=(\n",
    "        \"Score from 0-10 on friendliness and helpfulness. \"\n",
    "        \"Friendly responses should acknowledge the customer's frustration. \"\n",
    "        \"Helpful responses should offer concrete next steps.\"\n",
    "    ),\n",
    "    model='anthropic:claude-haiku-4-5'\n",
    ")\n",
    "\n",
    "print(\"Created LLMJudge evaluator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works**:\n",
    "\n",
    "1. Your agent generates an output\n",
    "2. LLMJudge sends that output + rubric to an LLM\n",
    "3. LLM scores the output based on the rubric\n",
    "4. Score is recorded in the evaluation report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Evaluators: Domain-Specific Checks\n",
    "\n",
    "You can create custom evaluators by subclassing `Evaluator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created custom ContainsURL evaluator\n"
     ]
    }
   ],
   "source": [
    "class ContainsURL(Evaluator):\n",
    "    \"\"\"Check if output contains a valid URL.\"\"\"\n",
    "\n",
    "    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n",
    "        output = ctx.output\n",
    "        # Simple URL detection\n",
    "        has_url = 'http://' in str(output) or 'https://' in str(output)\n",
    "\n",
    "        return EvaluationReason(\n",
    "            value=has_url,\n",
    "            explanation=\"URL found\" if has_url else \"No URL found\"\n",
    "        )\n",
    "\n",
    "# Test it\n",
    "url_checker = ContainsURL()\n",
    "print(\"Created custom ContainsURL evaluator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key points**:\n",
    "\n",
    "- Implement `evaluate` method (can be sync or async)\n",
    "- Access inputs/outputs through `EvaluatorContext`\n",
    "- Return `EvaluationReason` with value and explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Design Evaluators\n",
    "\n",
    "For the sentiment analysis agent from Exercise 1:\n",
    "\n",
    "1. What **deterministic** evaluators would you use?\n",
    "   - Think about checking exact label matches, valid sentiment values\n",
    "2. What **subjective** aspects might need LLM evaluation?\n",
    "   - Example: When a review has mixed sentiment, is the chosen label reasonable?\n",
    "   - What would your LLMJudge rubric say?\n",
    "3. Design one **custom evaluator** for a domain-specific check\n",
    "   - Example: \"Output should be all lowercase\" or \"Response time should be fast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here\n",
    "# Create evaluators for sentiment analysis\n",
    "\n",
    "# Example deterministic evaluator\n",
    "# sentiment_dataset.evaluators.append(EqualsExpected())\n",
    "\n",
    "# Example custom evaluator\n",
    "# class ValidSentiment(Evaluator):\n",
    "#     \"\"\"Check if output is a valid sentiment label.\"\"\"\n",
    "#     def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n",
    "#         valid_sentiments = {\"positive\", \"negative\", \"neutral\"}\n",
    "#         is_valid = ctx.output.lower() in valid_sentiments\n",
    "#         return EvaluationReason(\n",
    "#             value=is_valid,\n",
    "#             explanation=f\"Output is {'valid' if is_valid else 'invalid'} sentiment\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Evaluations: From Code to Reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Evaluation Loop\n",
    "\n",
    "#### Step 1: Define your task function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created intent classification agent\n"
     ]
    }
   ],
   "source": [
    "# Create a simple intent classifier agent\n",
    "support_agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    system_prompt=(\n",
    "        \"You are a customer support intent classifier. \"\n",
    "        \"Classify user messages into one of: return, shipping, account. \"\n",
    "        \"Respond with ONLY the classification label, nothing else.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "async def classify_intent(inputs: str) -> str:\n",
    "    \"\"\"Our task: classify customer message intent.\"\"\"\n",
    "    result = await support_agent.run(inputs)\n",
    "    return result.output.lower().strip()\n",
    "\n",
    "print(\"Created intent classification agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c255567d3f4994bdf857d217e2e00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete!\n",
      "Evaluated 3 cases\n"
     ]
    }
   ],
   "source": [
    "# Run experiment\n",
    "report = await intent_dataset_with_evals.evaluate(\n",
    "    task=classify_intent,\n",
    "    max_concurrency=3,  # Run 3 cases in parallel\n",
    "    progress=True  # Show progress bar\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation complete!\")\n",
    "print(f\"Evaluated {len(report.cases)} cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">    Evaluation Summary: classify_intent     </span>\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID          </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\"> Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> return_request   </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │  736.7ms │\n",
       "├──────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> shipping_status  </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │     1.6s │\n",
       "├──────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> account_question </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │     1.0s │\n",
       "├──────────────────┼────────────┼──────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">         </span>│ 100.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>   │     1.1s │\n",
       "└──────────────────┴────────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m    Evaluation Summary: classify_intent     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAssertions\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDuration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1mreturn_request  \u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m         │  736.7ms │\n",
       "├──────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mshipping_status \u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m         │     1.6s │\n",
       "├──────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1maccount_question\u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m         │     1.0s │\n",
       "├──────────────────┼────────────┼──────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m│ 100.0% \u001b[32m✔\u001b[0m   │     1.1s │\n",
       "└──────────────────┴────────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Detailed results:\n",
      "==================================================\n",
      "\n",
      "Case: return_request\n",
      "  Input: I want to return my order\n",
      "  Output: return\n",
      "  Expected: return\n",
      "  Assertions: {'EqualsExpected': EvaluationResult(name='EqualsExpected', value=True, reason=None, source=EvaluatorSpec(name='EqualsExpected', arguments=None)), 'MaxDuration': EvaluationResult(name='MaxDuration', value=True, reason=None, source=EvaluatorSpec(name='MaxDuration', arguments=(2.0,)))}\n",
      "  All passed: True\n",
      "\n",
      "Case: shipping_status\n",
      "  Input: Where is my package?\n",
      "  Output: shipping\n",
      "  Expected: shipping\n",
      "  Assertions: {'EqualsExpected': EvaluationResult(name='EqualsExpected', value=True, reason=None, source=EvaluatorSpec(name='EqualsExpected', arguments=None)), 'MaxDuration': EvaluationResult(name='MaxDuration', value=True, reason=None, source=EvaluatorSpec(name='MaxDuration', arguments=(2.0,)))}\n",
      "  All passed: True\n",
      "\n",
      "Case: account_question\n",
      "  Input: How do I reset my password?\n",
      "  Output: account\n",
      "  Expected: account\n",
      "  Assertions: {'EqualsExpected': EvaluationResult(name='EqualsExpected', value=True, reason=None, source=EvaluatorSpec(name='EqualsExpected', arguments=None)), 'MaxDuration': EvaluationResult(name='MaxDuration', value=True, reason=None, source=EvaluatorSpec(name='MaxDuration', arguments=(2.0,)))}\n",
      "  All passed: True\n"
     ]
    }
   ],
   "source": [
    "# Print formatted report\n",
    "report.print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Detailed results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Or get the data programmatically\n",
    "for case in report.cases:\n",
    "    print(f\"\\nCase: {case.name}\")\n",
    "    print(f\"  Input: {case.inputs}\")\n",
    "    print(f\"  Output: {case.output}\")\n",
    "    print(f\"  Expected: {case.expected_output}\")\n",
    "    print(f\"  Assertions: {case.assertions}\")\n",
    "    print(f\"  All passed: {all(case.assertions.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results with Logfire\n",
    "\n",
    "While the printed reports are useful, **Pydantic Logfire** provides a web UI for visualizing and analyzing evaluation results over time.\n",
    "\n",
    "**Why use Logfire?**\n",
    "- Interactive dashboards for evaluation metrics\n",
    "- Trace exploration for debugging failed cases\n",
    "- Track trends across multiple evaluation runs\n",
    "- Team collaboration and sharing results\n",
    "\n",
    "Let's configure Logfire integration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: With Logfire configured, all subsequent evaluations will automatically send their results to the Logfire web UI. You can then:\n",
    "- View evaluation reports in an interactive dashboard\n",
    "- Explore individual traces and spans\n",
    "- Compare results across multiple runs\n",
    "- Set up alerts for failing evaluations\n",
    "\n",
    "Visit [logfire.pydantic.dev](https://logfire.pydantic.dev) to view your evaluation results.\n",
    "\n",
    "### Understanding Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfire configured!\n",
      "Future evaluations will automatically appear in Logfire web UI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=50384;https://logfire-us.pydantic.dev/sglyon/cap-6318-example\u001b\\\u001b[4;36mhttps://logfire-us.pydantic.dev/sglyon/cap-6318-example\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "import logfire\n",
    "\n",
    "# Configure Logfire\n",
    "# This will automatically send evaluation results to Logfire if token is present\n",
    "logfire.configure(\n",
    "    send_to_logfire='if-token-present',\n",
    "\n",
    ")\n",
    "\n",
    "print(\"Logfire configured!\")\n",
    "print(\"Future evaluations will automatically appear in Logfire web UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Experiments: Tracking Improvements\n",
    "\n",
    "Let's modify our agent and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce63410f23e431f806c69f6ff437550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:11:13.865 evaluate improved_classify_intent\n",
      "20:11:13.869   case: return_request\n",
      "20:11:13.869     execute improved_classify_intent\n",
      "20:11:13.871   case: shipping_status\n",
      "20:11:13.871     execute improved_classify_intent\n",
      "20:11:13.872   case: account_question\n",
      "20:11:13.872     execute improved_classify_intent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:11:14.569     evaluator: EqualsExpected\n",
      "20:11:14.569     evaluator: MaxDuration\n",
      "               case: return_request\n",
      "20:11:14.570     evaluator: EqualsExpected\n",
      "20:11:14.570     evaluator: MaxDuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               case: shipping_status\n",
      "20:11:14.963     evaluator: EqualsExpected\n",
      "20:11:14.963     evaluator: MaxDuration\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an improved agent with better prompt\n",
    "improved_agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    system_prompt=(\n",
    "        \"You are a customer support intent classifier. \"\n",
    "        \"Classify user messages into exactly one of these categories:\\n\"\n",
    "        \"- return: for return/refund requests\\n\"\n",
    "        \"- shipping: for delivery/tracking questions\\n\"\n",
    "        \"- account: for login/password/profile issues\\n\\n\"\n",
    "        \"Respond with ONLY the classification label in lowercase, nothing else.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "async def improved_classify_intent(inputs: str) -> str:\n",
    "    \"\"\"Improved task function.\"\"\"\n",
    "    result = await improved_agent.run(inputs)\n",
    "    return result.output.lower().strip()\n",
    "\n",
    "# Run evaluation with improved agent\n",
    "improved_report = await intent_dataset_with_evals.evaluate(\n",
    "    task=improved_classify_intent,\n",
    "    max_concurrency=3,\n",
    "    progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">     Evaluation Diff: classify_intent → improved_classify_intent      </span>\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Case ID          </span>┃<span style=\"font-weight: bold\"> Assertions </span>┃<span style=\"font-weight: bold\">                           Duration </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"font-weight: bold\"> account_question </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │ <span style=\"color: #008000; text-decoration-color: #008000\">1.0s → 696.5ms (-320.9ms / -31.5%)</span> │\n",
       "├──────────────────┼────────────┼────────────────────────────────────┤\n",
       "│<span style=\"font-weight: bold\"> return_request   </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │                  736.7ms → 700.2ms │\n",
       "├──────────────────┼────────────┼────────────────────────────────────┤\n",
       "│<span style=\"font-weight: bold\"> shipping_status  </span>│ <span style=\"color: #008000; text-decoration-color: #008000\">✔✔</span>         │    <span style=\"color: #008000; text-decoration-color: #008000\">1.6s → 1.1s (-544.0ms / -33.3%)</span> │\n",
       "├──────────────────┼────────────┼────────────────────────────────────┤\n",
       "│<span style=\"font-weight: bold\"> </span><span style=\"font-weight: bold; font-style: italic\">Averages</span><span style=\"font-weight: bold\">         </span>│ 100.0% <span style=\"color: #008000; text-decoration-color: #008000\">✔</span>   │ <span style=\"color: #008000; text-decoration-color: #008000\">1.1s → 829.4ms (-300.5ms / -26.6%)</span> │\n",
       "└──────────────────┴────────────┴────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m     Evaluation Diff: classify_intent → improved_classify_intent      \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mCase ID         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAssertions\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                          Duration\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[1m \u001b[0m\u001b[1maccount_question\u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m         │ \u001b[32m1.0s → 696.5ms (-320.9ms / -31.5%)\u001b[0m │\n",
       "├──────────────────┼────────────┼────────────────────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mreturn_request  \u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m         │                  736.7ms → 700.2ms │\n",
       "├──────────────────┼────────────┼────────────────────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1mshipping_status \u001b[0m\u001b[1m \u001b[0m│ \u001b[32m✔\u001b[0m\u001b[32m✔\u001b[0m         │    \u001b[32m1.6s → 1.1s (-544.0ms / -33.3%)\u001b[0m │\n",
       "├──────────────────┼────────────┼────────────────────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1;3mAverages\u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m│ 100.0% \u001b[32m✔\u001b[0m   │ \u001b[32m1.1s → 829.4ms (-300.5ms / -26.6%)\u001b[0m │\n",
       "└──────────────────┴────────────┴────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare against baseline\n",
    "improved_report.print(baseline=report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Run Your First Evaluation\n",
    "\n",
    "Using the sentiment analysis agent you designed in Exercise 1:\n",
    "\n",
    "1. Create an agent that performs sentiment classification\n",
    "2. Use your dataset from Exercise 1 (add more cases if needed)\n",
    "3. Add 2-3 evaluators (mix of built-in and custom from Exercise 2)\n",
    "4. Run the evaluation using `await dataset.evaluate(task=your_function)`\n",
    "5. Interpret the results:\n",
    "   - Which cases passed/failed?\n",
    "   - What patterns do you notice?\n",
    "   - What would you improve about the agent or the test cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics: Span-Based Evaluation and Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Span-Based Evaluation: Evaluating the Process, Not Just the Output\n",
    "\n",
    "**The Problem**: Sometimes the final answer is correct, but the *how* matters\n",
    "\n",
    "- Example: Math problem solving\n",
    "  - Output: \"42\" ✓ Correct!\n",
    "  - But did the agent:\n",
    "    - Use the right formula?\n",
    "    - Show its work?\n",
    "    - Make calculation errors that happened to cancel out?\n",
    "\n",
    "**Spans**: Execution traces from OpenTelemetry\n",
    "\n",
    "- Capture what the agent did internally\n",
    "- Tool calls made\n",
    "- LLM requests and responses\n",
    "- Intermediate reasoning steps\n",
    "\n",
    "#### HasMatchingSpan Evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure agent called a specific tool\n",
    "span_evaluator = HasMatchingSpan(\n",
    "    query={'name_contains': 'calculator_tool'},\n",
    "    evaluation_name='used_calculator'\n",
    ")\n",
    "\n",
    "print(\"Created span-based evaluator\")\n",
    "print(\"This evaluator checks that a span named 'calculator_tool' was called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why this matters**:\n",
    "\n",
    "- Catches \"lucky guesses\" where agent gets answer right for wrong reasons\n",
    "- Validates agent is following intended reasoning process\n",
    "- Useful for multi-step tasks where process correctness matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Datasets with LLMs\n",
    "\n",
    "**The Challenge**: Creating comprehensive test datasets is tedious\n",
    "\n",
    "- Need diverse inputs covering edge cases\n",
    "- Need correct expected outputs\n",
    "- Manual creation doesn't scale\n",
    "\n",
    "**Solution**: Use an LLM to generate test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_evals.generation import generate_dataset\n",
    "\n",
    "# Generate dataset for sentiment analysis\n",
    "generated_dataset = await generate_dataset(\n",
    "    dataset_type=Dataset[str, str],  # Input and output types\n",
    "    n_examples=5,  # Generate 5 test cases\n",
    "    extra_instructions=(\n",
    "        \"Create diverse restaurant reviews with clear sentiment. \"\n",
    "        \"Input should be a restaurant review. \"\n",
    "        \"Output should be the sentiment: positive, negative, or neutral.\"\n",
    "    ),\n",
    "    model='anthropic:claude-haiku-4-5'\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(generated_dataset.cases)} test cases\")\n",
    "for i, case in enumerate(generated_dataset.cases[:3]):\n",
    "    print(f\"\\nCase {i+1}:\")\n",
    "    print(f\"  Input: {case.inputs}\")\n",
    "    print(f\"  Expected: {case.expected_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it works**:\n",
    "\n",
    "1. You specify the dataset type with input/output types (e.g., `Dataset[str, str]`)\n",
    "2. LLM generates diverse test scenarios based on your instructions\n",
    "3. Returns properly structured `Dataset` object with generated cases\n",
    "4. Can optionally save to file for version control (using `path` parameter)\n",
    "\n",
    "**Best practices**:\n",
    "\n",
    "- Review generated cases before using\n",
    "- Mix generated and hand-crafted cases\n",
    "- Regenerate periodically to expand coverage\n",
    "- Use `extra_instructions` to guide the LLM toward specific edge cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating RAG Systems: A Two-Stage Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RAG?\n",
    "\n",
    "- RAG = Retrieval-Augmented Generation\n",
    "- Common pattern for AI agents that need to answer questions about documents/data\n",
    "- Two stages:\n",
    "  1. **Retrieval**: Find relevant documents/passages from knowledge base\n",
    "  2. **Generation**: Use retrieved context to generate answer\n",
    "\n",
    "#### Why RAG Evaluation is Different\n",
    "\n",
    "- Traditional evaluation: just check the final answer\n",
    "- RAG evaluation: need to check *both* stages\n",
    "  - Is the retrieval finding the right documents?\n",
    "  - Is the generation using those documents correctly?\n",
    "- Failure can happen at either stage (or both!)\n",
    "\n",
    "**Example Failure Modes**:\n",
    "- ✓ Retrieval works, ✗ Generation fails: Found right docs, but hallucinated answer\n",
    "- ✗ Retrieval fails, ✓ Generation works: Couldn't find relevant docs, so generated plausible but wrong answer\n",
    "- ✗ Both fail: Retrieved irrelevant docs *and* made up information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Evaluation Metrics\n",
    "\n",
    "**Retrieval Metrics** (Is the retrieval working?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecisionAtK(Evaluator):\n",
    "    \"\"\"Check if retrieved documents are relevant.\"\"\"\n",
    "\n",
    "    def __init__(self, k: int = 5):\n",
    "        self.k = k\n",
    "\n",
    "    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n",
    "        # Assume ctx.metadata contains retrieved doc IDs and ground truth\n",
    "        retrieved_docs = ctx.metadata.get('retrieved_doc_ids', [])[:self.k]\n",
    "        relevant_docs = ctx.metadata.get('relevant_doc_ids', [])\n",
    "\n",
    "        relevant_retrieved = set(retrieved_docs) & set(relevant_docs)\n",
    "        precision = len(relevant_retrieved) / self.k if self.k > 0 else 0\n",
    "\n",
    "        return EvaluationReason(\n",
    "            value=precision,\n",
    "            explanation=f\"Retrieved {len(relevant_retrieved)}/{self.k} relevant docs\"\n",
    "        )\n",
    "\n",
    "class RecallAtK(Evaluator):\n",
    "    \"\"\"Check if all relevant documents were found.\"\"\"\n",
    "\n",
    "    def __init__(self, k: int = 5):\n",
    "        self.k = k\n",
    "\n",
    "    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n",
    "        retrieved_docs = ctx.metadata.get('retrieved_doc_ids', [])[:self.k]\n",
    "        relevant_docs = ctx.metadata.get('relevant_doc_ids', [])\n",
    "\n",
    "        relevant_retrieved = set(retrieved_docs) & set(relevant_docs)\n",
    "        recall = len(relevant_retrieved) / len(relevant_docs) if len(relevant_docs) > 0 else 0\n",
    "\n",
    "        return EvaluationReason(\n",
    "            value=recall,\n",
    "            explanation=f\"Found {len(relevant_retrieved)}/{len(relevant_docs)} relevant docs\"\n",
    "        )\n",
    "\n",
    "print(\"Created RAG retrieval evaluators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generation Metrics** (Is the generation working?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faithfulness(Evaluator):\n",
    "    \"\"\"Check if answer is grounded in retrieved context.\"\"\"\n",
    "\n",
    "    async def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n",
    "        answer = ctx.output\n",
    "        context = ctx.metadata.get('retrieved_context', '')\n",
    "\n",
    "        # Use LLM to judge faithfulness\n",
    "        judge = Agent('anthropic:claude-haiku-4-5')\n",
    "        result = await judge.run(\n",
    "            f\"Context: {context}\\n\\n\"\n",
    "            f\"Answer: {answer}\\n\\n\"\n",
    "            f\"Is the answer fully supported by the context? \"\n",
    "            f\"Respond with YES, NO, or PARTIAL and explain why.\"\n",
    "        )\n",
    "\n",
    "        assessment = result.data\n",
    "        is_faithful = assessment.startswith('YES')\n",
    "\n",
    "        return EvaluationReason(\n",
    "            value=is_faithful,\n",
    "            explanation=assessment\n",
    "        )\n",
    "\n",
    "print(\"Created Faithfulness evaluator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for RAG Evaluation\n",
    "\n",
    "**1. Evaluate Stages Independently**\n",
    "\n",
    "Create separate datasets for retrieval and generation to pinpoint where failures occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Retrieval evaluation dataset\n",
    "retrieval_dataset = Dataset(\n",
    "    name=\"Retrieval Quality\",\n",
    "    cases=[\n",
    "        Case(\n",
    "            inputs={\"query\": \"What are the return policies?\"},\n",
    "            metadata={\n",
    "                \"relevant_doc_ids\": [\"doc_42\", \"doc_87\"],  # Ground truth\n",
    "                \"retrieved_doc_ids\": [\"doc_42\", \"doc_87\", \"doc_13\", \"doc_99\", \"doc_5\"],  # Simulated retrieval\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    "    evaluators=[\n",
    "        PrecisionAtK(k=5),\n",
    "        RecallAtK(k=5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Created retrieval evaluation dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why separate?**\n",
    "- Pinpoints where failures occur\n",
    "- Can optimize retrieval and generation independently\n",
    "- Clearer diagnosis: \"Our retrieval is great but generation hallucinates\" vs \"Both need work\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Design RAG Evaluation\n",
    "\n",
    "**Scenario**: You're building a RAG system that answers questions about a company's internal documentation.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "1. **Identify failure modes**: What are 3 ways this RAG system could fail?\n",
    "2. **Design retrieval tests**: What cases would test if retrieval is working?\n",
    "   - What queries should always retrieve specific documents?\n",
    "   - What edge cases might break retrieval?\n",
    "3. **Design generation tests**: Assuming perfect retrieval, how do you test generation?\n",
    "   - What makes a \"good\" answer?\n",
    "   - How do you detect hallucinations?\n",
    "4. **Create evaluation pipeline**: Sketch code for evaluating both stages\n",
    "   - What metrics would you track?\n",
    "   - How would you report results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Evaluation Type\n",
    "\n",
    "**Deterministic Evaluators** when:\n",
    "\n",
    "- Clear right/wrong answers exist\n",
    "- Output format matters (structured data)\n",
    "- Security/safety constraints (no PII leakage)\n",
    "- Performance requirements (latency, cost)\n",
    "\n",
    "**LLM as Judge** when:\n",
    "\n",
    "- Multiple valid answers exist\n",
    "- Quality is subjective (helpfulness, tone)\n",
    "- Semantic equivalence matters (\"Paris\" vs \"The capital of France is Paris\")\n",
    "\n",
    "**Span-Based Evaluation** when:\n",
    "\n",
    "- Process correctness matters, not just output\n",
    "- Multi-step reasoning needs validation\n",
    "- Tool usage patterns are important\n",
    "- Debugging complex agent behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for Effective Evaluation\n",
    "\n",
    "**Start Small, Grow Gradually**:\n",
    "\n",
    "- Begin with 5-10 cases covering main scenarios\n",
    "- Add cases as you discover failures\n",
    "- Prioritize cases that would impact users most\n",
    "\n",
    "**Balance Coverage and Maintainability**:\n",
    "\n",
    "- Don't try to test everything\n",
    "- Focus on high-risk or high-value scenarios\n",
    "- Remove redundant cases\n",
    "\n",
    "**Make Evaluators Specific and Clear**:\n",
    "\n",
    "- Good rubric: \"Score 0-10 on factual accuracy. Check claims against provided context.\"\n",
    "- Bad rubric: \"Score the quality of the response.\"\n",
    "\n",
    "**Version Control Your Datasets**:\n",
    "\n",
    "- Store datasets as YAML/JSON in git\n",
    "- Track changes over time\n",
    "- Share across team\n",
    "\n",
    "**Automate Where Possible**:\n",
    "\n",
    "- Run evals in CI/CD pipeline\n",
    "- Block deploys if pass rate drops\n",
    "- Generate alerts for regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Evaluation Strategy Design\n",
    "\n",
    "Choose one scenario:\n",
    "\n",
    "1. **E-commerce support agent**: Handles returns, shipping, account questions\n",
    "2. **Code review agent**: Reviews pull requests, suggests improvements\n",
    "3. **Data analysis agent**: Answers questions about datasets using pandas\n",
    "\n",
    "For your chosen scenario:\n",
    "\n",
    "1. Design an evaluation strategy:\n",
    "   - What's in your test dataset? (10+ cases)\n",
    "   - What evaluators would you use?\n",
    "   - How would you measure success?\n",
    "2. Describe your development workflow:\n",
    "   - When do you run evals?\n",
    "   - What metrics do you track?\n",
    "   - How do you decide when to deploy?\n",
    "3. Plan for production:\n",
    "   - How do you handle failures?\n",
    "   - When do you update your dataset?\n",
    "   - What triggers re-evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections to Course Themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Theory and Evaluation\n",
    "\n",
    "- **Agent alignment**: Evaluation as mechanism design\n",
    "  - You design rubrics (rules) to incentivize desired behaviors\n",
    "  - LLM as Judge is like a referee in a game\n",
    "  - Pass/fail thresholds create strategic constraints\n",
    "\n",
    "- **Adversarial evaluation**: Red team vs Blue team\n",
    "  - Attackers try to make agent fail (jailbreaking, prompt injection)\n",
    "  - Defenders build evals that catch these attacks\n",
    "  - Nash equilibrium between robustness and capability\n",
    "\n",
    "### Network Effects in AI Systems\n",
    "\n",
    "- **Evaluation datasets as networks**:\n",
    "  - Cases can have dependencies (one builds on another)\n",
    "  - Failures can cascade (if base functionality breaks, many cases fail)\n",
    "  - Coverage metrics: are there \"clusters\" of untested scenarios?\n",
    "\n",
    "- **Agent-to-agent evaluation**:\n",
    "  - Multi-agent systems need coordinated evaluation\n",
    "  - Agent A's outputs become Agent B's inputs\n",
    "  - Network of evals reflects agent interaction topology\n",
    "\n",
    "### Emergence in Complex AI Systems\n",
    "\n",
    "- **Emergent behaviors** in multi-step agents:\n",
    "  - Simple agent + simple tools → complex behaviors\n",
    "  - Can't predict all outcomes from components\n",
    "  - Evaluation discovers emergent capabilities (and failures)\n",
    "\n",
    "- **Evaluation as an ABM simulation**:\n",
    "  - Each test case is like running the simulation once\n",
    "  - Aggregate results reveal patterns\n",
    "  - Edge cases show boundary conditions of agent \"behavior space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Learned\n",
    "\n",
    "1. **Why Evals Matter**\n",
    "   - AI systems are non-deterministic\n",
    "   - Systematic testing catches issues before production\n",
    "   - Evals enable confident iteration and deployment\n",
    "\n",
    "2. **Core Framework: Pydantic Evals**\n",
    "   - Cases: individual test scenarios\n",
    "   - Datasets: collections of cases\n",
    "   - Evaluators: scoring mechanisms (deterministic, LLM, custom)\n",
    "   - Experiments: runs that generate reports\n",
    "\n",
    "3. **Evaluation Strategies**\n",
    "   - Deterministic checks for clear criteria\n",
    "   - LLM as Judge for subjective quality\n",
    "   - Span-based evaluation for process correctness\n",
    "   - Custom evaluators for domain-specific needs\n",
    "   - RAG-specific metrics for retrieval + generation\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Start small, iterate based on failures\n",
    "   - Balance coverage with maintainability\n",
    "   - Version control datasets and track metrics\n",
    "   - Integrate into development and deployment workflows\n",
    "\n",
    "### Looking Forward\n",
    "\n",
    "- Evaluations are \"an emerging art/science\"\n",
    "- No single \"right\" approach exists\n",
    "- Adapt techniques to your domain and constraints\n",
    "- Key principle: **Test systematically, deploy confidently**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Exercise: Reflection\n",
    "\n",
    "Think about an AI agent you might build:\n",
    "\n",
    "1. What are the top 3 risks or failure modes?\n",
    "2. How would you design evals to catch those?\n",
    "3. What would \"success\" look like quantitatively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Pydantic AI Evals Documentation](https://ai.pydantic.dev/evals/index.md)\n",
    "- [Dataset API Reference](https://ai.pydantic.dev/api/pydantic_evals/dataset/index.md)\n",
    "- [Evaluators API Reference](https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.md)\n",
    "- [Pydantic Logfire for Visualization](https://ai.pydantic.dev/logfire/index.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
