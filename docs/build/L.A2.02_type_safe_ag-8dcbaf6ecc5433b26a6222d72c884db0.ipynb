{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "source": [
    "# Type-Safe Agent Development with PydanticAI Patterns\n",
    "\n",
    "> Computational Analysis of Social Complexity\n",
    ">\n",
    "> Fall 2025, Spencer Lyon\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Function calling concepts (L.A2.01)\n",
    "- Python type hints and type safety\n",
    "- Basic agent architectures\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand type-safe agent development principles\n",
    "- Implement validation-first agent architectures with Pydantic\n",
    "- Apply dependency injection patterns for testable agents\n",
    "- Build production-ready AI agents with PydanticAI\n",
    "\n",
    "**References**\n",
    "\n",
    "- [PydanticAI Documentation](https://ai.pydantic.dev/)\n",
    "- [Pydantic Documentation](https://docs.pydantic.dev/)\n",
    "- [Python Type Hints](https://docs.python.org/3/library/typing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-bcde-f23456789012",
   "metadata": {},
   "source": [
    "# Why Type Safety Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-a7b8-9012-cdef-234567890123",
   "metadata": {},
   "source": [
    "## The Problem: Unvalidated Agent Systems\n",
    "\n",
    "- Consider a simple agent that processes user queries and makes database updates\n",
    "- What could go wrong?\n",
    "  - Wrong data types passed to functions\n",
    "  - Invalid values that violate business logic\n",
    "  - Runtime errors deep in execution\n",
    "  - Silent failures that corrupt data\n",
    "- These bugs are *expensive* in production\n",
    "- Type safety catches entire classes of errors before they happen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-b8c9-0123-def0-345678901234",
   "metadata": {},
   "source": [
    "## A Concrete Example\n",
    "\n",
    "Suppose we're building a research assistant agent that:\n",
    "1. Takes a research question\n",
    "2. Searches academic databases\n",
    "3. Returns structured citations\n",
    "\n",
    "Without type safety:\n",
    "```python\n",
    "# Python - no validation\n",
    "def search_papers(query, max_results, min_year):\n",
    "    # What if query is None?\n",
    "    # What if max_results is negative?\n",
    "    # What if min_year is 99999?\n",
    "    ...\n",
    "```\n",
    "\n",
    "With type safety:\n",
    "```python\n",
    "# Python with Pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str = Field(min_length=1, max_length=500)\n",
    "    max_results: int = Field(ge=1, le=100)\n",
    "    min_year: int = Field(ge=1900, le=2025)\n",
    "\n",
    "def search_papers(request: SearchRequest):\n",
    "    # Guaranteed valid inputs!\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-c9d0-1234-ef01-456789012345",
   "metadata": {},
   "source": [
    "## Key Benefits\n",
    "\n",
    "**Fail Fast, Fail Loud**\n",
    "- Invalid inputs rejected immediately\n",
    "- Clear error messages\n",
    "- No silent corruption\n",
    "\n",
    "**Self-Documenting Code**\n",
    "- Types tell you what's expected\n",
    "- IDE autocomplete and hints\n",
    "- Less need for comments\n",
    "\n",
    "**Refactoring Confidence**\n",
    "- Change a type definition\n",
    "- Compiler/type checker finds all affected code\n",
    "- Safe to modify large systems\n",
    "\n",
    "**Testing Made Easier**\n",
    "- Don't need to test invalid types\n",
    "- Focus on business logic\n",
    "- Reduce test surface area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-d0e1-2345-f012-567890123456",
   "metadata": {},
   "source": [
    "# The PydanticAI Philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0-e1f2-3456-0123-678901234567",
   "metadata": {},
   "source": [
    "## What is PydanticAI?\n",
    "\n",
    "- PydanticAI is a Python framework for building production-ready AI agents\n",
    "- Created by the team behind Pydantic\n",
    "- Pydantic powers the OpenAI SDK, Anthropic SDK, FastAPI, and countless other production systems\n",
    "- Philosophy: **validation-first development**\n",
    "\n",
    "**Why PydanticAI?**\n",
    "- Type safety catches errors before runtime\n",
    "- Validation ensures data integrity\n",
    "- Production-ready patterns from day one\n",
    "- Seamless integration with Python's type system\n",
    "- Used by major companies in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4567-1234-789012345678",
   "metadata": {},
   "source": [
    "## Chef's Knife and Cutting Board\n",
    "\n",
    "- PydanticAI describes itself as providing a \"chef's knife and cutting board\"\n",
    "- What does this mean?\n",
    "  - **NOT a framework** with opinions about everything\n",
    "  - **IS a toolkit** with powerful, composable primitives\n",
    "  - Simple tools that do one thing well\n",
    "  - Combine them however you want\n",
    "\n",
    "**Contrast with LangChain**:\n",
    "- LangChain: Full kitchen with every appliance\n",
    "  - Chains, memory, vector stores, callbacks, etc.\n",
    "  - Complex abstractions\n",
    "  - Steep learning curve\n",
    "  - Hard to customize\n",
    "- PydanticAI: Essential tools\n",
    "  - Agent, tools, structured outputs\n",
    "  - Simple abstractions\n",
    "  - Easy to understand\n",
    "  - Flexible customization\n",
    "\n",
    "**When to Use Which**:\n",
    "- Use PydanticAI when: building production systems, need type safety, want simplicity\n",
    "- Use LangChain when: rapid prototyping, need batteries-included features, okay with complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-a3b4-5678-2345-890123456789",
   "metadata": {},
   "source": [
    "## Core Principles\n",
    "\n",
    "**1. Type Safety First**\n",
    "- All inputs validated before use\n",
    "- All outputs structured and validated\n",
    "- Catch errors at design time, not runtime\n",
    "\n",
    "**2. Validation Before Computation**\n",
    "- Never process invalid data\n",
    "- Fail immediately with clear messages\n",
    "- Make invalid states unrepresentable\n",
    "\n",
    "**3. Dependency Injection**\n",
    "- Agents don't create their dependencies\n",
    "- Dependencies passed in at construction\n",
    "- Easy to test with mocks\n",
    "- Easy to swap implementations\n",
    "\n",
    "**4. Explicit Over Implicit**\n",
    "- No magic global state\n",
    "- No hidden configuration\n",
    "- Everything visible in function signatures\n",
    "\n",
    "**5. Production Ready**\n",
    "- Designed for real systems, not demos\n",
    "- Handles errors gracefully\n",
    "- Observability built-in\n",
    "- Performance matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3-b4c5-6789-3456-901234567890",
   "metadata": {},
   "source": [
    "# Core Agent Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-7890-4567-012345678901",
   "metadata": {},
   "source": [
    "## The Four Essential Pieces\n",
    "\n",
    "Every PydanticAI agent has these components:\n",
    "\n",
    "**1. Agent Definition**\n",
    "- Specifies the LLM model to use\n",
    "- Defines system prompt\n",
    "- Registers available tools\n",
    "\n",
    "**2. Tool Functions**\n",
    "- Functions the agent can call\n",
    "- Type-annotated parameters\n",
    "- Validated inputs and outputs\n",
    "\n",
    "**3. Structured Outputs**\n",
    "- Define what the agent should return\n",
    "- Pydantic models for validation\n",
    "- Guaranteed schema compliance\n",
    "\n",
    "**4. Run Context (Dependencies)**\n",
    "- External services (databases, APIs)\n",
    "- Configuration\n",
    "- State that flows through execution\n",
    "\n",
    "Let's examine each in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5-d6e7-8901-5678-123456789012",
   "metadata": {},
   "source": [
    "## 1. Agent Definition\n",
    "\n",
    "In PydanticAI (Python):\n",
    "```python\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    system_prompt=\"You are a helpful research assistant.\",\n",
    ")\n",
    "```\n",
    "\n",
    "Key aspects:\n",
    "- **Model string**: Specifies which LLM to use\n",
    "- **System prompt**: Sets agent behavior and role\n",
    "- **Simple constructor**: No complex configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-9012-6789-234567890123",
   "metadata": {},
   "source": [
    "## 2. Tool Functions\n",
    "\n",
    "Tools are functions the agent can call:\n",
    "\n",
    "```python\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('anthropic:claude-haiku-4-5')\n",
    "\n",
    "@agent.tool\n",
    "def search_papers(\n",
    "    query: str,\n",
    "    max_results: int = 10\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search academic papers by query.\"\"\"\n",
    "    # Implementation here\n",
    "    return results\n",
    "```\n",
    "\n",
    "**What happens**:\n",
    "1. LLM sees tool name and docstring\n",
    "2. LLM decides when to call it\n",
    "3. Parameters are validated against type hints\n",
    "4. Function executes\n",
    "5. Return value validated\n",
    "6. Result given back to LLM\n",
    "\n",
    "**Type safety at every step!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7-f8a9-0123-7890-345678901234",
   "metadata": {},
   "source": [
    "## 3. Structured Outputs\n",
    "\n",
    "Define exactly what you want back:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    title: str\n",
    "    authors: list[str]\n",
    "    year: int\n",
    "    doi: str | None = None\n",
    "\n",
    "class ResearchResult(BaseModel):\n",
    "    summary: str\n",
    "    citations: list[Citation]\n",
    "    confidence: float\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    output_type=ResearchResult\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- LLM must return this exact structure\n",
    "- Automatic validation\n",
    "- No parsing strings or JSON\n",
    "- IDE autocomplete on results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-a9b0-1234-8901-456789012345",
   "metadata": {},
   "source": [
    "## 4. Run Context (Dependencies)\n",
    "\n",
    "The secret sauce for testability:\n",
    "\n",
    "```python\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Dependencies:\n",
    "    db: DatabaseClient\n",
    "    api_key: str\n",
    "    max_retries: int = 3\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=Dependencies\n",
    ")\n",
    "\n",
    "@agent.tool\n",
    "def search_papers(\n",
    "    ctx: RunContext[Dependencies],\n",
    "    query: str\n",
    ") -> list[dict]:\n",
    "    # Access dependencies via ctx.deps\n",
    "    return ctx.deps.db.search(query)\n",
    "```\n",
    "\n",
    "**Why this matters**:\n",
    "- No global variables\n",
    "- Easy to inject mock dependencies for testing\n",
    "- Explicit about what each tool needs\n",
    "- Can have different deps for dev/prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-b0c1-2345-9012-567890123456",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "A complete PydanticAI agent:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 1. Define dependencies\n",
    "@dataclass\n",
    "class ResearchDeps:\n",
    "    db: DatabaseClient\n",
    "    api_key: str\n",
    "\n",
    "# 2. Define output structure\n",
    "class Citation(BaseModel):\n",
    "    title: str\n",
    "    authors: list[str]\n",
    "    year: int\n",
    "\n",
    "class ResearchResult(BaseModel):\n",
    "    summary: str\n",
    "    citations: list[Citation]\n",
    "\n",
    "# 3. Create agent\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=ResearchDeps,\n",
    "    result_type=ResearchResult,\n",
    "    system_prompt='You are a research assistant.'\n",
    ")\n",
    "\n",
    "# 4. Register tools\n",
    "@agent.tool\n",
    "def search_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    query: str,\n",
    "    max_results: int = 10\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search academic database.\"\"\"\n",
    "    return ctx.deps.db.search(query, limit=max_results)\n",
    "\n",
    "# 5. Run agent\n",
    "async def run_research(question: str):\n",
    "    deps = ResearchDeps(\n",
    "        db=get_database(),\n",
    "        api_key=get_api_key()\n",
    "    )\n",
    "    result = await agent.run(question, deps=deps)\n",
    "    return result.data  # Guaranteed to be ResearchResult!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0-e1f2-3457-0123-678901234568",
   "metadata": {},
   "source": [
    "# Building a Research Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4568-1234-789012345679",
   "metadata": {},
   "source": [
    "## System Design\n",
    "\n",
    "Let's build a complete research assistant that demonstrates all the PydanticAI concepts we've learned:\n",
    "\n",
    "**Features**:\n",
    "- Search academic papers by query\n",
    "- Get detailed paper information\n",
    "- Generate structured research summaries\n",
    "- Type-safe dependencies and outputs\n",
    "- Multiple tools working together\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "PydanticAI Agent (with system prompt)\n",
    "    ‚Üì\n",
    "Tool Selection (agent decides which tools to use)\n",
    "    ‚Üì\n",
    "Tool Execution (with validated inputs via RunContext)\n",
    "    ‚Üì\n",
    "Result Synthesis (agent combines tool outputs)\n",
    "    ‚Üì\n",
    "Structured Output (validated ResearchSummary)\n",
    "```\n",
    "\n",
    "This is a realistic pattern you'd use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-a3b4-5679-2345-890123456780",
   "metadata": {},
   "source": [
    "## Step 1: Define Output Structure\n",
    "\n",
    "First, define what we want the agent to return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e1f2a3-b4c5-6780-3456-901234567891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Validation caught error: ValidationError\n",
      "3 validation errors for Paper\n",
      "title\n",
      "  String should have at least 1 character [type=string_too_short, input_value='', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/string_too_short\n",
      "authors\n",
      "  List should have at least 1 item after validation, not 0 [type=too_short, input_value=[], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/too_short\n",
      "year\n",
      "  Input should be less than or equal to 2025 [type=less_than_equal, input_value=3000, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/less_than_equal\n",
      "‚úì Valid paper created: Attention Is All You Need\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Academic paper with validated fields.\"\"\"\n",
    "    title: str = Field(min_length=1)\n",
    "    authors: list[str] = Field(min_length=1)\n",
    "    year: int = Field(ge=1800, le=2025)\n",
    "    abstract: str\n",
    "    doi: Optional[str] = None\n",
    "\n",
    "class ResearchSummary(BaseModel):\n",
    "    \"\"\"Final research output with validation.\"\"\"\n",
    "    query: str = Field(min_length=1)\n",
    "    summary: str = Field(min_length=1, description=\"Natural language summary\")\n",
    "    key_papers: list[Paper] = Field(min_length=1, description=\"Most relevant papers found\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Agent's confidence that an answer to the user's query has been found\")\n",
    "\n",
    "    def _repr_html_(self) -> str:\n",
    "        \"\"\"Rich HTML display for Jupyter notebooks.\"\"\"\n",
    "        papers_html = \"\"\n",
    "        for i, paper in enumerate(self.key_papers, 1):\n",
    "            authors = \", \".join(paper.authors)\n",
    "            doi_link = f'<a href=\"https://doi.org/{paper.doi}\" target=\"_blank\">DOI</a>' if paper.doi else \"\"\n",
    "            papers_html += f\"\"\"\n",
    "            <div style=\"margin: 10px 0; padding: 10px; background: #f8f9fa; border-left: 3px solid #007bff;\">\n",
    "                <strong>{i}. {paper.title}</strong> ({paper.year}) {doi_link}<br>\n",
    "                <em>{authors}</em><br>\n",
    "                <small style=\"color: #666;\">{paper.abstract[:200]}...</small>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "\n",
    "        confidence_color = \"#28a745\" if self.confidence >= 0.7 else \"#ffc107\" if self.confidence >= 0.4 else \"#dc3545\"\n",
    "        confidence_pct = f\"{self.confidence:.0%}\"\n",
    "\n",
    "        return f\"\"\"\n",
    "        <div style=\"font-family: sans-serif; max-width: 800px;\">\n",
    "            <h3 style=\"color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px;\">\n",
    "                üìö Research Results: \"{self.query}\"\n",
    "            </h3>\n",
    "            <div style=\"margin: 15px 0; padding: 15px; background: #e7f3ff; border-radius: 5px;\">\n",
    "                <strong>Summary:</strong><br>\n",
    "                {self.summary}\n",
    "            </div>\n",
    "            <div style=\"margin: 10px 0;\">\n",
    "                <strong>Confidence:</strong>\n",
    "                <span style=\"color: {confidence_color}; font-weight: bold;\">{confidence_pct}</span>\n",
    "            </div>\n",
    "            <div style=\"margin-top: 20px;\">\n",
    "                <strong>Key Papers ({len(self.key_papers)}):</strong>\n",
    "                {papers_html}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Plain text representation.\"\"\"\n",
    "        lines = [\n",
    "            f\"Research Results: {self.query}\",\n",
    "            \"=\" * 60,\n",
    "            f\"\\nSummary:\\n{self.summary}\",\n",
    "            f\"\\nConfidence: {self.confidence:.0%}\",\n",
    "            f\"\\nKey Papers ({len(self.key_papers)}):\",\n",
    "        ]\n",
    "\n",
    "        for i, paper in enumerate(self.key_papers, 1):\n",
    "            lines.append(f\"\\n{i}. {paper.title} ({paper.year})\")\n",
    "            lines.append(f\"   Authors: {', '.join(paper.authors)}\")\n",
    "            if paper.doi:\n",
    "                lines.append(f\"   DOI: {paper.doi}\")\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Test that validation works\n",
    "try:\n",
    "    invalid = Paper(title=\"\", authors=[], year=3000, abstract=\"test\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úì Validation caught error: {type(e).__name__}\")\n",
    "\n",
    "    # print the error\n",
    "    print(e)\n",
    "\n",
    "valid = Paper(\n",
    "    title=\"Attention Is All You Need\",\n",
    "    authors=[\"Vaswani et al.\"],\n",
    "    year=2017,\n",
    "    abstract=\"We propose the Transformer...\"\n",
    ")\n",
    "print(f\"‚úì Valid paper created: {valid.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-7891-4567-012345678902",
   "metadata": {},
   "source": [
    "## Step 2: Define Dependencies\n",
    "\n",
    "Define external resources the agent needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2a3b4c5-d6e7-8902-5678-123456789013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies configured\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ResearchDeps:\n",
    "    \"\"\"Dependencies for the research agent.\"\"\"\n",
    "    database_url: str\n",
    "    api_key: str\n",
    "    max_papers_per_search: int = 10\n",
    "\n",
    "    def search_database(self, query: str, limit: int) -> list[Paper]:\n",
    "        \"\"\"Simulate database search (in production, would query real DB).\"\"\"\n",
    "        print(f\"  üîç Searching database for: '{query}' (limit: {limit})\")\n",
    "\n",
    "        # In production, this would be:\n",
    "        # conn = connect(self.database_url, self.api_key)\n",
    "        # results = conn.execute(search_query)\n",
    "        # return [Paper(**row) for row in results]\n",
    "\n",
    "        # For demo, return mock results\n",
    "        return [\n",
    "            Paper(\n",
    "                title=\"Attention Is All You Need\",\n",
    "                authors=[\"Vaswani, A.\", \"Shazeer, N.\", \"Parmar, N.\"],\n",
    "                year=2017,\n",
    "                abstract=\"We propose a new simple network architecture, the Transformer...\",\n",
    "                doi=\"10.5555/3295222.3295349\"\n",
    "            ),\n",
    "            Paper(\n",
    "                title=\"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "                authors=[\"Devlin, J.\", \"Chang, M.\", \"Lee, K.\"],\n",
    "                year=2019,\n",
    "                abstract=\"We introduce BERT, a method for pre-training language representations...\",\n",
    "                doi=\"10.18653/v1/N19-1423\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_paper_by_id(self, paper_id: str) -> Paper:\n",
    "        \"\"\"Retrieve specific paper by ID.\"\"\"\n",
    "        print(f\"  üìÑ Fetching paper: {paper_id}\")\n",
    "        return Paper(\n",
    "            title=\"Example Paper\",\n",
    "            authors=[\"Author, A.\"],\n",
    "            year=2023,\n",
    "            abstract=\"This is an example paper abstract.\"\n",
    "        )\n",
    "\n",
    "# Create dependencies\n",
    "deps = ResearchDeps(\n",
    "    database_url=\"sqlite3:///papers.db\",\n",
    "    api_key=\"demo-api-key-12345\",\n",
    "    max_papers_per_search=5\n",
    ")\n",
    "\n",
    "print(\"‚úì Dependencies configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-9013-6789-234567890124",
   "metadata": {},
   "source": [
    "## Step 3: Create Agent and Register Tools\n",
    "\n",
    "Now create the agent and give it tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4c5d6e7-f8a9-0124-7890-345678901235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent created with 2 tools registered\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "# Create agent with system prompt and structured output\n",
    "research_agent = Agent(\n",
    "    'anthropic:claude-sonnet-4-5',\n",
    "    deps_type=ResearchDeps,\n",
    "    output_type=ResearchSummary,\n",
    "    system_prompt=\"\"\"You are a helpful research assistant.\n",
    "\n",
    "    When asked to research a topic:\n",
    "    1. Search for relevant papers using the search_papers tool\n",
    "    2. Analyze the results carefully\n",
    "    3. Provide a clear summary with the most important papers\n",
    "    4. Rate your confidence based on result quality and relevance\n",
    "\n",
    "    Be thorough but concise. Focus on the most impactful papers.\n",
    "    IMPORTANT: never, under any circumstances, make up paper titles, authors, years, abstracts, or DOIs.\n",
    "\n",
    "    IMPORTANT: If you cannot find relevant papers, state that clearly in your summary and set confidence to 0.0. Also don't return any key papers.\n",
    "\n",
    "    IMPORTANT: only use details from the papers you find when generating the summary. do not invent information or use general knowledge.\n",
    "               again if you cannot find relevant papers, state that clearly in your summary and set confidence to 0.0.\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Register tool 1: Search for papers\n",
    "@research_agent.tool\n",
    "def search_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    query: str,\n",
    "    max_results: int = 5\n",
    ") -> list[Paper]:\n",
    "    \"\"\"\n",
    "    Search for academic papers by query string.\n",
    "\n",
    "    Args:\n",
    "        query: Search query (keywords, topics, authors)\n",
    "        max_results: Maximum number of papers to return (default 5)\n",
    "\n",
    "    Returns:\n",
    "        List of relevant papers\n",
    "    \"\"\"\n",
    "    # Access dependencies via ctx.deps\n",
    "    limit = min(max_results, ctx.deps.max_papers_per_search)\n",
    "    papers = ctx.deps.search_database(query, limit)\n",
    "    return papers\n",
    "\n",
    "# Register tool 2: Get specific paper details\n",
    "@research_agent.tool\n",
    "def get_paper_details(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    paper_id: str\n",
    ") -> Paper:\n",
    "    \"\"\"\n",
    "    Get detailed information about a specific paper by its ID.\n",
    "\n",
    "    Args:\n",
    "        paper_id: Unique identifier for the paper\n",
    "\n",
    "    Returns:\n",
    "        Paper object with full details\n",
    "    \"\"\"\n",
    "    return ctx.deps.get_paper_by_id(paper_id)\n",
    "\n",
    "print(\"‚úì Agent created with 2 tools registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-e9f0-1235-8901-456789012346",
   "metadata": {},
   "source": [
    "## Step 4: Run the Agent\n",
    "\n",
    "Execute the agent with a research query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39a53924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import UsageLimits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9-b0c1-2346-9012-567890123457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üîç Searching database for: 'monkeys species North America distribution' (limit: 5)\n",
      "  üîç Searching database for: 'primate diversity North America native species' (limit: 5)\n",
      "  üîç Searching database for: 'New World monkeys United States Canada Mexico biogeography' (limit: 5)\n",
      "  üîç Searching database for: 'non-human primates North American continent native endemic' (limit: 5)\n",
      "  üîç Searching database for: 'mammal fauna North America primates absence' (limit: 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3697, in run_code\n",
      "    await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/7d/x0yzvh3172zdzj7wsxtnhhh00000gn/T/ipykernel_4094/1915870946.py\", line 14, in <module>\n",
      "    research_result = await run_research(\"How many species of monkeys live in north america\")\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7d/x0yzvh3172zdzj7wsxtnhhh00000gn/T/ipykernel_4094/1915870946.py\", line 7, in run_research\n",
      "    result = await research_agent.run(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/agent/abstract.py\", line 225, in run\n",
      "    async with self.iter(\n",
      "               ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "    await self.gen.athrow(value)\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/agent/__init__.py\", line 647, in iter\n",
      "    async with graph.iter(\n",
      "               ^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "    await self.gen.athrow(value)\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_graph/beta/graph.py\", line 271, in iter\n",
      "    async with GraphRun[StateT, DepsT, OutputT](\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_graph/beta/graph.py\", line 400, in __aexit__\n",
      "    await self._async_exit_stack.__aexit__(exc_type, exc_val, exc_tb)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 754, in __aexit__\n",
      "    raise exc_details[1]\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 735, in __aexit__\n",
      "    cb_suppress = cb(*exc_details)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_graph/beta/graph.py\", line 939, in _unwrap_exception_groups\n",
      "    raise exception\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_graph/beta/graph.py\", line 711, in _run_tracked_task\n",
      "    result = await self._run_task(t_)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_graph/beta/graph.py\", line 740, in _run_task\n",
      "    output = await node.call(step_context)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_graph/beta/step.py\", line 253, in _call_node\n",
      "    return await node.run(GraphRunContext(state=ctx.state, deps=ctx.deps))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 544, in run\n",
      "    async with self.stream(ctx):\n",
      "               ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.12/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 217, in __aexit__\n",
      "    await anext(self.gen)\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 558, in stream\n",
      "    async for _event in stream:\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 676, in _run_stream\n",
      "    async for event in self._events_iterator:\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 637, in _run_stream\n",
      "    async for event in self._handle_tool_calls(ctx, tool_calls):\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 692, in _handle_tool_calls\n",
      "    async for event in process_tool_calls(\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 888, in process_tool_calls\n",
      "    async for event in _call_tools(\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/_agent_graph.py\", line 955, in _call_tools\n",
      "    usage_limits.check_before_tool_call(projected_usage)\n",
      "  File \"/Users/sglyon/Teaching/UCF/CAP-6318/book-myst/weekA02/.venv/lib/python3.12/site-packages/pydantic_ai/usage.py\", line 393, in check_before_tool_call\n",
      "    raise UsageLimitExceeded(\n",
      "pydantic_ai.exceptions.UsageLimitExceeded: The next tool call(s) would exceed the tool_calls_limit of 5 (tool_calls=6).\n"
     ]
    }
   ],
   "source": [
    "async def run_research(question: str):\n",
    "    deps = ResearchDeps(\n",
    "        database_url=\"sqlite3:///papers.db\",\n",
    "        api_key=\"demo-api-key-12345\",\n",
    "        max_papers_per_search=5\n",
    "    )\n",
    "    result = await research_agent.run(\n",
    "        question,\n",
    "        deps=deps,\n",
    "        usage_limits=UsageLimits(tool_calls_limit=5),\n",
    "    )\n",
    "    return result\n",
    "\n",
    "research_result = await run_research(\"How many species of monkeys live in north america\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content='You are a helpful research assistant.\\n\\n    When asked to research a topic:\\n    1. Search for relevant papers using the search_papers tool\\n    2. Analyze the results carefully\\n    3. Provide a clear summary with the most important papers\\n    4. Rate your confidence based on result quality and relevance\\n\\n    Be thorough but concise. Focus on the most impactful papers.\\n    IMPORTANT: never, under any circumstances, make up paper titles, authors, years, abstracts, or DOIs.\\n\\n    IMPORTANT: If you cannot find relevant papers, state that clearly in your summary and set confidence to 0.0.\\n\\n    IMPORTANT: only use details from the papers you find when generating the summary. do not invent information or use general knowledge.\\n               again if you cannot find relevant papers, state that clearly in your summary and set confidence to 0.0.\\n\\n    ', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 8, 103168, tzinfo=datetime.timezone.utc)), UserPromptPart(content='How many species of monkeys live in north america', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 8, 103172, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='search_papers', args={'query': 'monkey species North America distribution native', 'max_results': 10}, tool_call_id='toolu_015iVGTiBwvEf1vSB4GDtH8s')], usage=RequestUsage(input_tokens=1387, output_tokens=62, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1387, 'output_tokens': 62}), model_name='claude-sonnet-4-5-20250929', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 10, 617960, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_01AEYVeiqSynQXnyaVLd4osj', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_015iVGTiBwvEf1vSB4GDtH8s', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 10, 618748, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='search_papers', args={'query': 'primate diversity North America biogeography', 'max_results': 10}, tool_call_id='toolu_01EBPBpzy5uiXzoc292kcW2M')], usage=RequestUsage(input_tokens=1627, output_tokens=64, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1627, 'output_tokens': 64}), model_name='claude-sonnet-4-5-20250929', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 13, 304667, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_014HCpQLThvjGCvj9HgFs2dH', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_01EBPBpzy5uiXzoc292kcW2M', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 13, 305500, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='search_papers', args={'query': 'native monkeys Mexico Central America United States', 'max_results': 10}, tool_call_id='toolu_01AVt8UCiMyphHSzmTC8BxYG')], usage=RequestUsage(input_tokens=1869, output_tokens=64, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1869, 'output_tokens': 64}), model_name='claude-sonnet-4-5-20250929', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 16, 474688, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_01YAhvsa4njfepdKacmsotjQ', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_01AVt8UCiMyphHSzmTC8BxYG', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 16, 476643, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='search_papers', args={'query': 'New World monkeys biogeography geographic range', 'max_results': 10}, tool_call_id='toolu_01KDUPKnpMeDUsvWXGsvnsbW')], usage=RequestUsage(input_tokens=2111, output_tokens=65, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2111, 'output_tokens': 65}), model_name='claude-sonnet-4-5-20250929', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 20, 354698, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_01Ay7fCeJ5QezNvGbetcRw76', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_01KDUPKnpMeDUsvWXGsvnsbW', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 20, 356167, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='search_papers', args={'query': 'Platyrrhini species richness northern limit', 'max_results': 10}, tool_call_id='toolu_017bNCgZUUaXoTRkJeZbxbLw')], usage=RequestUsage(input_tokens=2354, output_tokens=66, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2354, 'output_tokens': 66}), model_name='claude-sonnet-4-5-20250929', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 23, 447720, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_017djSEjhtoeJ3xUFs5puZ96', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_017bNCgZUUaXoTRkJeZbxbLw', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 23, 448744, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='final_result', args={'query': 'How many species of monkeys live in north america', 'summary': 'I was unable to find relevant academic papers addressing the question of how many monkey species live in North America. The search queries returned unrelated papers (about machine learning transformers and language models) rather than papers about primate biogeography or species distribution in North America. \\n\\nWithout access to relevant scientific literature through this search tool, I cannot provide a scientifically-supported answer to this question based on the available search results.', 'key_papers': [{'title': 'Attention Is All You Need', 'authors': ['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], 'year': 2017, 'abstract': 'We propose a new simple network architecture, the Transformer...'}], 'confidence': 0.0}, tool_call_id='toolu_01PikVbFar7CsAZBpN2JDEBr')], usage=RequestUsage(input_tokens=2598, output_tokens=264, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2598, 'output_tokens': 264}), model_name='claude-sonnet-4-5-20250929', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 29, 127835, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_01TEjdUsCEHBazcEzoEdfoUd', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='final_result', content='Final result processed.', tool_call_id='toolu_01PikVbFar7CsAZBpN2JDEBr', timestamp=datetime.datetime(2025, 11, 11, 0, 53, 29, 128273, tzinfo=datetime.timezone.utc))])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_result.all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: sans-serif; max-width: 800px;\">\n",
       "            <h3 style=\"color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px;\">\n",
       "                üìö Research Results: \"How many species of monkeys live in north america\"\n",
       "            </h3>\n",
       "            <div style=\"margin: 15px 0; padding: 15px; background: #e7f3ff; border-radius: 5px;\">\n",
       "                <strong>Summary:</strong><br>\n",
       "                I was unable to find relevant academic papers addressing the question of how many monkey species live in North America. The search queries returned unrelated papers (about machine learning transformers and language models) rather than papers about primate biogeography or species distribution in North America. \n",
       "\n",
       "Without access to relevant scientific literature through this search tool, I cannot provide a scientifically-supported answer to this question based on the available search results.\n",
       "            </div>\n",
       "            <div style=\"margin: 10px 0;\">\n",
       "                <strong>Confidence:</strong>\n",
       "                <span style=\"color: #dc3545; font-weight: bold;\">0%</span>\n",
       "            </div>\n",
       "            <div style=\"margin-top: 20px;\">\n",
       "                <strong>Key Papers (1):</strong>\n",
       "                \n",
       "            <div style=\"margin: 10px 0; padding: 10px; background: #f8f9fa; border-left: 3px solid #007bff;\">\n",
       "                <strong>1. Attention Is All You Need</strong> (2017) <br>\n",
       "                <em>Vaswani, A., Shazeer, N., Parmar, N.</em><br>\n",
       "                <small style=\"color: #666;\">We propose a new simple network architecture, the Transformer......</small>\n",
       "            </div>\n",
       "            \n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "ResearchSummary(query='How many species of monkeys live in north america', summary='I was unable to find relevant academic papers addressing the question of how many monkey species live in North America. The search queries returned unrelated papers (about machine learning transformers and language models) rather than papers about primate biogeography or species distribution in North America. \\n\\nWithout access to relevant scientific literature through this search tool, I cannot provide a scientifically-supported answer to this question based on the available search results.', key_papers=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi=None)], confidence=0.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0-c1d2-3457-0123-678901234568",
   "metadata": {},
   "source": [
    "## What Just Happened?\n",
    "\n",
    "Let's break down the agent execution:\n",
    "\n",
    "**1. Agent Received Query**\n",
    "- User asked: \"transformer architectures in NLP\"\n",
    "- Agent has access to: search_papers and get_paper_details tools\n",
    "\n",
    "**2. Agent Made Decisions**\n",
    "- LLM analyzed the query\n",
    "- Decided to call `search_papers(\"transformer architectures in NLP\", max_results=5)`\n",
    "- PydanticAI validated the parameters\n",
    "- Tool executed with access to deps via RunContext\n",
    "\n",
    "**3. Tool Returned Validated Data**\n",
    "- Tool returned `list[Paper]` (type-checked)\n",
    "- Each Paper object validated by Pydantic\n",
    "- Agent received the results\n",
    "\n",
    "**4. Agent Synthesized Response**\n",
    "- LLM analyzed the papers\n",
    "- Generated natural language summary\n",
    "- Selected key papers\n",
    "- Assigned confidence score\n",
    "- Returned ResearchSummary (validated!)\n",
    "\n",
    "**5. Type Safety Throughout**\n",
    "- Every input validated before execution\n",
    "- Every output validated before return\n",
    "- Impossible to get wrong types\n",
    "- Clear errors if validation fails\n",
    "\n",
    "This is the power of type-safe agent development!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1-d2e3-4568-1234-789012345679",
   "metadata": {},
   "source": [
    "## Exercise: Add a New Tool\n",
    "\n",
    "Add a `compare_papers` tool that takes two paper titles and returns a comparison.\n",
    "\n",
    "**Requirements**:\n",
    "1. Use `@research_agent.tool` decorator\n",
    "2. Accept `RunContext[ResearchDeps]` as first parameter\n",
    "3. Take two paper titles as strings\n",
    "4. Return a string with the comparison\n",
    "5. Use proper docstring (the agent sees this!)\n",
    "\n",
    "**Hint**: The agent will automatically see this new tool and can call it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b0c1d2-e3f4-5679-2345-890123456780",
   "metadata": {},
   "outputs": [],
   "source": [
    "@research_agent.tool\n",
    "def compare_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    paper1_title: str,\n",
    "    paper2_title: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compare two papers by their titles.\n",
    "\n",
    "    Args:\n",
    "        paper1_title: Title of first paper\n",
    "        paper2_title: Title of second paper\n",
    "\n",
    "    Returns:\n",
    "        Comparison summary\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison logic\n",
    "    # In production, would search for both papers and compare:\n",
    "    # - Publication dates\n",
    "    # - Citation counts\n",
    "    # - Research methods\n",
    "    # - Key contributions\n",
    "\n",
    "    return f\"Comparison between '{paper1_title}' and '{paper2_title}': [TODO: implement]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3-f4a5-6780-3456-901234567891",
   "metadata": {},
   "source": [
    "## Advanced: Dynamic System Prompts\n",
    "\n",
    "You can also generate system prompts dynamically based on context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d2e3f4-a5b6-7891-4567-012345678902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Adaptive agent created with dynamic system prompt\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import RunContext\n",
    "\n",
    "# Create agent without static system prompt\n",
    "adaptive_agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=ResearchDeps,\n",
    "    output_type=ResearchSummary\n",
    ")\n",
    "\n",
    "# Dynamic system prompt based on context\n",
    "@adaptive_agent.system_prompt(dynamic=True)\n",
    "def get_system_prompt(ctx: RunContext[ResearchDeps]) -> str:\n",
    "    \"\"\"Generate system prompt based on current context.\"\"\"\n",
    "    max_papers = ctx.deps.max_papers_per_search\n",
    "\n",
    "    return f\"\"\"You are a research assistant with access to {max_papers} papers per search.\n",
    "\n",
    "Focus on quality over quantity. When analyzing papers:\n",
    "- Prioritize recent publications (last 5 years)\n",
    "- Look for highly-cited works\n",
    "- Consider methodological rigor\n",
    "- Provide balanced summaries\n",
    "\n",
    "Your database: {ctx.deps.database_url}\n",
    "    \"\"\"\n",
    "\n",
    "print(\"‚úì Adaptive agent created with dynamic system prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8-e9f0-1235-8901-456789012346",
   "metadata": {},
   "source": [
    "## Why This Pattern Scales\n",
    "\n",
    "This agent architecture works for production systems because:\n",
    "\n",
    "**1. Type Safety**\n",
    "- Catch errors at development time\n",
    "- Invalid data rejected before processing\n",
    "- Clear validation errors guide debugging\n",
    "\n",
    "**2. Separation of Concerns**\n",
    "- Agent logic separate from business logic\n",
    "- Dependencies injected, not hard-coded\n",
    "- Easy to test each component independently\n",
    "\n",
    "**3. Composability**\n",
    "- Tools are just Python functions\n",
    "- Add new capabilities by adding new tools\n",
    "- Mix and match tools for different agents\n",
    "\n",
    "**4. Observability**\n",
    "- Every tool call is logged\n",
    "- Clear execution trace\n",
    "- Easy to monitor and debug\n",
    "\n",
    "**5. Maintainability**\n",
    "- Simple, readable code\n",
    "- Standard Python patterns\n",
    "- Type hints document the code\n",
    "\n",
    "This is how you build AI agents that last!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-b0c1-2347-9012-567890123458",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Type Safety Prevents Errors**\n",
    "- Validate data at construction time\n",
    "- Invalid states become unrepresentable  \n",
    "- Clear error messages guide debugging\n",
    "- Catch bugs before they reach production\n",
    "\n",
    "**PydanticAI Core Concepts**\n",
    "1. **Pydantic Models** - Define validated data structures\n",
    "2. **Agent** - Coordinates LLM and tools\n",
    "3. **@agent.tool** - Register functions as tools\n",
    "4. **RunContext[DepsType]** - Inject dependencies safely\n",
    "5. **result_type** - Guarantee output structure\n",
    "\n",
    "**Production-Ready Pattern**\n",
    "```python\n",
    "# 1. Define structures\n",
    "class Output(BaseModel): ...\n",
    "class Deps: ...\n",
    "\n",
    "# 2. Create agent\n",
    "agent = Agent(model, deps_type=Deps, result_type=Output)\n",
    "\n",
    "# 3. Register tools\n",
    "@agent.tool\n",
    "def my_tool(ctx: RunContext[Deps], arg: str): ...\n",
    "\n",
    "# 4. Run\n",
    "result = await agent.run(query, deps=deps)\n",
    "```\n",
    "\n",
    "**Why This Works**\n",
    "- Simple, composable primitives\n",
    "- Type safety throughout\n",
    "- Easy to test and maintain\n",
    "- Scales to complex systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1-d2e3-4569-1234-789012345670",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "**Pydantic Validation**:\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    name: str = Field(min_length=1)\n",
    "    count: int = Field(ge=0, le=100)\n",
    "```\n",
    "\n",
    "**Agent Creation**:\n",
    "```python\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=MyDeps,\n",
    "    result_type=MyOutput,\n",
    "    system_prompt=\"...\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Tool Registration**:\n",
    "```python\n",
    "@agent.tool\n",
    "def my_tool(\n",
    "    ctx: RunContext[MyDeps],\n",
    "    arg: str\n",
    ") -> MyType:\n",
    "    \"\"\"Docstring that LLM sees.\"\"\"\n",
    "    return ctx.deps.some_method(arg)\n",
    "```\n",
    "\n",
    "**Dynamic System Prompt**:\n",
    "```python\n",
    "@agent.system_prompt\n",
    "def get_prompt(ctx: RunContext[MyDeps]) -> str:\n",
    "    return f\"Context: {ctx.deps.value}\"\n",
    "```\n",
    "\n",
    "**Running Agent**:\n",
    "```python\n",
    "# Async\n",
    "result = await agent.run(query, deps=my_deps)\n",
    "output: MyOutput = result.data\n",
    "\n",
    "# Streaming\n",
    "async with agent.run_stream(query, deps=my_deps) as response:\n",
    "    async for chunk in response.stream_text():\n",
    "        print(chunk, end='')\n",
    "    final = await response.get_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2-e3f4-5670-2345-890123456781",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**In L.A2.03: Agent Frameworks Comparison**, we'll:\n",
    "- Compare different agent frameworks (PydanticAI, LangChain, CrewAI)\n",
    "- Understand architectural trade-offs\n",
    "- Learn when to use each approach\n",
    "- See the same task implemented multiple ways\n",
    "\n",
    "**In Week A03: Multi-Agent Systems**, we'll:\n",
    "- Build agent swarms with collective intelligence\n",
    "- Apply game theory to AI agent interactions\n",
    "- Create digital twins with agent-based models\n",
    "- Scale from single agents to agent ecosystems\n",
    "\n",
    "**Practice Ideas**:\n",
    "1. Add more tools to the research agent (filter by year, compare citations)\n",
    "2. Build an agent for a different domain (finance, healthcare, education)\n",
    "3. Implement streaming responses for better UX\n",
    "4. Add error handling and retry logic\n",
    "5. Create a simple web UI with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3-f4a5-6781-3456-901234567892",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "**PydanticAI Documentation**:\n",
    "- [Getting Started](https://ai.pydantic.dev/)\n",
    "- [Agents Guide](https://ai.pydantic.dev/agents/)\n",
    "- [Tools Documentation](https://ai.pydantic.dev/tools/)\n",
    "- [Dependencies](https://ai.pydantic.dev/dependencies/)\n",
    "- [Examples](https://ai.pydantic.dev/examples/)\n",
    "\n",
    "**Pydantic Core**:\n",
    "- [Pydantic V2](https://docs.pydantic.dev/)\n",
    "- [Field Validation](https://docs.pydantic.dev/latest/concepts/fields/)\n",
    "- [Models](https://docs.pydantic.dev/latest/concepts/models/)\n",
    "\n",
    "**Python Type System**:\n",
    "- [Type Hints](https://docs.python.org/3/library/typing.html)\n",
    "- [Protocols](https://peps.python.org/pep-0544/)\n",
    "- [Generics](https://docs.python.org/3/library/typing.html#generics)\n",
    "\n",
    "**Production Topics** (for later):\n",
    "- Testing with pytest\n",
    "- Monitoring with Logfire\n",
    "- Deployment patterns\n",
    "- Cost optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
