{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
   "metadata": {},
   "source": [
    "# Type-Safe Agent Development with PydanticAI Patterns\n",
    "\n",
    "> Computational Analysis of Social Complexity\n",
    ">\n",
    "> Fall 2025, Spencer Lyon\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Function calling concepts (L.A2.01)\n",
    "- Python type hints and type safety\n",
    "- Basic agent architectures\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand type-safe agent development principles\n",
    "- Implement validation-first agent architectures with Pydantic\n",
    "- Apply dependency injection patterns for testable agents\n",
    "- Build production-ready AI agents with PydanticAI\n",
    "\n",
    "**References**\n",
    "\n",
    "- [PydanticAI Documentation](https://ai.pydantic.dev/)\n",
    "- [Pydantic Documentation](https://docs.pydantic.dev/)\n",
    "- [Python Type Hints](https://docs.python.org/3/library/typing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-bcde-f23456789012",
   "metadata": {},
   "source": [
    "# Why Type Safety Matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-a7b8-9012-cdef-234567890123",
   "metadata": {},
   "source": [
    "## The Problem: Unvalidated Agent Systems\n",
    "\n",
    "- Consider a simple agent that processes user queries and makes database updates\n",
    "- What could go wrong?\n",
    "  - Wrong data types passed to functions\n",
    "  - Invalid values that violate business logic\n",
    "  - Runtime errors deep in execution\n",
    "  - Silent failures that corrupt data\n",
    "- These bugs are *expensive* in production\n",
    "- Type safety catches entire classes of errors before they happen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-b8c9-0123-def0-345678901234",
   "metadata": {},
   "source": [
    "## A Concrete Example\n",
    "\n",
    "Suppose we're building a research assistant agent that:\n",
    "1. Takes a research question\n",
    "2. Searches academic databases\n",
    "3. Returns structured citations\n",
    "\n",
    "Without type safety:\n",
    "```python\n",
    "# Python - no validation\n",
    "def search_papers(query, max_results, min_year):\n",
    "    # What if query is None?\n",
    "    # What if max_results is negative?\n",
    "    # What if min_year is 99999?\n",
    "    ...\n",
    "```\n",
    "\n",
    "With type safety:\n",
    "```python\n",
    "# Python with Pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str = Field(min_length=1, max_length=500)\n",
    "    max_results: int = Field(ge=1, le=100)\n",
    "    min_year: int = Field(ge=1900, le=2025)\n",
    "\n",
    "def search_papers(request: SearchRequest):\n",
    "    # Guaranteed valid inputs!\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-c9d0-1234-ef01-456789012345",
   "metadata": {},
   "source": [
    "## Key Benefits\n",
    "\n",
    "**Fail Fast, Fail Loud**\n",
    "- Invalid inputs rejected immediately\n",
    "- Clear error messages\n",
    "- No silent corruption\n",
    "\n",
    "**Self-Documenting Code**\n",
    "- Types tell you what's expected\n",
    "- IDE autocomplete and hints\n",
    "- Less need for comments\n",
    "\n",
    "**Refactoring Confidence**\n",
    "- Change a type definition\n",
    "- Compiler/type checker finds all affected code\n",
    "- Safe to modify large systems\n",
    "\n",
    "**Testing Made Easier**\n",
    "- Don't need to test invalid types\n",
    "- Focus on business logic\n",
    "- Reduce test surface area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-d0e1-2345-f012-567890123456",
   "metadata": {},
   "source": [
    "# The PydanticAI Philosophy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0-e1f2-3456-0123-678901234567",
   "metadata": {},
   "source": [
    "## What is PydanticAI?\n",
    "\n",
    "- PydanticAI is a Python framework for building production-ready AI agents\n",
    "- Created by the team behind Pydantic\n",
    "- Pydantic powers the OpenAI SDK, Anthropic SDK, FastAPI, and countless other production systems\n",
    "- Philosophy: **validation-first development**\n",
    "\n",
    "**Why PydanticAI?**\n",
    "- Type safety catches errors before runtime\n",
    "- Validation ensures data integrity\n",
    "- Production-ready patterns from day one\n",
    "- Seamless integration with Python's type system\n",
    "- Used by major companies in production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4567-1234-789012345678",
   "metadata": {},
   "source": [
    "## Chef's Knife and Cutting Board\n",
    "\n",
    "- PydanticAI describes itself as providing a \"chef's knife and cutting board\"\n",
    "- What does this mean?\n",
    "  - **NOT a framework** with opinions about everything\n",
    "  - **IS a toolkit** with powerful, composable primitives\n",
    "  - Simple tools that do one thing well\n",
    "  - Combine them however you want\n",
    "\n",
    "**Contrast with LangChain**:\n",
    "- LangChain: Full kitchen with every appliance\n",
    "  - Chains, memory, vector stores, callbacks, etc.\n",
    "  - Complex abstractions\n",
    "  - Steep learning curve\n",
    "  - Hard to customize\n",
    "- PydanticAI: Essential tools\n",
    "  - Agent, tools, structured outputs\n",
    "  - Simple abstractions\n",
    "  - Easy to understand\n",
    "  - Flexible customization\n",
    "\n",
    "**When to Use Which**:\n",
    "- Use PydanticAI when: building production systems, need type safety, want simplicity\n",
    "- Use LangChain when: rapid prototyping, need batteries-included features, okay with complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-a3b4-5678-2345-890123456789",
   "metadata": {},
   "source": [
    "## Core Principles\n",
    "\n",
    "**1. Type Safety First**\n",
    "- All inputs validated before use\n",
    "- All outputs structured and validated\n",
    "- Catch errors at design time, not runtime\n",
    "\n",
    "**2. Validation Before Computation**\n",
    "- Never process invalid data\n",
    "- Fail immediately with clear messages\n",
    "- Make invalid states unrepresentable\n",
    "\n",
    "**3. Dependency Injection**\n",
    "- Agents don't create their dependencies\n",
    "- Dependencies passed in at construction\n",
    "- Easy to test with mocks\n",
    "- Easy to swap implementations\n",
    "\n",
    "**4. Explicit Over Implicit**\n",
    "- No magic global state\n",
    "- No hidden configuration\n",
    "- Everything visible in function signatures\n",
    "\n",
    "**5. Production Ready**\n",
    "- Designed for real systems, not demos\n",
    "- Handles errors gracefully\n",
    "- Observability built-in\n",
    "- Performance matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3-b4c5-6789-3456-901234567890",
   "metadata": {},
   "source": [
    "# Core Agent Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-7890-4567-012345678901",
   "metadata": {},
   "source": [
    "## The Four Essential Pieces\n",
    "\n",
    "Every PydanticAI agent has these components:\n",
    "\n",
    "**1. Agent Definition**\n",
    "- Specifies the LLM model to use\n",
    "- Defines system prompt\n",
    "- Registers available tools\n",
    "\n",
    "**2. Tool Functions**\n",
    "- Functions the agent can call\n",
    "- Type-annotated parameters\n",
    "- Validated inputs and outputs\n",
    "\n",
    "**3. Structured Outputs**\n",
    "- Define what the agent should return\n",
    "- Pydantic models for validation\n",
    "- Guaranteed schema compliance\n",
    "\n",
    "**4. Run Context (Dependencies)**\n",
    "- External services (databases, APIs)\n",
    "- Configuration\n",
    "- State that flows through execution\n",
    "\n",
    "Let's examine each in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5-d6e7-8901-5678-123456789012",
   "metadata": {},
   "source": [
    "## 1. Agent Definition\n",
    "\n",
    "In PydanticAI (Python):\n",
    "```python\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    system_prompt=\"You are a helpful research assistant.\",\n",
    ")\n",
    "```\n",
    "\n",
    "Key aspects:\n",
    "- **Model string**: Specifies which LLM to use\n",
    "- **System prompt**: Sets agent behavior and role\n",
    "- **Simple constructor**: No complex configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-9012-6789-234567890123",
   "metadata": {},
   "source": [
    "## 2. Tool Functions\n",
    "\n",
    "Tools are functions the agent can call:\n",
    "\n",
    "```python\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent('anthropic:claude-haiku-4-5')\n",
    "\n",
    "@agent.tool\n",
    "def search_papers(\n",
    "    query: str,\n",
    "    max_results: int = 10\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search academic papers by query.\"\"\"\n",
    "    # Implementation here\n",
    "    return results\n",
    "```\n",
    "\n",
    "**What happens**:\n",
    "1. LLM sees tool name and docstring\n",
    "2. LLM decides when to call it\n",
    "3. Parameters are validated against type hints\n",
    "4. Function executes\n",
    "5. Return value validated\n",
    "6. Result given back to LLM\n",
    "\n",
    "**Type safety at every step!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7-f8a9-0123-7890-345678901234",
   "metadata": {},
   "source": [
    "## 3. Structured Outputs\n",
    "\n",
    "Define exactly what you want back:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    title: str\n",
    "    authors: list[str]\n",
    "    year: int\n",
    "    doi: str | None = None\n",
    "\n",
    "class ResearchResult(BaseModel):\n",
    "    summary: str\n",
    "    citations: list[Citation]\n",
    "    confidence: float\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    result_type=ResearchResult\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- LLM must return this exact structure\n",
    "- Automatic validation\n",
    "- No parsing strings or JSON\n",
    "- IDE autocomplete on results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-a9b0-1234-8901-456789012345",
   "metadata": {},
   "source": [
    "## 4. Run Context (Dependencies)\n",
    "\n",
    "The secret sauce for testability:\n",
    "\n",
    "```python\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Dependencies:\n",
    "    db: DatabaseClient\n",
    "    api_key: str\n",
    "    max_retries: int = 3\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=Dependencies\n",
    ")\n",
    "\n",
    "@agent.tool\n",
    "def search_papers(\n",
    "    ctx: RunContext[Dependencies],\n",
    "    query: str\n",
    ") -> list[dict]:\n",
    "    # Access dependencies via ctx.deps\n",
    "    return ctx.deps.db.search(query)\n",
    "```\n",
    "\n",
    "**Why this matters**:\n",
    "- No global variables\n",
    "- Easy to inject mock dependencies for testing\n",
    "- Explicit about what each tool needs\n",
    "- Can have different deps for dev/prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-b0c1-2345-9012-567890123456",
   "metadata": {},
   "source": [
    "## Putting It All Together\n",
    "\n",
    "A complete PydanticAI agent:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from pydantic_ai import Agent, RunContext\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 1. Define dependencies\n",
    "@dataclass\n",
    "class ResearchDeps:\n",
    "    db: DatabaseClient\n",
    "    api_key: str\n",
    "\n",
    "# 2. Define output structure\n",
    "class Citation(BaseModel):\n",
    "    title: str\n",
    "    authors: list[str]\n",
    "    year: int\n",
    "\n",
    "class ResearchResult(BaseModel):\n",
    "    summary: str\n",
    "    citations: list[Citation]\n",
    "\n",
    "# 3. Create agent\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=ResearchDeps,\n",
    "    result_type=ResearchResult,\n",
    "    system_prompt='You are a research assistant.'\n",
    ")\n",
    "\n",
    "# 4. Register tools\n",
    "@agent.tool\n",
    "def search_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    query: str,\n",
    "    max_results: int = 10\n",
    ") -> list[dict]:\n",
    "    \"\"\"Search academic database.\"\"\"\n",
    "    return ctx.deps.db.search(query, limit=max_results)\n",
    "\n",
    "# 5. Run agent\n",
    "async def run_research(question: str):\n",
    "    deps = ResearchDeps(\n",
    "        db=get_database(),\n",
    "        api_key=get_api_key()\n",
    "    )\n",
    "    result = await agent.run(question, deps=deps)\n",
    "    return result.data  # Guaranteed to be ResearchResult!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0-e1f2-3457-0123-678901234568",
   "metadata": {},
   "source": [
    "# Building a Research Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4568-1234-789012345679",
   "metadata": {},
   "source": [
    "## System Design\n",
    "\n",
    "Let's build a complete research assistant that demonstrates all the PydanticAI concepts we've learned:\n",
    "\n",
    "**Features**:\n",
    "- Search academic papers by query\n",
    "- Get detailed paper information\n",
    "- Generate structured research summaries\n",
    "- Type-safe dependencies and outputs\n",
    "- Multiple tools working together\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "PydanticAI Agent (with system prompt)\n",
    "    ‚Üì\n",
    "Tool Selection (agent decides which tools to use)\n",
    "    ‚Üì\n",
    "Tool Execution (with validated inputs via RunContext)\n",
    "    ‚Üì\n",
    "Result Synthesis (agent combines tool outputs)\n",
    "    ‚Üì\n",
    "Structured Output (validated ResearchSummary)\n",
    "```\n",
    "\n",
    "This is a realistic pattern you'd use in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-a3b4-5679-2345-890123456780",
   "metadata": {},
   "source": [
    "## Step 1: Define Output Structure\n",
    "\n",
    "First, define what we want the agent to return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e1f2a3-b4c5-6780-3456-901234567891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Validation caught error: ValidationError\n",
      "‚úì Valid paper created: Attention Is All You Need\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Academic paper with validated fields.\"\"\"\n",
    "    title: str = Field(min_length=1)\n",
    "    authors: list[str] = Field(min_length=1)\n",
    "    year: int = Field(ge=1800, le=2025)\n",
    "    abstract: str\n",
    "    doi: Optional[str] = None\n",
    "\n",
    "class ResearchSummary(BaseModel):\n",
    "    \"\"\"Final research output with validation.\"\"\"\n",
    "    query: str = Field(min_length=1)\n",
    "    summary: str = Field(min_length=1, description=\"Natural language summary\")\n",
    "    key_papers: list[Paper] = Field(min_length=1, description=\"Most relevant papers found\")\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Agent's confidence in results\")\n",
    "\n",
    "    def _repr_html_(self) -> str:\n",
    "        \"\"\"Rich HTML display for Jupyter notebooks.\"\"\"\n",
    "        papers_html = \"\"\n",
    "        for i, paper in enumerate(self.key_papers, 1):\n",
    "            authors = \", \".join(paper.authors)\n",
    "            doi_link = f'<a href=\"https://doi.org/{paper.doi}\" target=\"_blank\">DOI</a>' if paper.doi else \"\"\n",
    "            papers_html += f\"\"\"\n",
    "            <div style=\"margin: 10px 0; padding: 10px; background: #f8f9fa; border-left: 3px solid #007bff;\">\n",
    "                <strong>{i}. {paper.title}</strong> ({paper.year}) {doi_link}<br>\n",
    "                <em>{authors}</em><br>\n",
    "                <small style=\"color: #666;\">{paper.abstract[:200]}...</small>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "\n",
    "        confidence_color = \"#28a745\" if self.confidence >= 0.7 else \"#ffc107\" if self.confidence >= 0.4 else \"#dc3545\"\n",
    "        confidence_pct = f\"{self.confidence:.0%}\"\n",
    "\n",
    "        return f\"\"\"\n",
    "        <div style=\"font-family: sans-serif; max-width: 800px;\">\n",
    "            <h3 style=\"color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px;\">\n",
    "                üìö Research Results: \"{self.query}\"\n",
    "            </h3>\n",
    "            <div style=\"margin: 15px 0; padding: 15px; background: #e7f3ff; border-radius: 5px;\">\n",
    "                <strong>Summary:</strong><br>\n",
    "                {self.summary}\n",
    "            </div>\n",
    "            <div style=\"margin: 10px 0;\">\n",
    "                <strong>Confidence:</strong>\n",
    "                <span style=\"color: {confidence_color}; font-weight: bold;\">{confidence_pct}</span>\n",
    "            </div>\n",
    "            <div style=\"margin-top: 20px;\">\n",
    "                <strong>Key Papers ({len(self.key_papers)}):</strong>\n",
    "                {papers_html}\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Plain text representation.\"\"\"\n",
    "        lines = [\n",
    "            f\"Research Results: {self.query}\",\n",
    "            \"=\" * 60,\n",
    "            f\"\\nSummary:\\n{self.summary}\",\n",
    "            f\"\\nConfidence: {self.confidence:.0%}\",\n",
    "            f\"\\nKey Papers ({len(self.key_papers)}):\",\n",
    "        ]\n",
    "\n",
    "        for i, paper in enumerate(self.key_papers, 1):\n",
    "            lines.append(f\"\\n{i}. {paper.title} ({paper.year})\")\n",
    "            lines.append(f\"   Authors: {', '.join(paper.authors)}\")\n",
    "            if paper.doi:\n",
    "                lines.append(f\"   DOI: {paper.doi}\")\n",
    "\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# Test that validation works\n",
    "try:\n",
    "    invalid = Paper(title=\"\", authors=[], year=3000, abstract=\"test\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úì Validation caught error: {type(e).__name__}\")\n",
    "\n",
    "valid = Paper(\n",
    "    title=\"Attention Is All You Need\",\n",
    "    authors=[\"Vaswani et al.\"],\n",
    "    year=2017,\n",
    "    abstract=\"We propose the Transformer...\"\n",
    ")\n",
    "print(f\"‚úì Valid paper created: {valid.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-7891-4567-012345678902",
   "metadata": {},
   "source": [
    "## Step 2: Define Dependencies\n",
    "\n",
    "Define external resources the agent needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a3b4c5-d6e7-8902-5678-123456789013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies configured\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ResearchDeps:\n",
    "    \"\"\"Dependencies for the research agent.\"\"\"\n",
    "    database_url: str\n",
    "    api_key: str\n",
    "    max_papers_per_search: int = 10\n",
    "\n",
    "    def search_database(self, query: str, limit: int) -> list[Paper]:\n",
    "        \"\"\"Simulate database search (in production, would query real DB).\"\"\"\n",
    "        print(f\"  üîç Searching database for: '{query}' (limit: {limit})\")\n",
    "\n",
    "        # In production, this would be:\n",
    "        # conn = connect(self.database_url, self.api_key)\n",
    "        # results = conn.execute(search_query)\n",
    "        # return [Paper(**row) for row in results]\n",
    "\n",
    "        # For demo, return mock results\n",
    "        return [\n",
    "            Paper(\n",
    "                title=\"Attention Is All You Need\",\n",
    "                authors=[\"Vaswani, A.\", \"Shazeer, N.\", \"Parmar, N.\"],\n",
    "                year=2017,\n",
    "                abstract=\"We propose a new simple network architecture, the Transformer...\",\n",
    "                doi=\"10.5555/3295222.3295349\"\n",
    "            ),\n",
    "            Paper(\n",
    "                title=\"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "                authors=[\"Devlin, J.\", \"Chang, M.\", \"Lee, K.\"],\n",
    "                year=2019,\n",
    "                abstract=\"We introduce BERT, a method for pre-training language representations...\",\n",
    "                doi=\"10.18653/v1/N19-1423\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def get_paper_by_id(self, paper_id: str) -> Paper:\n",
    "        \"\"\"Retrieve specific paper by ID.\"\"\"\n",
    "        print(f\"  üìÑ Fetching paper: {paper_id}\")\n",
    "        return Paper(\n",
    "            title=\"Example Paper\",\n",
    "            authors=[\"Author, A.\"],\n",
    "            year=2023,\n",
    "            abstract=\"This is an example paper abstract.\"\n",
    "        )\n",
    "\n",
    "# Create dependencies\n",
    "deps = ResearchDeps(\n",
    "    database_url=\"sqlite3:///papers.db\",\n",
    "    api_key=\"demo-api-key-12345\",\n",
    "    max_papers_per_search=5\n",
    ")\n",
    "\n",
    "print(\"‚úì Dependencies configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-9013-6789-234567890124",
   "metadata": {},
   "source": [
    "## Step 3: Create Agent and Register Tools\n",
    "\n",
    "Now create the agent and give it tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c5d6e7-f8a9-0124-7890-345678901235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Agent created with 2 tools registered\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "# Create agent with system prompt and structured output\n",
    "research_agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=ResearchDeps,\n",
    "    output_type=ResearchSummary,\n",
    "    system_prompt=\"\"\"You are a helpful research assistant.\n",
    "\n",
    "    When asked to research a topic:\n",
    "    1. Search for relevant papers using the search_papers tool\n",
    "    2. Analyze the results carefully\n",
    "    3. Provide a clear summary with the most important papers\n",
    "    4. Rate your confidence based on result quality and relevance\n",
    "\n",
    "    Be thorough but concise. Focus on the most impactful papers.\"\"\"\n",
    ")\n",
    "\n",
    "# Register tool 1: Search for papers\n",
    "@research_agent.tool\n",
    "def search_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    query: str,\n",
    "    max_results: int = 5\n",
    ") -> list[Paper]:\n",
    "    \"\"\"\n",
    "    Search for academic papers by query string.\n",
    "\n",
    "    Args:\n",
    "        query: Search query (keywords, topics, authors)\n",
    "        max_results: Maximum number of papers to return (default 5)\n",
    "\n",
    "    Returns:\n",
    "        List of relevant papers\n",
    "    \"\"\"\n",
    "    # Access dependencies via ctx.deps\n",
    "    limit = min(max_results, ctx.deps.max_papers_per_search)\n",
    "    papers = ctx.deps.search_database(query, limit)\n",
    "    return papers\n",
    "\n",
    "# Register tool 2: Get specific paper details\n",
    "@research_agent.tool\n",
    "def get_paper_details(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    paper_id: str\n",
    ") -> Paper:\n",
    "    \"\"\"\n",
    "    Get detailed information about a specific paper by its ID.\n",
    "\n",
    "    Args:\n",
    "        paper_id: Unique identifier for the paper\n",
    "\n",
    "    Returns:\n",
    "        Paper object with full details\n",
    "    \"\"\"\n",
    "    return ctx.deps.get_paper_by_id(paper_id)\n",
    "\n",
    "print(\"‚úì Agent created with 2 tools registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-e9f0-1235-8901-456789012346",
   "metadata": {},
   "source": [
    "## Step 4: Run the Agent\n",
    "\n",
    "Execute the agent with a research query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e7f8a9-b0c1-2346-9012-567890123457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  üîç Searching database for: 'transformer architectures NLP natural language processing' (limit: 5)\n",
      "  üìÑ Fetching paper: 10.5555/3295222.3295349\n",
      "  üîç Searching database for: 'GPT language model transformer decoder architecture' (limit: 5)\n",
      "  üìÑ Fetching paper: 10.18653/v1/N19-1423\n",
      "  üîç Searching database for: 'vision transformer ViT multimodal transformers' (limit: 5)\n"
     ]
    }
   ],
   "source": [
    "async def run_research(question: str):\n",
    "    deps = ResearchDeps(\n",
    "        database_url=\"sqlite3:///papers.db\",\n",
    "        api_key=\"demo-api-key-12345\",\n",
    "        max_papers_per_search=5\n",
    "    )\n",
    "    result = await research_agent.run(question, deps=deps)\n",
    "    return result\n",
    "\n",
    "research_result = await run_research(\"transformer architectures in NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content='You are a helpful research assistant.\\n\\n    When asked to research a topic:\\n    1. Search for relevant papers using the search_papers tool\\n    2. Analyze the results carefully\\n    3. Provide a clear summary with the most important papers\\n    4. Rate your confidence based on result quality and relevance\\n\\n    Be thorough but concise. Focus on the most impactful papers.', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 38, 998062, tzinfo=datetime.timezone.utc)), UserPromptPart(content='transformer architectures in NLP', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 38, 998067, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='search_papers', args={'query': 'transformer architectures NLP natural language processing', 'max_results': 10}, tool_call_id='toolu_01HMYGCHHKepQbGLLnE77RTA')], usage=RequestUsage(input_tokens=1265, output_tokens=64, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1265, 'output_tokens': 64}), model_name='claude-haiku-4-5-20251001', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 40, 151296, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_01TXmSMSuieQVGRVVvCvrk7p', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_01HMYGCHHKepQbGLLnE77RTA', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 40, 152853, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='get_paper_details', args={'paper_id': '10.5555/3295222.3295349'}, tool_call_id='toolu_01SAyghgAgQQioeXSeDLBg6S'), ToolCallPart(tool_name='get_paper_details', args={'paper_id': '10.18653/v1/N19-1423'}, tool_call_id='toolu_01CzRgpL8C2jHXem8UA7EXVW'), ToolCallPart(tool_name='search_papers', args={'query': 'GPT language model transformer decoder architecture', 'max_results': 5}, tool_call_id='toolu_01EHPzYsfa1VcN1numeKnka8'), ToolCallPart(tool_name='search_papers', args={'query': 'vision transformer ViT multimodal transformers', 'max_results': 5}, tool_call_id='toolu_016HLuY8zBwhBXVbogsVga69')], usage=RequestUsage(input_tokens=1507, output_tokens=230, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 1507, 'output_tokens': 230}), model_name='claude-haiku-4-5-20251001', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 41, 808031, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_01KGv7g5ApDoCDnSenRMjfwW', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='get_paper_details', content=Paper(title='Example Paper', authors=['Author, A.'], year=2023, abstract='This is an example paper abstract.', doi=None), tool_call_id='toolu_01SAyghgAgQQioeXSeDLBg6S', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 41, 810833, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='get_paper_details', content=Paper(title='Example Paper', authors=['Author, A.'], year=2023, abstract='This is an example paper abstract.', doi=None), tool_call_id='toolu_01CzRgpL8C2jHXem8UA7EXVW', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 41, 810881, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_01EHPzYsfa1VcN1numeKnka8', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 41, 810865, tzinfo=datetime.timezone.utc)), ToolReturnPart(tool_name='search_papers', content=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer...', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations...', doi='10.18653/v1/N19-1423')], tool_call_id='toolu_016HLuY8zBwhBXVbogsVga69', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 41, 810899, tzinfo=datetime.timezone.utc))]),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='final_result', args={'query': 'transformer architectures in NLP', 'summary': \"Transformer architectures have become the foundation of modern natural language processing. The key innovation was the introduction of the self-attention mechanism, which allows models to process entire sequences in parallel and capture long-range dependencies effectively. The original Transformer architecture (Vaswani et al., 2017) proposed the encoder-decoder structure with multi-head self-attention and feed-forward networks. This was followed by influential pre-trained models like BERT (Devlin et al., 2019), which introduced bidirectional pre-training for language understanding. Transformer-based models have revolutionized NLP across diverse tasks including machine translation, question answering, sentiment analysis, and text generation. The architecture's parallelization capabilities and effectiveness in capturing semantic relationships have made it the dominant paradigm in both research and industry applications.\", 'key_papers': [{'title': 'Attention Is All You Need', 'authors': ['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], 'year': 2017, 'abstract': 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The Transformer achieves new state-of-the-art BLEU scores on WMT 2014 English-to-German and English-to-French translation tasks. This work introduces multi-head self-attention, enabling parallel processing and effective capture of long-range dependencies in sequences.', 'doi': '10.5555/3295222.3295349'}, {'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'authors': ['Devlin, J.', 'Chang, M.', 'Lee, K.'], 'year': 2019, 'abstract': 'We introduce BERT, a method for pre-training language representations using masked language modeling and next sentence prediction on large unlabeled text corpora. BERT achieves state-of-the-art results on a wide range of NLP tasks through fine-tuning, demonstrating the power of bidirectional pre-trained representations.', 'doi': '10.18653/v1/N19-1423'}], 'confidence': 0.75}, tool_call_id='toolu_01WbAZFPb3WusK49Gkv7A2U6')], usage=RequestUsage(input_tokens=2237, output_tokens=600, details={'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 2237, 'output_tokens': 600}), model_name='claude-haiku-4-5-20251001', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 46, 698547, tzinfo=datetime.timezone.utc), provider_name='anthropic', provider_details={'finish_reason': 'tool_use'}, provider_response_id='msg_01Ub7dhxfuQEKChRtd5zP8Vg', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='final_result', content='Final result processed.', tool_call_id='toolu_01WbAZFPb3WusK49Gkv7A2U6', timestamp=datetime.datetime(2025, 11, 10, 18, 7, 46, 699638, tzinfo=datetime.timezone.utc))])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_result.all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"font-family: sans-serif; max-width: 800px;\">\n",
       "            <h3 style=\"color: #333; border-bottom: 2px solid #007bff; padding-bottom: 10px;\">\n",
       "                üìö Research Results: \"transformer architectures in NLP\"\n",
       "            </h3>\n",
       "            <div style=\"margin: 15px 0; padding: 15px; background: #e7f3ff; border-radius: 5px;\">\n",
       "                <strong>Summary:</strong><br>\n",
       "                Transformer architectures have become the foundation of modern natural language processing. The key innovation was the introduction of the self-attention mechanism, which allows models to process entire sequences in parallel and capture long-range dependencies effectively. The original Transformer architecture (Vaswani et al., 2017) proposed the encoder-decoder structure with multi-head self-attention and feed-forward networks. This was followed by influential pre-trained models like BERT (Devlin et al., 2019), which introduced bidirectional pre-training for language understanding. Transformer-based models have revolutionized NLP across diverse tasks including machine translation, question answering, sentiment analysis, and text generation. The architecture's parallelization capabilities and effectiveness in capturing semantic relationships have made it the dominant paradigm in both research and industry applications.\n",
       "            </div>\n",
       "            <div style=\"margin: 10px 0;\">\n",
       "                <strong>Confidence:</strong>\n",
       "                <span style=\"color: #28a745; font-weight: bold;\">75%</span>\n",
       "            </div>\n",
       "            <div style=\"margin-top: 20px;\">\n",
       "                <strong>Key Papers (2):</strong>\n",
       "                \n",
       "            <div style=\"margin: 10px 0; padding: 10px; background: #f8f9fa; border-left: 3px solid #007bff;\">\n",
       "                <strong>1. Attention Is All You Need</strong> (2017) <a href=\"https://doi.org/10.5555/3295222.3295349\" target=\"_blank\">DOI</a><br>\n",
       "                <em>Vaswani, A., Shazeer, N., Parmar, N.</em><br>\n",
       "                <small style=\"color: #666;\">We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The Transformer achieves new state-of-the-art...</small>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"margin: 10px 0; padding: 10px; background: #f8f9fa; border-left: 3px solid #007bff;\">\n",
       "                <strong>2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong> (2019) <a href=\"https://doi.org/10.18653/v1/N19-1423\" target=\"_blank\">DOI</a><br>\n",
       "                <em>Devlin, J., Chang, M., Lee, K.</em><br>\n",
       "                <small style=\"color: #666;\">We introduce BERT, a method for pre-training language representations using masked language modeling and next sentence prediction on large unlabeled text corpora. BERT achieves state-of-the-art result...</small>\n",
       "            </div>\n",
       "            \n",
       "            </div>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "ResearchSummary(query='transformer architectures in NLP', summary=\"Transformer architectures have become the foundation of modern natural language processing. The key innovation was the introduction of the self-attention mechanism, which allows models to process entire sequences in parallel and capture long-range dependencies effectively. The original Transformer architecture (Vaswani et al., 2017) proposed the encoder-decoder structure with multi-head self-attention and feed-forward networks. This was followed by influential pre-trained models like BERT (Devlin et al., 2019), which introduced bidirectional pre-training for language understanding. Transformer-based models have revolutionized NLP across diverse tasks including machine translation, question answering, sentiment analysis, and text generation. The architecture's parallelization capabilities and effectiveness in capturing semantic relationships have made it the dominant paradigm in both research and industry applications.\", key_papers=[Paper(title='Attention Is All You Need', authors=['Vaswani, A.', 'Shazeer, N.', 'Parmar, N.'], year=2017, abstract='We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The Transformer achieves new state-of-the-art BLEU scores on WMT 2014 English-to-German and English-to-French translation tasks. This work introduces multi-head self-attention, enabling parallel processing and effective capture of long-range dependencies in sequences.', doi='10.5555/3295222.3295349'), Paper(title='BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', authors=['Devlin, J.', 'Chang, M.', 'Lee, K.'], year=2019, abstract='We introduce BERT, a method for pre-training language representations using masked language modeling and next sentence prediction on large unlabeled text corpora. BERT achieves state-of-the-art results on a wide range of NLP tasks through fine-tuning, demonstrating the power of bidirectional pre-trained representations.', doi='10.18653/v1/N19-1423')], confidence=0.75)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_result.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8a9b0-c1d2-3457-0123-678901234568",
   "metadata": {},
   "source": [
    "## What Just Happened?\n",
    "\n",
    "Let's break down the agent execution:\n",
    "\n",
    "**1. Agent Received Query**\n",
    "- User asked: \"transformer architectures in NLP\"\n",
    "- Agent has access to: search_papers and get_paper_details tools\n",
    "\n",
    "**2. Agent Made Decisions**\n",
    "- LLM analyzed the query\n",
    "- Decided to call `search_papers(\"transformer architectures in NLP\", max_results=5)`\n",
    "- PydanticAI validated the parameters\n",
    "- Tool executed with access to deps via RunContext\n",
    "\n",
    "**3. Tool Returned Validated Data**\n",
    "- Tool returned `list[Paper]` (type-checked)\n",
    "- Each Paper object validated by Pydantic\n",
    "- Agent received the results\n",
    "\n",
    "**4. Agent Synthesized Response**\n",
    "- LLM analyzed the papers\n",
    "- Generated natural language summary\n",
    "- Selected key papers\n",
    "- Assigned confidence score\n",
    "- Returned ResearchSummary (validated!)\n",
    "\n",
    "**5. Type Safety Throughout**\n",
    "- Every input validated before execution\n",
    "- Every output validated before return\n",
    "- Impossible to get wrong types\n",
    "- Clear errors if validation fails\n",
    "\n",
    "This is the power of type-safe agent development!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1-d2e3-4568-1234-789012345679",
   "metadata": {},
   "source": [
    "## Exercise: Add a New Tool\n",
    "\n",
    "Add a `compare_papers` tool that takes two paper titles and returns a comparison.\n",
    "\n",
    "**Requirements**:\n",
    "1. Use `@research_agent.tool` decorator\n",
    "2. Accept `RunContext[ResearchDeps]` as first parameter\n",
    "3. Take two paper titles as strings\n",
    "4. Return a string with the comparison\n",
    "5. Use proper docstring (the agent sees this!)\n",
    "\n",
    "**Hint**: The agent will automatically see this new tool and can call it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b0c1d2-e3f4-5679-2345-890123456780",
   "metadata": {},
   "outputs": [],
   "source": [
    "@research_agent.tool\n",
    "def compare_papers(\n",
    "    ctx: RunContext[ResearchDeps],\n",
    "    paper1_title: str,\n",
    "    paper2_title: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compare two papers by their titles.\n",
    "\n",
    "    Args:\n",
    "        paper1_title: Title of first paper\n",
    "        paper2_title: Title of second paper\n",
    "\n",
    "    Returns:\n",
    "        Comparison summary\n",
    "    \"\"\"\n",
    "    # TODO: Implement comparison logic\n",
    "    # In production, would search for both papers and compare:\n",
    "    # - Publication dates\n",
    "    # - Citation counts\n",
    "    # - Research methods\n",
    "    # - Key contributions\n",
    "\n",
    "    return f\"Comparison between '{paper1_title}' and '{paper2_title}': [TODO: implement]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3-f4a5-6780-3456-901234567891",
   "metadata": {},
   "source": [
    "## Advanced: Dynamic System Prompts\n",
    "\n",
    "You can also generate system prompts dynamically based on context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d2e3f4-a5b6-7891-4567-012345678902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Adaptive agent created with dynamic system prompt\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import RunContext\n",
    "\n",
    "# Create agent without static system prompt\n",
    "adaptive_agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=ResearchDeps,\n",
    "    output_type=ResearchSummary\n",
    ")\n",
    "\n",
    "# Dynamic system prompt based on context\n",
    "@adaptive_agent.system_prompt(dynamic=True)\n",
    "def get_system_prompt(ctx: RunContext[ResearchDeps]) -> str:\n",
    "    \"\"\"Generate system prompt based on current context.\"\"\"\n",
    "    max_papers = ctx.deps.max_papers_per_search\n",
    "\n",
    "    return f\"\"\"You are a research assistant with access to {max_papers} papers per search.\n",
    "\n",
    "Focus on quality over quantity. When analyzing papers:\n",
    "- Prioritize recent publications (last 5 years)\n",
    "- Look for highly-cited works\n",
    "- Consider methodological rigor\n",
    "- Provide balanced summaries\n",
    "\n",
    "Your database: {ctx.deps.database_url}\n",
    "    \"\"\"\n",
    "\n",
    "print(\"‚úì Adaptive agent created with dynamic system prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8-e9f0-1235-8901-456789012346",
   "metadata": {},
   "source": [
    "## Why This Pattern Scales\n",
    "\n",
    "This agent architecture works for production systems because:\n",
    "\n",
    "**1. Type Safety**\n",
    "- Catch errors at development time\n",
    "- Invalid data rejected before processing\n",
    "- Clear validation errors guide debugging\n",
    "\n",
    "**2. Separation of Concerns**\n",
    "- Agent logic separate from business logic\n",
    "- Dependencies injected, not hard-coded\n",
    "- Easy to test each component independently\n",
    "\n",
    "**3. Composability**\n",
    "- Tools are just Python functions\n",
    "- Add new capabilities by adding new tools\n",
    "- Mix and match tools for different agents\n",
    "\n",
    "**4. Observability**\n",
    "- Every tool call is logged\n",
    "- Clear execution trace\n",
    "- Easy to monitor and debug\n",
    "\n",
    "**5. Maintainability**\n",
    "- Simple, readable code\n",
    "- Standard Python patterns\n",
    "- Type hints document the code\n",
    "\n",
    "This is how you build AI agents that last!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9-b0c1-2347-9012-567890123458",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Type Safety Prevents Errors**\n",
    "- Validate data at construction time\n",
    "- Invalid states become unrepresentable  \n",
    "- Clear error messages guide debugging\n",
    "- Catch bugs before they reach production\n",
    "\n",
    "**PydanticAI Core Concepts**\n",
    "1. **Pydantic Models** - Define validated data structures\n",
    "2. **Agent** - Coordinates LLM and tools\n",
    "3. **@agent.tool** - Register functions as tools\n",
    "4. **RunContext[DepsType]** - Inject dependencies safely\n",
    "5. **result_type** - Guarantee output structure\n",
    "\n",
    "**Production-Ready Pattern**\n",
    "```python\n",
    "# 1. Define structures\n",
    "class Output(BaseModel): ...\n",
    "class Deps: ...\n",
    "\n",
    "# 2. Create agent\n",
    "agent = Agent(model, deps_type=Deps, result_type=Output)\n",
    "\n",
    "# 3. Register tools\n",
    "@agent.tool\n",
    "def my_tool(ctx: RunContext[Deps], arg: str): ...\n",
    "\n",
    "# 4. Run\n",
    "result = await agent.run(query, deps=deps)\n",
    "```\n",
    "\n",
    "**Why This Works**\n",
    "- Simple, composable primitives\n",
    "- Type safety throughout\n",
    "- Easy to test and maintain\n",
    "- Scales to complex systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1-d2e3-4569-1234-789012345670",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "**Pydantic Validation**:\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    name: str = Field(min_length=1)\n",
    "    count: int = Field(ge=0, le=100)\n",
    "```\n",
    "\n",
    "**Agent Creation**:\n",
    "```python\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    'anthropic:claude-haiku-4-5',\n",
    "    deps_type=MyDeps,\n",
    "    result_type=MyOutput,\n",
    "    system_prompt=\"...\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Tool Registration**:\n",
    "```python\n",
    "@agent.tool\n",
    "def my_tool(\n",
    "    ctx: RunContext[MyDeps],\n",
    "    arg: str\n",
    ") -> MyType:\n",
    "    \"\"\"Docstring that LLM sees.\"\"\"\n",
    "    return ctx.deps.some_method(arg)\n",
    "```\n",
    "\n",
    "**Dynamic System Prompt**:\n",
    "```python\n",
    "@agent.system_prompt\n",
    "def get_prompt(ctx: RunContext[MyDeps]) -> str:\n",
    "    return f\"Context: {ctx.deps.value}\"\n",
    "```\n",
    "\n",
    "**Running Agent**:\n",
    "```python\n",
    "# Async\n",
    "result = await agent.run(query, deps=my_deps)\n",
    "output: MyOutput = result.data\n",
    "\n",
    "# Streaming\n",
    "async with agent.run_stream(query, deps=my_deps) as response:\n",
    "    async for chunk in response.stream_text():\n",
    "        print(chunk, end='')\n",
    "    final = await response.get_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2-e3f4-5670-2345-890123456781",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**In L.A2.03: Agent Frameworks Comparison**, we'll:\n",
    "- Compare different agent frameworks (PydanticAI, LangChain, CrewAI)\n",
    "- Understand architectural trade-offs\n",
    "- Learn when to use each approach\n",
    "- See the same task implemented multiple ways\n",
    "\n",
    "**In Week A03: Multi-Agent Systems**, we'll:\n",
    "- Build agent swarms with collective intelligence\n",
    "- Apply game theory to AI agent interactions\n",
    "- Create digital twins with agent-based models\n",
    "- Scale from single agents to agent ecosystems\n",
    "\n",
    "**Practice Ideas**:\n",
    "1. Add more tools to the research agent (filter by year, compare citations)\n",
    "2. Build an agent for a different domain (finance, healthcare, education)\n",
    "3. Implement streaming responses for better UX\n",
    "4. Add error handling and retry logic\n",
    "5. Create a simple web UI with FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3-f4a5-6781-3456-901234567892",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "**PydanticAI Documentation**:\n",
    "- [Getting Started](https://ai.pydantic.dev/)\n",
    "- [Agents Guide](https://ai.pydantic.dev/agents/)\n",
    "- [Tools Documentation](https://ai.pydantic.dev/tools/)\n",
    "- [Dependencies](https://ai.pydantic.dev/dependencies/)\n",
    "- [Examples](https://ai.pydantic.dev/examples/)\n",
    "\n",
    "**Pydantic Core**:\n",
    "- [Pydantic V2](https://docs.pydantic.dev/)\n",
    "- [Field Validation](https://docs.pydantic.dev/latest/concepts/fields/)\n",
    "- [Models](https://docs.pydantic.dev/latest/concepts/models/)\n",
    "\n",
    "**Python Type System**:\n",
    "- [Type Hints](https://docs.python.org/3/library/typing.html)\n",
    "- [Protocols](https://peps.python.org/pep-0544/)\n",
    "- [Generics](https://docs.python.org/3/library/typing.html#generics)\n",
    "\n",
    "**Production Topics** (for later):\n",
    "- Testing with pytest\n",
    "- Monitoring with Logfire\n",
    "- Deployment patterns\n",
    "- Cost optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
