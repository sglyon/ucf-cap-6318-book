{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; TD learning\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Statistics and Probability\n",
    "- Dynamic Programming\n",
    "- Reinforcement Learning Introduction\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the meaning of the $Q(s, a)$ function\n",
    "- Understand the concept of a temporal difference\n",
    "- Apply temporal differences to form an RL algorithm (Sarsa)\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 4-6\n",
    "- [Stokey and Lucas (1989)](https://www.jstor.org/stable/j.ctvjnrt76) Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Reminder: Dynamic programming\n",
    "\n",
    "- Let's begin by recalling what we know about dynamic programming\n",
    "- Recall the cake eating problem:\n",
    "    - Time is discrete\n",
    "    - $\\beta$ is discount factor\n",
    "    - Size of cake is $\\bar{x}$\n",
    "    - Consumption of cake in period $t$ is $c_t$\n",
    "    - Utility function $u: \\mathbb{R} \\rightarrow \\mathbb{R}$ maps from consumption today into happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sequential Problem\n",
    "\n",
    "- Objective\n",
    "$$\\begin{aligned}\n",
    "\\max_{c_t} &\\sum_{t=0}^{\\infty} \\beta^t u(c_t) \\\\\n",
    "\\text{subject to } \\quad & \\sum_{t=0}^{\\infty} c_t \\le \\bar{x} \\\\\n",
    "& c_t \\ge 0 \\quad \\forall t\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Need to solve for *infinite* sequence $\\{c_t\\}_t$\n",
    "- Or..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Value Function\n",
    "\n",
    "- We can set up a value function $$v(\\bar{x}) \\equiv \\sum_{t=0}^{\\infty} \\beta^t u(c_t)$$\n",
    "- $v(\\bar{x})$ is the total *value* the consumer places on having a cake of size $\\bar{x}$\n",
    "- Decompose $v$ into two steps: first period + later periods $$v(\\bar{x}) = \\underbrace{u(c_0)}_{\\text{flow utility}} + \\underbrace{\\beta \\sum_{t=1}^{\\infty} \\beta^{t-1} u(c_t)}_{\\text{continuation utility}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Recursive Formulation\n",
    "\n",
    "- Note: continuation utility depends on $x_{t+1} = x_t - c_t$\n",
    "- Use this observation to write $v: \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ recursively:\n",
    "$$\\begin{aligned}\n",
    "  v(x_t) &= \\max_{0 \\leq c_t  \\leq x} \\underbrace{u(c_t)}_{\\text{flow utility}} + \\underbrace{\\beta v(x_t - c_t)}_{\\text{continuation value}}\n",
    "\\end{aligned}$$\n",
    "- This is known as the **Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Solution to Recursive Problem\n",
    "\n",
    "- A solution to the dynamic program consists of two functions:\n",
    "    1. **Value function** $v^*(x): \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ -- value of beginning period with $x$ cake remaining\n",
    "    2. **Policy function** $c^*(x): \\mathbb{R}^+ \\rightarrow [0, x]$ -- optimal level of consumption with $x$ cake remaining\n",
    "- Under certain regularity conditions (which we assume), the recursive problem (and its solution) is equivalent to the sequential problem we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Connection to RL\n",
    "\n",
    "- How does this connect to RL?\n",
    "- (S, A, R) pattern for RL is very closely related to recursive formulation of dynamic programming\n",
    "    - $S_t \\Longrightarrow x_t$\n",
    "    - $A_t \\Longrightarrow c_t$ \n",
    "    - $R_t \\Longrightarrow u(c_t)$\n",
    "- Expressing $v(x) = \\text{flow utility} + \\text{ continuation value}$ is like repeating (S, A, R) sequence many times\n",
    "- Baseline algorithm for solving DP problem (VFI) is quite similar to how basic RL algorithms work\n",
    "    - Start with guess for value, make decision, update guess, repeat..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## TD-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Baseline Assumptions\n",
    "\n",
    "- Let state space $\\mathcal{S}$ and action space $\\mathcal{A}$ be discrete\n",
    "- Let $S_t \\in \\mathcal{S}$  represent state at time $t$\n",
    "- Let $A_t \\in \\mathcal{A}(S_t)$ represent action at time $t$\n",
    "- Let $R_{t+1} \\in \\mathcal{R} \\subseteq \\mathbb{R}$ represent reward at time $t+1$\n",
    "- Let state transitions satisfy the Markov property such that \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& p(s', r | s, a) = \\text{Prob}(S_{t+1}=s', R_{t+1}=r | S_{t} = s, A_{t} = 1) \\\\\n",
    "\\text{ where } \\quad & \\sum_{s' \\in \\mathcal{S}} \\sum_{R \\in \\mathcal{R}} p(s', r | s, a) = 1 \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(S) \\\\\n",
    "\\text{ and } \\quad & p(s', r | s, a) \\ge = 0 \\quad \\forall s, s' \\in \\mathcal{S}, a \\in \\mathcal{A}(s), r \\in \\mathcal{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Note *Markov* means that probability for $S_{t+1}, R_{t+1}$ only depends on $S_t, A_t$ and not and $S_i, A_i, R_i$ where $i < t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### State Value Function\n",
    "\n",
    "- Let $v^*(s)$ be the optimal value of being in state $s$ (called *state value function*)\n",
    "- We write this as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^*(s) &= \\max_{a \\in \\mathcal{A}(s)} E \\left[R_{t+1} + \\beta v^*(S_{t+1}) | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\max_{a \\in \\mathcal{A}(s)} \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta v^*(s') \\right]\n",
    "\\end{aligned}$$\n",
    "- Should be familiar from our dynamic programming studies\n",
    "- Note expectation around the flow utility term $R_{t+1}$, leaving room for that reward to be stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Action Value Function\n",
    "\n",
    "- We can also write an *action value function*\n",
    "- Let $q^*(s, a)$ be the optimal value of being in state $s$ and choosing action $a$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q^*(s, a) &= E \\left[R_{t+1} + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(S_{t+1}, a') | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(s', a') \\right]\n",
    "\\end{aligned}$$\n",
    "- Notice max operator is now *inside* the expectation and applied to future decision $a'$\n",
    "- The function $q^*(s, a)$ is more general than $v^*(s)$: $$v^*(s) = \\max_{a \\in \\mathcal{A}(s)} q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Acting with $q^*$\n",
    "\n",
    "- Goal of RL is to learn to make decisions that maximize $\\sum \\beta^t R_t$\n",
    "- Knowing $q^*(s, a)$ tells us maximium value of being in state $s$ and choosing $a$\n",
    "- If we *knew* $q^*$, acting optimally would be easy: $$a^*(s) = \\text{argmax}_{a\n",
    "           \\in \\mathcal{A}(s)} q^*(s, a)$$\n",
    "- However, we rarely if ever *know* $q^*$, so we must approximate it\n",
    "- We will let $Q(s, a)$ represent our *approximation* of $q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Approximating $q^*$\n",
    "\n",
    "- The goal of TD learning is to find an accurate approximation $Q(s, a)$ such that $Q(s, a) \\approx q^*(s, a) \\; \\forall s, a)$\n",
    "- There are many RL algorithms that seek to do this\n",
    "- We'll focus on two:\n",
    "    - Sarsa\n",
    "    - Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Temporal Differences\n",
    "\n",
    "\n",
    "- The Bellman equaition for our approximation $Q(s, a)$ is $$Q(s,a) = E[R' + \\beta \\max_{a'} Q(s', a') | s,a]$$\n",
    "- Suppose that we interacted with environment and have in hand $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "- Now we plug these into the Bellman by:\n",
    "    1. Using the form of the Bellman\n",
    "    2. But drop $E$ and $\\max$ because we already know the transition that did occur from $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "    3. Change $=$ an $\\approx$ because this isn't evaluating full Bellman\n",
    "$$Q(S_t,A_t) \\approx R_{t+1} + \\beta  Q(S_{t+1}, A_{t+1})$$\n",
    "- The difference between the left and right and sides is known as a temporal difference: $$TD(0)(Q) \\equiv R_{t+1} + \\beta Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$$\n",
    "\n",
    "> There are extensions to the temporal difference that allow for multiple time periods. The 0 in $TD(0)$ indicates that this is *one-step* TD learning. See Chapters 7 and 12 of Sutton/Barto for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Learning using TD(0)\n",
    "\n",
    "- We can use temporal differences to *improve* our approximation $Q$:\n",
    "- Let $Q_t(s, a)$ represent our approximation at the start of period $t$\n",
    "- Similar to gradient descent methods, we will take a step from $Q_t$ in a direction that improves its accuracy\n",
    "- To improve accuracy we step in direction of $TD(0)(Q_t)$ (using step size $\\alpha$): \n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{t+1}(S_t, A_t) &= Q_t(S_t, A_t) + \\alpha TD(0)(Q_t) \\\\\n",
    "&= Q_t(S_t, A_t) + \\alpha \\left[R_{t+1} + \\beta Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t, A_t) \\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sarsa Algorithm\n",
    "\n",
    "- The Sarsa algorithm applies the update rule we just described\n",
    "- The algorithm is summarized by Barto and Sutton as follows (section 6.4)\n",
    "\n",
    "![sarsa_barto_sutton.png](./sarsa_barto_sutton.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ:\n",
    "    def __init__(self):\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "\n",
    "    def __call__(self, s, a):\n",
    "        return self.Q[(s.observable_state(), a)]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        s, a = k\n",
    "        self.Q[(s.observable_state(), a)] = v\n",
    "\n",
    "    def get_greedy(self, s, A_s):\n",
    "        vals = [self(s, a) for a in A_s]\n",
    "        max_val = max(vals)\n",
    "        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sarsa(object):\n",
    "    def __init__(self, environment, epsilon=0.9, alpha=0.1, beta=1.0):\n",
    "        self.env = environment\n",
    "        self.Q = TabularQ()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.restart_episode()\n",
    "\n",
    "    def restart_episode(self):\n",
    "        # These will be (S, A) in our notation. Need to initialize\n",
    "        self.s = self.env.reset()\n",
    "        self.a = self.act(self.s, self.env.enumerate_options(self.s))\n",
    "\n",
    "    def get_greedy(self, s, A_s):\n",
    "        return self.Q.get_greedy(s, A_s)\n",
    "\n",
    "    def act(self, s, A_s):\n",
    "        if random.random() > self.epsilon:\n",
    "            return random.choice(A_s)\n",
    "        return self.get_greedy(s, A_s)\n",
    "\n",
    "    def done(self, s=None) -> bool:\n",
    "        return self.env.done(s if s else self.s)\n",
    "\n",
    "    def step(self):\n",
    "        # first take the step (s, a)\n",
    "        s, a = self.s, self.a\n",
    "        sp, r = self.env.step(s, a)\n",
    "\n",
    "        if self.done(sp):\n",
    "            # game is over\n",
    "            self.s = sp\n",
    "            return\n",
    "\n",
    "        # then use policy to compute ap\n",
    "        A_sp = self.env.enumerate_options(sp)\n",
    "        ap = self.act(sp, A_sp)\n",
    "\n",
    "        # now we know S-A-R-S'-A' -- ready to do update\n",
    "        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n",
    "        Q[(s, a)] = Q(s, a) + α * (r + β * Q(sp, ap) - Q(s, a))\n",
    "\n",
    "        # step forward in time\n",
    "        self.s = sp\n",
    "        self.a = ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: Farkle\n",
    "\n",
    "- In a separate video we implemented the dice game farkle\n",
    "- We'll re-use that code as an environment for `Sarsa` algorithm\n",
    "- For a review of farkle, see video\n",
    "- Today we'll approach it like the RL algorihtm will: \n",
    "    - A stochastic environment that sends states, a list of possible actions, and rewards\n",
    "    - We will *not* specialize based on rules of game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Farkle env\n",
    "\n",
    "- Let's wrap farkle code into environment `Sarsa` expects\n",
    "- Need a few key methods:\n",
    "    - `reset() -> State`\n",
    "    - `enumerate_options(state) -> List[Action]`\n",
    "    - `step(s: State, a: Action) -> Tuple[State, Reward]`\n",
    "    - `done(s:State) -> bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import State, Action, RandomFarklePlayer, FarklePlayer, STOP, BANKRUPT\n",
    "from typing import List, Tuple\n",
    "\n",
    "class FarkleEnv:\n",
    "    # first, some helper methods\n",
    "    def __init__(\n",
    "            self,\n",
    "            opponent: FarklePlayer=RandomFarklePlayer(),\n",
    "            points_to_win=10_000,\n",
    "            verbose: bool = False\n",
    "        ):\n",
    "        self.points_to_win = points_to_win\n",
    "        self.opponent = opponent\n",
    "        self.n_players = 2\n",
    "        self._state = State(self.n_players)\n",
    "        self._history: List[Tuple[State, Action]] = []\n",
    "\n",
    "    @property\n",
    "    def state(self) -> State:\n",
    "        return self._state\n",
    "\n",
    "    def set_state(self, action: Action, new_state: State):\n",
    "        self._history.append((self.state, action))\n",
    "        self._state = new_state\n",
    "\n",
    "    def opponent_turn(self, s: State) -> State:\n",
    "        choices = s.enumerate_options()\n",
    "        action = self.opponent.act(s, choices)\n",
    "        sp = s.step(action)\n",
    "\n",
    "        # check if player chose to stop\n",
    "        if sp.current_player != 1:\n",
    "            return sp\n",
    "\n",
    "        # Player didn't stop, but still their turn. Call again\n",
    "        return self.opponent_turn(sp)\n",
    "\n",
    "    # key methods needed\n",
    "    def done(self, state) -> bool:\n",
    "        return any(score > self.points_to_win for score in state.scores)\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = State(self.n_players)\n",
    "        self._history = []\n",
    "        return self.state.roll()\n",
    "\n",
    "    def step(self, s: State, a: Action) -> Tuple[State, int]:\n",
    "        sp = s.step(a)\n",
    "        r = 0\n",
    "\n",
    "        # see if we ended\n",
    "        if sp.current_player != 0:\n",
    "            if a is STOP:\n",
    "                # only score when we choose to stop\n",
    "                r = s.turn_sum\n",
    "\n",
    "            # take opponent turn\n",
    "            sp = self.opponent_turn(sp)\n",
    "\n",
    "        self.set_state(a, sp)\n",
    "        return sp, r\n",
    "\n",
    "    def enumerate_options(self, s: State) -> List[Action]:\n",
    "        return self.state.enumerate_options(s.rolled_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Playing Farkle\n",
    "\n",
    "- Let's try it out!\n",
    "- We need to create an env, then pass it to sarsa\n",
    "- We'll also define a helper function to play a game for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FarkleEnv()\n",
    "sarsa = Sarsa(env, epsilon=0.9, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_game(algo):\n",
    "    algo.restart_episode()\n",
    "    while not algo.done():\n",
    "        algo.step()\n",
    "    return algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Round: 70. Score: [10050, 7450]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(sarsa)\n",
    "sarsa.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1176e96a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [h[0].scores for h in env._history]\n",
    "plt.plot(scores)\n",
    "plt.legend([\"sarsa\", \"random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import HumanFarklePlayer\n",
    "env_human = FarkleEnv(opponent=HumanFarklePlayer(name=\"Spencer\"), points_to_win=2000)\n",
    "sarsa_human = Sarsa(env_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_game(sarsa_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Round: 0. Score: [0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa_human.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Learning\n",
    "\n",
    "- Great! Our algorithm can play Farkle\n",
    "- But... it needs to play *many* games to learn how to play well\n",
    "- Let's let it play many more games to build up some intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0/20000 (len(Q) = 385)\n",
      "Done with 2000/20000 (len(Q) = 9030)\n",
      "Done with 4000/20000 (len(Q) = 10516)\n",
      "Done with 6000/20000 (len(Q) = 11895)\n",
      "Done with 8000/20000 (len(Q) = 12958)\n",
      "Done with 10000/20000 (len(Q) = 14040)\n",
      "Done with 12000/20000 (len(Q) = 14726)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:16\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:5\u001b[0m, in \u001b[0;36mplay_many_games\u001b[0;34m(N)\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mplay_game\u001b[0;34m(algo)\u001b[0m\n\u001b[1;32m      2\u001b[0m algo\u001b[38;5;241m.\u001b[39mrestart_episode()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m algo\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m----> 4\u001b[0m     \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algo\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mSarsa.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# first take the step (s, a)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     s, a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma\n\u001b[0;32m---> 30\u001b[0m     sp, r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone(sp):\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;66;03m# game is over\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms \u001b[38;5;241m=\u001b[39m sp\n",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m, in \u001b[0;36mFarkleEnv.step\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     55\u001b[0m         r \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mturn_sum\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# take opponent turn\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     sp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopponent_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_state(a, sp)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sp, r\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mFarkleEnv.opponent_turn\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sp\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Player didn't stop, but still their turn. Call again\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopponent_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mFarkleEnv.opponent_turn\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     27\u001b[0m choices \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39menumerate_options()\n\u001b[1;32m     28\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopponent\u001b[38;5;241m.\u001b[39mact(s, choices)\n\u001b[0;32m---> 29\u001b[0m sp \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# check if player chose to stop\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39mcurrent_player \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Teaching/UCF/CAP-6318/book-myst/reinforcement_learning/farkle.py:283\u001b[0m, in \u001b[0;36mState.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    280\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_turn(forced\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# otherwise the player used some dice\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_dice\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_state\n",
      "File \u001b[0;32m~/Teaching/UCF/CAP-6318/book-myst/reinforcement_learning/farkle.py:175\u001b[0m, in \u001b[0;36mState.play_dice\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_dice\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: Action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# TODO: validate that the chosen dice exist\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# add value to the current sum\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     out\u001b[38;5;241m.\u001b[39mturn_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/miniforge3/envs/cap-6318/lib/python3.12/copy.py:79\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     77\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__copy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Teaching/UCF/CAP-6318/book-myst/reinforcement_learning/farkle.py:126\u001b[0m, in \u001b[0;36mState.__copy__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__copy__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    125\u001b[0m     out \u001b[38;5;241m=\u001b[39m State(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_players)\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Teaching/UCF/CAP-6318/book-myst/reinforcement_learning/farkle.py:119\u001b[0m, in \u001b[0;36mState.__dict__\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;129m@__dict__\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dict__\u001b[39m(\u001b[38;5;28mself\u001b[39m, val: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m val\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, k, \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/cap-6318/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/cap-6318/lib/python3.12/copy.py:196\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    194\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[0;32m--> 196\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniforge3/envs/cap-6318/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniforge3/envs/cap-6318/lib/python3.12/copy.py:253\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n\u001b[1;32m    252\u001b[0m     args \u001b[38;5;241m=\u001b[39m (deepcopy(arg, memo) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[0;32m--> 253\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m    255\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n",
      "File \u001b[0;32m~/miniforge3/envs/cap-6318/lib/python3.12/copy.py:252\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m deep \u001b[38;5;241m=\u001b[39m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n\u001b[0;32m--> 252\u001b[0m     args \u001b[38;5;241m=\u001b[39m (deepcopy(arg, memo) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)\n\u001b[1;32m    253\u001b[0m y \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def play_many_games(N):\n",
    "    terminal_states = []\n",
    "    print_skip = N // 10\n",
    "    for i in range(N):\n",
    "        play_game(sarsa)\n",
    "        terminal_states.append(sarsa.s)\n",
    "        if i % print_skip == 0:\n",
    "            print(f\"Done with {i}/{N} (len(Q) = {len(sarsa.Q.Q)})\")\n",
    "    return terminal_states\n",
    "\n",
    "# WARNING: this takes a *long time* and requires a lot of ram!\n",
    "# Only use on a computer with at least 32 GB ram\n",
    "# There are ways we could optimize this... such as only including\n",
    "# final score in `terminal_states` and dropping things like current_round\n",
    "# from the state\n",
    "sarsa_history = play_many_games(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Did we learn?\n",
    "\n",
    "- Let's analyze the history and see if the algorithm learned with experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won = np.array([s.scores[0] > s.scores[1] for s in sarsa_history])\n",
    "game_idx = np.arange(len(won))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(game_idx, won.cumsum())\n",
    "ax.plot(game_idx, 0.5 * game_idx)\n",
    "plt.legend([\"sarsa\", \"E[random agent]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "cap-6318",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
