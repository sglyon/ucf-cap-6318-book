{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation: Augmenting AI with External Knowledge\n",
    "\n",
    "> Computational Analysis of Social Complexity\n",
    ">\n",
    "> Fall 2025, Spencer Lyon\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- L.A1.01 (LLMs and API calls)\n",
    "- Basic understanding of vector spaces\n",
    "- Networks concepts (Weeks 3-5)\n",
    "\n",
    "**Required Julia Packages**\n",
    "\n",
    "Install the following packages if you haven't already:\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add([\"HTTP\", \"JSON3\", \"DotEnv\", \"LinearAlgebra\", \"DataFrames\"])\n",
    "```\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the limitations of parametric knowledge in LLMs\n",
    "- Implement a complete Retrieval-Augmented Generation (RAG) system\n",
    "- Work with embeddings and perform vector similarity search\n",
    "- Design knowledge bases for domain-specific AI applications\n",
    "- Analyze trade-offs between parametric and retrieval-based knowledge\n",
    "\n",
    "**References**\n",
    "\n",
    "- Lewis et al. (2020) \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" [arXiv:2005.11401](https://arxiv.org/abs/2005.11401)\n",
    "- Gao et al. (2023) \"Retrieval-Augmented Generation for Large Language Models: A Survey\" [arXiv:2312.10997](https://arxiv.org/abs/2312.10997)\n",
    "- OpenAI Embeddings Documentation: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "- OpenAI Responses API Quickstart: [https://platform.openai.com/docs/quickstart?api-mode=responses](https://platform.openai.com/docs/quickstart?api-mode=responses)\n",
    "- Anthropic Context Window Documentation: [https://docs.anthropic.com/claude/docs/models-overview](https://docs.anthropic.com/claude/docs/models-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## The Knowledge Problem\n",
    "\n",
    "- In the previous lecture we learned how to interact with LLMs via API calls\n",
    "- We saw that these models can be remarkably helpful for various tasks\n",
    "- However, LLMs have a fundamental limitation: they only \"know\" what was in their training data\n",
    "- This creates several problems that we'll explore now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "### Problem 1: Knowledge Cutoffs\n",
    "\n",
    "- LLMs are trained on data collected up to a specific date\n",
    "- For example, GPT-4 (original) had a knowledge cutoff of September 2021\n",
    "- Claude 3.5 Sonnet has a knowledge cutoff of April 2024\n",
    "- Any events, research, or information after the cutoff date is unknown to the model\n",
    "- This means:\n",
    "  - Can't answer questions about recent events\n",
    "  - Doesn't know about new research papers or discoveries\n",
    "  - Unaware of updated statistics or data\n",
    "\n",
    "**Example**: If you ask GPT-4 (original) about a paper published in 2023, it will either say it doesn't know or worse, hallucinate details about a paper it has never seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "### Problem 2: Hallucination\n",
    "\n",
    "- LLMs are trained to generate plausible-sounding text\n",
    "- When asked about something they don't know, they often **hallucinate**: generate confident-sounding but factually incorrect information\n",
    "- This is particularly dangerous because:\n",
    "  - The model sounds authoritative\n",
    "  - Users may not realize the information is false\n",
    "  - The hallucinated content can include fake citations, statistics, or facts\n",
    "\n",
    "**Example**: Ask an LLM about a research paper that doesn't exist. It might generate a plausible abstract, author names, and even \"key findings\" - all completely fabricated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "### Problem 3: Domain-Specific Knowledge\n",
    "\n",
    "- Even within their training data, LLMs have uneven knowledge\n",
    "- Some domains are well-represented in training data (e.g., popular programming languages)\n",
    "- Others are sparse (e.g., specialized academic subfields, proprietary company data)\n",
    "- For your organization's specific documents, data, or knowledge base, the LLM knows nothing\n",
    "\n",
    "**Example**: You want an AI assistant to help with your company's internal policies, product documentation, or research papers. The LLM has never seen these documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Connection to Network Information Flow\n",
    "\n",
    "- Recall from Weeks 3-4 our study of information diffusion in networks\n",
    "- We saw how information flows through network structures\n",
    "- Key insight: **who you're connected to determines what information you can access**\n",
    "- LLMs face a similar problem:\n",
    "  - Their \"connections\" are frozen at training time\n",
    "  - They can't access new information sources\n",
    "  - They can't form new \"edges\" to external knowledge\n",
    "- RAG solves this by creating a **dynamic connection** between the LLM and external knowledge sources\n",
    "- Think of RAG as giving the LLM the ability to form new edges in the information network on-demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### The Solution: Retrieval-Augmented Generation\n",
    "\n",
    "- Instead of relying solely on parametric knowledge (what's encoded in model weights), we can augment the LLM with **retrieval**\n",
    "- Basic idea:\n",
    "  1. User asks a question\n",
    "  2. Search a knowledge base for relevant information\n",
    "  3. Provide that information to the LLM as context\n",
    "  4. LLM generates answer based on retrieved information\n",
    "- This approach:\n",
    "  - Eliminates knowledge cutoff problem (use up-to-date sources)\n",
    "  - Reduces hallucination (model works from provided facts)\n",
    "  - Enables domain-specific applications (use your own documents)\n",
    "  - Provides citations (you know which documents were used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Embeddings: Representing Meaning as Vectors\n",
    "\n",
    "- To implement retrieval, we need to search for relevant documents\n",
    "- But how do we determine if a document is relevant to a query?\n",
    "- The key innovation: **embeddings** - representations of text as high-dimensional vectors\n",
    "- Embeddings capture semantic meaning: similar meanings → similar vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### What are Embeddings?\n",
    "\n",
    "- An embedding is a function $f: \\text{Text} \\rightarrow \\mathbb{R}^d$ that maps text to a d-dimensional vector\n",
    "- Typically $d$ ranges from 384 to 3072 dimensions\n",
    "- Key property: texts with similar meanings have similar embeddings (vectors close together in vector space)\n",
    "\n",
    "**Intuition**: Think of each dimension as capturing some aspect of meaning\n",
    "- One dimension might capture \"technical vs casual\"\n",
    "- Another might capture \"positive vs negative sentiment\" \n",
    "- Another might capture \"about networks vs about game theory\"\n",
    "- With hundreds or thousands of dimensions, we can capture nuanced semantic relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### Example: Word Analogies\n",
    "\n",
    "One famous property of embeddings is that they capture relational meaning:\n",
    "\n",
    "$$\\text{embedding}(\\text{\"king\"}) - \\text{embedding}(\\text{\"man\"}) + \\text{embedding}(\\text{\"woman\"}) \\approx \\text{embedding}(\\text{\"queen\"})$$\n",
    "\n",
    "This works because the embedding space organizes concepts by their relationships:\n",
    "- The vector from \"man\" to \"king\" represents \"royalty\"\n",
    "- Adding that same vector to \"woman\" gives you something close to \"queen\"\n",
    "\n",
    "Similar patterns work for other relationships:\n",
    "- Paris - France + Germany ≈ Berlin\n",
    "- Walking - Walked + Swimming ≈ Swam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Measuring Similarity: Cosine Similarity\n",
    "\n",
    "- Once we have embeddings, we need a way to measure similarity between vectors\n",
    "- The most common metric is **cosine similarity**\n",
    "- For two vectors $\\mathbf{a}$ and $\\mathbf{b}$:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{a}|| \\, ||\\mathbf{b}||} = \\frac{\\sum_{i=1}^d a_i b_i}{\\sqrt{\\sum_{i=1}^d a_i^2} \\sqrt{\\sum_{i=1}^d b_i^2}}$$\n",
    "\n",
    "- Returns a value between -1 and 1\n",
    "  - 1: vectors point in same direction (very similar)\n",
    "  - 0: vectors are orthogonal (unrelated)\n",
    "  - -1: vectors point in opposite directions (opposite meaning)\n",
    "- Why cosine and not Euclidean distance?\n",
    "  - Cosine measures angle, not magnitude\n",
    "  - Makes sense for text: a longer document isn't necessarily \"farther\" in meaning\n",
    "  - Embedding models are typically optimized for cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Creating Embeddings with OpenAI\n",
    "\n",
    "Let's see how to create embeddings using OpenAI's API.\n",
    "\n",
    "**Setup**: Before running the code below, make sure you have a `.env` file in your working directory with your OpenAI API key:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-your-api-key-here\n",
    "```\n",
    "\n",
    "The `DotEnv` package will automatically load these environment variables from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_embedding (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using HTTP, JSON3, DotEnv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "DotEnv.load!()\n",
    "\n",
    "# Load API key from environment\n",
    "OPENAI_API_KEY = ENV[\"OPENAI_API_KEY\"]\n",
    "\n",
    "function get_embedding(text::String; model=\"text-embedding-3-small\")\n",
    "    \"\"\"\n",
    "    Get embedding vector for input text using OpenAI's API\n",
    "\n",
    "    Args:\n",
    "        text: Input text to embed\n",
    "        model: Embedding model to use (default: text-embedding-3-small)\n",
    "\n",
    "    Returns:\n",
    "        Vector of floats representing the embedding\n",
    "    \"\"\"\n",
    "    url = \"https://api.openai.com/v1/embeddings\"\n",
    "\n",
    "    headers = [\n",
    "        \"Authorization\" => \"Bearer $(OPENAI_API_KEY)\",\n",
    "        \"Content-Type\" => \"application/json\"\n",
    "    ]\n",
    "\n",
    "    body = JSON3.write(Dict(\n",
    "        \"input\" => text,\n",
    "        \"model\" => model\n",
    "    ))\n",
    "\n",
    "    response = HTTP.post(url, headers, body)\n",
    "    result = JSON3.read(String(response.body))\n",
    "\n",
    "    return Vector(result.data[1].embedding)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "Let's test this with some network-related concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1536\n",
      "First few values of first embedding: [-0.059553657, -0.029068848, 0.023821464, -0.023550766, -0.0076992884]\n"
     ]
    }
   ],
   "source": [
    "# Embed some network-related texts\n",
    "texts = [\n",
    "    \"Graph theory studies the mathematical structure of networks\",\n",
    "    \"The clustering coefficient measures the tendency of nodes to form triangles\",\n",
    "    \"Preferential attachment leads to power law degree distributions\",\n",
    "    \"I enjoy eating pizza with extra cheese\"\n",
    "]\n",
    "\n",
    "embeddings = [get_embedding(text) for text in texts]\n",
    "\n",
    "println(\"Embedding dimension: \", length(embeddings[1]))\n",
    "println(\"First few values of first embedding: \", embeddings[1][1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "Now let's compute cosine similarity between these texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities to: 'Graph theory studies the mathematical structure of networks'\n",
      "\n",
      "Text 1 (Graph theory studies the mathematical st...): 1.0\n",
      "Text 2 (The clustering coefficient measures the ...): 0.3744\n",
      "Text 3 (Preferential attachment leads to power l...): 0.4689\n",
      "Text 4 (I enjoy eating pizza with extra cheese...): 0.0422\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function cosine_similarity(a::Vector{Float64}, b::Vector{Float64})\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors\n",
    "    \"\"\"\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "end\n",
    "\n",
    "# Compare first text (graph theory) to all others\n",
    "println(\"Similarities to: '\", texts[1], \"'\\n\")\n",
    "for (i, text) in enumerate(texts)\n",
    "    sim = cosine_similarity(embeddings[1], embeddings[i])\n",
    "    println(\"Text $i ($(text[1:min(40, end)])...): $(round(sim, digits=4))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "- The first three texts (all about networks) have high similarity to each other (typically > 0.7)\n",
    "- The pizza text has low similarity to the network texts (typically < 0.5)\n",
    "- This demonstrates that embeddings capture semantic meaning\n",
    "- Texts about similar topics cluster together in the embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploring Embeddings\n",
    "\n",
    "Create embeddings for the following texts and compute their pairwise similarities:\n",
    "1. \"Nash equilibrium is a solution concept in game theory\"\n",
    "2. \"Players choose strategies to maximize their payoffs\"\n",
    "3. \"Agent-based models simulate individual decision makers\"\n",
    "4. \"The weather today is sunny and warm\"\n",
    "\n",
    "Questions:\n",
    "- Which pairs of texts have the highest similarity?\n",
    "- Does the game theory text have higher similarity to the ABM text or the weather text? Why?\n",
    "- What does this tell you about how embeddings capture domain knowledge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Building a RAG Pipeline\n",
    "\n",
    "- Now that we understand embeddings, we can build a complete RAG system\n",
    "- A RAG pipeline has four main stages:\n",
    "  1. **Chunking**: Break documents into manageable pieces\n",
    "  2. **Embedding**: Convert chunks to vectors and store them\n",
    "  3. **Retrieval**: Find most relevant chunks for a query\n",
    "  4. **Generation**: Use LLM to answer query with retrieved context\n",
    "\n",
    "Let's build each component step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Stage 1: Chunking\n",
    "\n",
    "- Most documents are too long to embed as a single unit\n",
    "- We need to break them into **chunks** - smaller, semantically meaningful pieces\n",
    "- Typical chunk sizes: 100-500 words or 400-2000 characters\n",
    "\n",
    "**Why chunk?**\n",
    "- Embeddings work best on focused, coherent pieces of text\n",
    "- Enables fine-grained retrieval (find specific relevant paragraphs, not whole documents)\n",
    "- Fits within LLM context limits (we can only inject so much context)\n",
    "\n",
    "**Chunking strategies**:\n",
    "1. Fixed size: Every N characters or words\n",
    "2. Natural boundaries: By paragraph, section, or sentence\n",
    "3. Semantic: Using embeddings to identify topic shifts\n",
    "\n",
    "For this lecture, we'll use a simple paragraph-based chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chunk_text (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function chunk_text(text::String; max_chunk_size::Int=1000, overlap::Int=100)\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks\n",
    "\n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        max_chunk_size: Maximum characters per chunk\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        Vector of text chunks\n",
    "    \"\"\"\n",
    "    # Split by paragraphs first (double newline)\n",
    "    paragraphs = split(text, \"\\n\\n\")\n",
    "\n",
    "    chunks = String[]\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for para in paragraphs\n",
    "        # If adding this paragraph would exceed max size, save current chunk\n",
    "        if length(current_chunk) + length(para) > max_chunk_size && !isempty(current_chunk)\n",
    "            push!(chunks, strip(current_chunk))\n",
    "            # Start new chunk with overlap from previous\n",
    "            current_chunk = current_chunk[max(1, end-overlap):end]\n",
    "        end\n",
    "\n",
    "        current_chunk *= para * \"\\n\\n\"\n",
    "    end\n",
    "\n",
    "    # Don't forget last chunk\n",
    "    if !isempty(strip(current_chunk))\n",
    "        push!(chunks, strip(current_chunk))\n",
    "    end\n",
    "\n",
    "    return chunks\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Stage 2: Vector Database\n",
    "\n",
    "- Once we have chunks and embeddings, we need to store them efficiently\n",
    "- A **vector database** stores embeddings and enables fast similarity search\n",
    "- For this lecture, we'll build a simple in-memory vector store\n",
    "- Production systems typically use specialized vector databases like:\n",
    "  - Pinecone\n",
    "  - Weaviate\n",
    "  - Qdrant\n",
    "  - Chroma\n",
    "  - pgvector (PostgreSQL extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "search (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using DataFrames\n",
    "\n",
    "mutable struct VectorStore\n",
    "    documents::Vector{String}          # Original text chunks\n",
    "    embeddings::Vector{Vector{Float64}}  # Corresponding embeddings\n",
    "    metadata::Vector{Dict{String, Any}}  # Optional metadata (source, page, etc.)\n",
    "end\n",
    "\n",
    "# Constructor for empty store\n",
    "VectorStore() = VectorStore(String[], Vector{Float64}[], Dict{String, Any}[])\n",
    "\n",
    "function add_documents!(store::VectorStore, docs::Vector{String}; metadata=nothing)\n",
    "    \"\"\"\n",
    "    Add documents to the vector store by computing and storing their embeddings\n",
    "    \"\"\"\n",
    "    println(\"Embedding $(length(docs)) documents...\")\n",
    "\n",
    "    for (i, doc) in enumerate(docs)\n",
    "        # Get embedding\n",
    "        emb = get_embedding(doc)\n",
    "\n",
    "        # Store document, embedding, and metadata\n",
    "        push!(store.documents, doc)\n",
    "        push!(store.embeddings, emb)\n",
    "\n",
    "        if metadata !== nothing && i <= length(metadata)\n",
    "            push!(store.metadata, metadata[i])\n",
    "        else\n",
    "            push!(store.metadata, Dict(\"index\" => i))\n",
    "        end\n",
    "\n",
    "        if i % 10 == 0\n",
    "            println(\"  Processed $i/$(length(docs))...\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    println(\"Done! Store now contains $(length(store.documents)) documents.\")\n",
    "end\n",
    "\n",
    "function search(store::VectorStore, query::String; top_k::Int=5)\n",
    "    \"\"\"\n",
    "    Search for most similar documents to query\n",
    "\n",
    "    Args:\n",
    "        store: Vector store to search\n",
    "        query: Search query text\n",
    "        top_k: Number of results to return\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: rank, document, similarity, metadata\n",
    "    \"\"\"\n",
    "    # Embed the query\n",
    "    query_emb = get_embedding(query)\n",
    "\n",
    "    # Compute similarities to all documents\n",
    "    similarities = [cosine_similarity(query_emb, doc_emb) for doc_emb in store.embeddings]\n",
    "\n",
    "    # Get top k indices\n",
    "    top_indices = partialsortperm(similarities, 1:min(top_k, length(similarities)), rev=true)\n",
    "\n",
    "    # Build result dataframe\n",
    "    results = DataFrame(\n",
    "        rank = 1:length(top_indices),\n",
    "        document = store.documents[top_indices],\n",
    "        similarity = similarities[top_indices],\n",
    "        metadata = store.metadata[top_indices]\n",
    "    )\n",
    "\n",
    "    return results\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Stage 3 & 4: Retrieval and Generation\n",
    "\n",
    "- Now we can combine retrieval with LLM generation\n",
    "- The key is to inject retrieved documents into the LLM's context\n",
    "- We'll create a function that:\n",
    "  1. Takes a user query\n",
    "  2. Retrieves relevant chunks from vector store\n",
    "  3. Constructs instructions with retrieved context\n",
    "  4. Sends to LLM via OpenAI's Responses API for final answer\n",
    "\n",
    "**Note**: We're using OpenAI's Responses API (`/v1/responses`) which is the modern successor to the Chat Completions API. The Responses API uses `instructions` for system-level guidance and `input` for user queries, providing a cleaner interface for stateful interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "call_gpt (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function rag_query(store::VectorStore, query::String; top_k::Int=3, model::String=\"gpt-5\")\n",
    "    \"\"\"\n",
    "    Answer a query using RAG: retrieve relevant docs, then generate answer\n",
    "\n",
    "    Args:\n",
    "        store: Vector store containing knowledge base\n",
    "        query: User's question\n",
    "        top_k: Number of documents to retrieve\n",
    "        model: OpenAI model to use for generation (default: gpt-5)\n",
    "\n",
    "    Returns:\n",
    "        Generated answer and retrieved documents\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = search(store, query; top_k=top_k)\n",
    "\n",
    "    # Step 2: Construct context from retrieved documents\n",
    "    context = \"\"\n",
    "    for i in 1:nrow(results)\n",
    "        context *= \"Document $i:\\n$(results.document[i])\\n\\n\"\n",
    "    end\n",
    "\n",
    "    # Step 3: Build instructions with context\n",
    "    instructions = \"\"\"\n",
    "    You are a helpful assistant that answers questions based on provided context.\n",
    "    Use the context below to answer the user's question. If the context doesn't\n",
    "    contain enough information to answer the question, say so - do not make up information.\n",
    "\n",
    "    Context:\n",
    "    $(context)\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 4: Call LLM using Responses API\n",
    "    response = call_gpt(\n",
    "        query;\n",
    "        instructions=instructions,\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    return (answer=response, sources=results)\n",
    "end\n",
    "\n",
    "function call_gpt(input::String; instructions::String=\"\", model::String=\"gpt-5-nano\", reasoning_effort::String=\"low\")\n",
    "    url = \"https://api.openai.com/v1/responses\"\n",
    "\n",
    "    # OPENAI_API_KEY should be loaded from ENV via DotEnv\n",
    "    headers = [\n",
    "        \"Authorization\" => \"Bearer $(ENV[\"OPENAI_API_KEY\"])\",\n",
    "        \"Content-Type\" => \"application/json\"\n",
    "    ]\n",
    "\n",
    "    # Build request body with Responses API format\n",
    "    request_body = Dict(\n",
    "        \"model\" => model,\n",
    "        \"input\" => input,\n",
    "        \"reasoning\" => Dict(\"effort\" => reasoning_effort)\n",
    "    )\n",
    "\n",
    "    # Add instructions if provided\n",
    "    if !isempty(instructions)\n",
    "        request_body[\"instructions\"] = instructions\n",
    "    end\n",
    "\n",
    "    body = JSON3.write(request_body)\n",
    "\n",
    "    response = HTTP.post(url, headers, body)\n",
    "    result = JSON3.read(String(response.body))\n",
    "\n",
    "    # NOTE: result.output is an array that can have multiple items. We are assuming a simple case here...\n",
    "    return result.output[end].content[1].text\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Application: Network Analysis Assistant\n",
    "\n",
    "- Let's build a practical RAG application: an AI assistant for graph theory and network analysis\n",
    "- We'll create a knowledge base from summaries of key network concepts\n",
    "- This assistant will be able to answer questions about network theory using retrieved information\n",
    "- In a real application, you would load actual research papers, textbooks, or documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### Building the Knowledge Base\n",
    "\n",
    "Let's create a knowledge base with information about graph theory and network analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created knowledge base with 8 documents\n"
     ]
    }
   ],
   "source": [
    "# Sample documents about network analysis\n",
    "# In practice, you would load these from files, scrape papers, etc.\n",
    "network_docs = [\n",
    "    \"\"\"\n",
    "    Clustering Coefficient\n",
    "\n",
    "    The clustering coefficient measures the degree to which nodes in a graph tend to\n",
    "    cluster together. For a given node, it is defined as the proportion of connections\n",
    "    between the node's neighbors which are also connected to each other.\n",
    "\n",
    "    Formally, for node i with degree k_i, if there are E_i edges between its neighbors,\n",
    "    the local clustering coefficient is C_i = 2*E_i / (k_i * (k_i - 1)).\n",
    "\n",
    "    The global clustering coefficient averages this measure across all nodes. High\n",
    "    clustering coefficients are characteristic of social networks and indicate the\n",
    "    presence of tightly-knit communities.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Preferential Attachment and Scale-Free Networks\n",
    "\n",
    "    Preferential attachment is a mechanism by which networks grow over time. When a new\n",
    "    node joins the network, it forms connections preferentially to nodes that already\n",
    "    have many connections - the \"rich get richer\" phenomenon.\n",
    "\n",
    "    The Barabási-Albert model (1999) demonstrated that preferential attachment leads to\n",
    "    scale-free networks with power law degree distributions. In such networks, P(k) ~ k^(-γ)\n",
    "    where γ is typically between 2 and 3.\n",
    "\n",
    "    Scale-free networks are characterized by the presence of hubs - nodes with\n",
    "    disproportionately high degree. Examples include the Internet, citation networks,\n",
    "    and social media platforms.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Small-World Networks\n",
    "\n",
    "    Small-world networks, introduced by Watts and Strogatz (1998), exhibit two key properties:\n",
    "    high clustering (like regular lattices) and short path lengths (like random graphs).\n",
    "\n",
    "    The small-world phenomenon, popularly known as \"six degrees of separation,\" refers to\n",
    "    the observation that most pairs of nodes in many real networks are connected by short\n",
    "    paths, typically of length 6 or less.\n",
    "\n",
    "    The Watts-Strogatz model generates small-world networks by starting with a ring lattice\n",
    "    and randomly rewiring edges with some probability p. For intermediate values of p,\n",
    "    the network maintains high clustering while gaining shortcuts that reduce path length.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Centrality Measures\n",
    "\n",
    "    Centrality measures identify the most important nodes in a network. Different\n",
    "    centrality measures capture different notions of importance:\n",
    "\n",
    "    1. Degree centrality: Simply counts the number of connections. High degree nodes\n",
    "       are \"hubs\" that directly connect to many others.\n",
    "\n",
    "    2. Betweenness centrality: Counts how often a node lies on shortest paths between\n",
    "       other nodes. High betweenness nodes are \"bridges\" that control information flow.\n",
    "\n",
    "    3. Closeness centrality: Measures the average distance from a node to all others.\n",
    "       High closeness nodes can reach the network quickly.\n",
    "\n",
    "    4. Eigenvector centrality: A node is important if it connects to other important\n",
    "       nodes. This is the principle behind Google's PageRank algorithm.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Community Detection\n",
    "\n",
    "    Community detection aims to identify groups of nodes that are more densely connected\n",
    "    internally than to the rest of the network. Communities represent functional modules\n",
    "    or social groups.\n",
    "\n",
    "    The modularity measure quantifies the quality of a partition. For partition C,\n",
    "    modularity Q = (1/2m) * Σ[A_ij - (k_i*k_j/2m)] * δ(c_i, c_j), where m is the total\n",
    "    number of edges, A_ij is the adjacency matrix, and δ(c_i, c_j) = 1 if nodes i and j\n",
    "    are in the same community.\n",
    "\n",
    "    Popular algorithms include the Louvain method, which greedily optimizes modularity,\n",
    "    and spectral methods based on the graph Laplacian. Community structure appears in\n",
    "    social networks (friend groups), biological networks (protein complexes), and\n",
    "    information networks (topic clusters).\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Network Resilience and Robustness\n",
    "\n",
    "    Network resilience refers to a network's ability to maintain functionality when nodes\n",
    "    or edges are removed. This is crucial for infrastructure networks, communication systems,\n",
    "    and biological systems.\n",
    "\n",
    "    Scale-free networks exhibit a peculiar property: they are robust to random failures\n",
    "    but vulnerable to targeted attacks. Removing random nodes has little effect because\n",
    "    most nodes have low degree. However, removing hubs quickly fragments the network.\n",
    "\n",
    "    The resilience of random graphs is more uniform - they handle both random and targeted\n",
    "    removal similarly. Measuring resilience involves computing the size of the largest\n",
    "    connected component as nodes are progressively removed.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Network Formation and Strategic Link Formation\n",
    "\n",
    "    In many contexts, network connections form through strategic decisions by self-interested\n",
    "    agents. Game-theoretic models of network formation consider the costs and benefits of\n",
    "    forming links.\n",
    "\n",
    "    Jackson and Wolinsky (1996) introduced the concept of pairwise stability: a network is\n",
    "    pairwise stable if no agent wants to sever a link and no pair of agents wants to form\n",
    "    a new link.\n",
    "\n",
    "    The tension between individual incentives and social welfare is central to network\n",
    "    formation theory. The star network, where one central node connects to all others,\n",
    "    often minimizes total distance (socially optimal) but may not be pairwise stable\n",
    "    without appropriate transfer payments.\n",
    "    \"\"\",\n",
    "\n",
    "    \"\"\"\n",
    "    Information Diffusion and Cascades\n",
    "\n",
    "    Information, behaviors, and innovations spread through networks via social influence.\n",
    "    The threshold model (Granovetter, 1978) posits that individuals adopt a behavior when\n",
    "    a threshold fraction of their neighbors have adopted it.\n",
    "\n",
    "    A cascade occurs when an initial adoption by a few nodes triggers a chain reaction\n",
    "    of adoptions throughout the network. Whether cascades occur depends on the distribution\n",
    "    of thresholds and the network structure.\n",
    "\n",
    "    Weak ties play a crucial role in diffusion (Granovetter, 1973). While strong ties\n",
    "    create dense, clustered groups, weak ties serve as bridges between communities and\n",
    "    enable information to reach new populations. This explains why job seekers often find\n",
    "    opportunities through acquaintances rather than close friends.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "println(\"Created knowledge base with $(length(network_docs)) documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "Now let's create our vector store and add these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 8 documents...\n",
      "Done! Store now contains 8 documents.\n"
     ]
    }
   ],
   "source": [
    "# Initialize vector store\n",
    "network_store = VectorStore()\n",
    "\n",
    "# Add documents to store (this will take a moment as we embed each document)\n",
    "add_documents!(network_store, network_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Testing Our Network Analysis Assistant\n",
    "\n",
    "Let's test our RAG system with some questions about networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the clustering coefficient and how is it calculated?\n",
      "\n",
      "Answer:\n",
      "The clustering coefficient quantifies how much a node’s neighbors are connected to each other (how “clustered” they are).\n",
      "\n",
      "- Local clustering coefficient for node i: C_i = 2*E_i / (k_i * (k_i - 1))\n",
      "  - k_i: degree of node i (number of neighbors)\n",
      "  - E_i: number of edges that actually exist between those neighbors\n",
      "  - It measures the fraction of possible neighbor-to-neighbor links that are present.\n",
      "\n",
      "- Global clustering coefficient: the average of the local clustering coefficients over all nodes.\n",
      "\n",
      "High values indicate tightly knit groups, common in social networks.\n",
      "\n",
      "================================================================================\n",
      "Top sources used:\n",
      "\n",
      "[1] (similarity: 0.765)\n",
      "Clustering Coefficient\n",
      "\n",
      "The clustering coefficient measures the degree to which nodes in a graph tend to\n",
      "cluster together. For a given node, it is defined as the proportion of connections\n",
      "between the ...\n",
      "\n",
      "[2] (similarity: 0.416)\n",
      "Community Detection\n",
      "\n",
      "Community detection aims to identify groups of nodes that are more densely connected\n",
      "internally than to the rest of the network. Communities represent functional modules\n",
      "or social...\n"
     ]
    }
   ],
   "source": [
    "# Question 1: About clustering\n",
    "q1 = \"What is the clustering coefficient and how is it calculated?\"\n",
    "result1 = rag_query(network_store, q1)\n",
    "\n",
    "println(\"Question: \", q1)\n",
    "println(\"\\nAnswer:\\n\", result1.answer)\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"Top sources used:\")\n",
    "for i in 1:min(2, nrow(result1.sources))\n",
    "    println(\"\\n[$i] (similarity: $(round(result1.sources.similarity[i], digits=3)))\")\n",
    "    println(result1.sources.document[i][1:min(200, end)] * \"...\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why are scale-free networks vulnerable to targeted attacks?\n",
      "\n",
      "Answer:\n",
      "Because scale-free networks rely on a few high-degree hubs to hold the network together. Targeted attacks that remove these hubs quickly fragment the network and shrink the largest connected component, causing a rapid loss of connectivity, even though random failures (which mostly hit low-degree nodes) have little effect.\n"
     ]
    }
   ],
   "source": [
    "# Question 2: About scale-free networks\n",
    "q2 = \"Why are scale-free networks vulnerable to targeted attacks?\"\n",
    "result2 = rag_query(network_store, q2)\n",
    "\n",
    "println(\"Question: \", q2)\n",
    "println(\"\\nAnswer:\\n\", result2.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What role do weak ties play in information diffusion?\n",
      "\n",
      "Answer:\n",
      "Weak ties act as bridges between otherwise separate, densely clustered groups. By connecting different communities, they let information travel beyond local circles, reaching new populations and enabling broader cascades of adoption. This is why people often hear about job opportunities through acquaintances rather than close friends.\n"
     ]
    }
   ],
   "source": [
    "# Question 3: About weak ties\n",
    "q3 = \"What role do weak ties play in information diffusion?\"\n",
    "result3 = rag_query(network_store, q3)\n",
    "\n",
    "println(\"Question: \", q3)\n",
    "println(\"\\nAnswer:\\n\", result3.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Comparing RAG vs Vanilla LLM\n",
    "\n",
    "Let's demonstrate the value of RAG by comparing answers with and without retrieved context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the formula for local clustering coefficient in a graph?\n",
      "\n",
      "================================================================================\n",
      "Vanilla LLM Answer:\n",
      "For an undirected, unweighted graph, the local clustering coefficient of a node v is:\n",
      "\n",
      "C(v) = 2 e_v / [k_v (k_v − 1)]\n",
      "\n",
      "- k_v = degree of v (number of neighbors)\n",
      "- e_v = number of edges between neighbors of v (equivalently, number of triangles involving v)\n",
      "\n",
      "By convention, C(v) = 0 if k_v < 2.\n",
      "\n",
      "Note (directed variant): For directed graphs, the maximal number of possible edges among neighbors is k_v (k_v − 1), so a common definition is C(v) = e_v / [k_v (k_v − 1)], where e_v counts directed edges among neighbors.\n",
      "\n",
      "================================================================================\n",
      "RAG Answer:\n",
      "C_i = 2 E_i / [k_i (k_i − 1)], where k_i is the degree of node i and E_i is the number of edges between its neighbors.\n"
     ]
    }
   ],
   "source": [
    "function vanilla_llm_query(query::String; model::String=\"gpt-5\")\n",
    "    \"\"\"\n",
    "    Query LLM without RAG (no retrieved context)\n",
    "    \"\"\"\n",
    "    return call_gpt(\n",
    "        query;\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        model=model\n",
    "    )\n",
    "end\n",
    "\n",
    "# Ask a specific question that requires precise information\n",
    "specific_q = \"What is the formula for local clustering coefficient in a graph?\"\n",
    "\n",
    "println(\"Question: \", specific_q)\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"Vanilla LLM Answer:\")\n",
    "println(vanilla_llm_query(specific_q))\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"RAG Answer:\")\n",
    "println(rag_query(network_store, specific_q).answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "Both answers are likely correct (this is a well-known formula in the LLM's training data). However:\n",
    "\n",
    "1. **With RAG**, we know exactly where the answer came from (we can cite sources)\n",
    "2. **With RAG**, if we had proprietary or recent information, the LLM couldn't answer without it\n",
    "3. **With RAG**, we can update knowledge without retraining the model\n",
    "\n",
    "Let's test with a question the LLM definitely doesn't know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 documents...\n",
      "Done! Store now contains 9 documents.\n",
      "Question: What is resonance centrality and how is it calculated?\n",
      "\n",
      "================================================================================\n",
      "Vanilla LLM Answer:\n",
      "Resonance centrality is a frequency-dependent notion of node importance for networks with dynamical processes. It ranks nodes by how strongly they respond to an oscillatory (periodic) input that propagates through the network dynamics. In other words, a node is “resonantly central” if the dynamics make it amplify signals at some frequency more than other nodes.\n",
      "\n",
      "How it is defined (general idea)\n",
      "- Choose a linear dynamical model on the network (first- or second-order), and a vector b describing which nodes are externally driven.\n",
      "- Drive the system with a sinusoidal input u(t) = Re{u0 e^{iωt}}.\n",
      "- Compute the steady-state frequency response (transfer function) from the input to each node. The magnitude of that response at node i is its frequency-dependent centrality gi(ω).\n",
      "- Resonance centrality of node i is then taken as, e.g., the peak response maxω gi(ω) (and the corresponding ω is the node’s resonant frequency). Some variants use the squared magnitude or the area under the curve across ω.\n",
      "\n",
      "Common formulations\n",
      "\n",
      "1) First-order (diffusive/consensus-like) dynamics\n",
      "- Dynamics: x′(t) = A x(t) + b u(t) with A stable (often A = −L for a Laplacian L of an undirected graph).\n",
      "- Frequency response: H(ω) = (iωI − A)^{-1} b.\n",
      "- Node-i response magnitude: gi(ω) = |ei^T H(ω)| = |[(iωI − A)^{-1} b]_i|.\n",
      "- Resonance centrality (peak gain): Ri = maxω gi(ω).\n",
      "\n",
      "Notes:\n",
      "- If A = −L with L ≥ 0 (diffusion), the response typically peaks at low frequency (ω → 0). Then Ri ≈ |[L^† b]_i| (L^† is the Moore–Penrose pseudoinverse), so “resonance” reduces to a low-frequency (quasi-static) sensitivity, closely related to resistance-based measures.\n",
      "\n",
      "2) Second-order (oscillator/inertial) dynamics\n",
      "- Dynamics: M x″(t) + C x′(t) + K x(t) = b u(t), with M, C, K ≽ 0 (e.g., coupled oscillators, power grids).\n",
      "- Dynamic stiffness: Z(ω) = K − ω^2 M + iω C.\n",
      "- Frequency response: H(ω) = Z(ω)^{-1} b.\n",
      "- Node-i response magnitude: gi(ω) = |ei^T H(ω)|.\n",
      "- Resonance centrality: Ri = maxω gi(ω). True resonance peaks occur near the system’s natural frequencies (eigenvalues of (M, K) with damping from C).\n",
      "\n",
      "Spectral form (useful for computation and insight)\n",
      "- For first-order A diagonalizable: A = VΛV^{-1}, H(ω) = V (iωI − Λ)^{-1} V^{-1} b, so\n",
      "  gi(ω) = |∑k (vik wk)/(iω − λk)|, where wk are components of V^{-1}b.\n",
      "- Peaks occur where |iω − λk| is small and the mode k is excited (wk large) and observed at node i (vik large).\n",
      "\n",
      "Practical computation\n",
      "- Choose dynamics (A, or M, C, K) and the input profile b (uniform, single node, or application-specific).\n",
      "- Pick a frequency grid ω ∈ [ωmin, ωmax] covering relevant dynamics (near natural frequencies if known).\n",
      "- For each ω, solve a linear system:\n",
      "  - First-order: y(ω) = (iωI − A)^{-1} b.\n",
      "  - Second-order: y(ω) = (K − ω^2 M + iω C)^{-1} b.\n",
      "- Take gi(ω) = |y_i(ω)|. Define Ri = maxω gi(ω) (and optionally record the argmax for the resonant frequency).\n",
      "- Variants: use squared magnitude, L2 norm over frequencies ∫ gi(ω)^2 dω, or multiple input profiles and average.\n",
      "\n",
      "Connections to other centralities\n",
      "- Setting ω = 0 in first-order systems recovers static sensitivity measures linked to resistance distance and current-flow centralities.\n",
      "- Evaluating (sI − A)^{-1} at complex s relates resonance centrality to resolvent/Katz-type centralities; resonance centrality is the frequency-response version (s on the imaginary axis).\n",
      "\n",
      "In short: resonance centrality is the node-wise peak (or frequency-shaped) gain of the network’s transfer function under oscillatory driving, computed via inverses of (iωI − A) for first-order or of dynamic stiffness matrices for second-order dynamics.\n",
      "\n",
      "================================================================================\n",
      "RAG Answer:\n",
      "Resonance centrality is a dynamic-network centrality measure that captures how a node’s importance oscillates over time due to feedback loops. It emphasizes not just current connections but how past influence reverberates and amplifies across time.\n",
      "\n",
      "Calculation:\n",
      "For node i at time t:\n",
      "R_i(t) = β * Σ_j A_ij(t) * R_j(t−1) + (1 − β) * d_i(t)\n",
      "\n",
      "- A_ij(t): time-varying adjacency (connection from j to i at time t)\n",
      "- R_j(t−1): resonance centrality of node j at the previous time step\n",
      "- d_i(t): degree of node i at time t\n",
      "- β ∈ [0,1]: memory parameter controlling how much past centrality influences the present\n",
      "\n",
      "Interpretation:\n",
      "- Higher β gives more weight to past influence propagating through the network (feedback).\n",
      "- Lower β leans more on the node’s current degree.\n",
      "In social networks, nodes with high resonance centrality tend to be “trend catalysts” that trigger cascades which amplify their future influence.\n"
     ]
    }
   ],
   "source": [
    "# Add a \"research paper\" about a fictional concept\n",
    "fictional_doc = [\"\"\"\n",
    "Recent work by Spencer Lyon (2025) introduced the concept of \"resonance centrality\"\n",
    "for dynamic networks. Unlike traditional centrality measures which focus on static\n",
    "network structure, resonance centrality captures how a node's importance oscillates\n",
    "over time due to feedback loops.\n",
    "\n",
    "The resonance centrality R_i(t) for node i at time t is computed as:\n",
    "R_i(t) = β * Σ_j A_ij(t) * R_j(t-1) + (1-β) * d_i(t)\n",
    "\n",
    "where A_ij(t) is the time-varying adjacency matrix, d_i(t) is the degree at time t,\n",
    "and β ∈ [0,1] controls the memory of past centrality.\n",
    "\n",
    "Lyon showed that in social networks, nodes with high resonance centrality are often\n",
    "\"trend catalysts\" - they don't just spread information, but trigger cascades that\n",
    "feed back to amplify their influence in subsequent time periods.\n",
    "\"\"\"]\n",
    "\n",
    "add_documents!(network_store, fictional_doc)\n",
    "\n",
    "# Now ask about it\n",
    "fictional_q = \"What is resonance centrality and how is it calculated?\"\n",
    "\n",
    "println(\"Question: \", fictional_q)\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"Vanilla LLM Answer:\")\n",
    "println(vanilla_llm_query(fictional_q))\n",
    "println(\"\\n\" * \"=\"^80)\n",
    "println(\"RAG Answer:\")\n",
    "println(rag_query(network_store, fictional_q).answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "**What happened?**\n",
    "\n",
    "- The vanilla LLM either says it doesn't know, or **hallucinates** a plausible-sounding definition\n",
    "- The RAG system correctly retrieves and uses our fictional paper\n",
    "- This demonstrates how RAG enables working with proprietary, recent, or domain-specific knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## Trade-offs and Design Considerations\n",
    "\n",
    "Now that we've built a working RAG system, let's discuss key design decisions and trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "### Parametric vs Retrieval-Based Knowledge\n",
    "\n",
    "**Parametric Knowledge** (in model weights):\n",
    "- Pros:\n",
    "  - Fast: no need to search external sources\n",
    "  - Synthesizes information from vast training corpus\n",
    "  - Good for general knowledge and reasoning\n",
    "- Cons:\n",
    "  - Fixed at training time\n",
    "  - Can't update without retraining\n",
    "  - May hallucinate when uncertain\n",
    "  - No citations or provenance\n",
    "\n",
    "**Retrieval-Based Knowledge** (via RAG):\n",
    "- Pros:\n",
    "  - Can use up-to-date information\n",
    "  - Access to proprietary/specialized knowledge\n",
    "  - Provides citations and sources\n",
    "  - Reduces hallucination\n",
    "  - Updates without retraining\n",
    "- Cons:\n",
    "  - Slower (requires retrieval step)\n",
    "  - Quality depends on retrieval accuracy\n",
    "  - Requires maintaining vector database\n",
    "  - May miss information requiring synthesis across many sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "### Key Design Decisions\n",
    "\n",
    "**1. Chunk Size**\n",
    "- Smaller chunks (100-200 words):\n",
    "  - More precise retrieval\n",
    "  - Better for question answering\n",
    "  - May lose context across chunk boundaries\n",
    "- Larger chunks (500-1000 words):\n",
    "  - Preserve more context\n",
    "  - Better for complex reasoning\n",
    "  - May include irrelevant information\n",
    "\n",
    "**2. Number of Retrieved Documents (top_k)**\n",
    "- Few documents (k=1-3):\n",
    "  - Focuses on most relevant information\n",
    "  - Faster, uses less context window\n",
    "  - May miss important information\n",
    "- Many documents (k=5-10):\n",
    "  - More comprehensive context\n",
    "  - Better for complex queries\n",
    "  - May include noise, uses more tokens\n",
    "\n",
    "**3. Embedding Model**\n",
    "- Smaller models (text-embedding-3-small: 512-1536 dims):\n",
    "  - Faster embedding and search\n",
    "  - Lower cost\n",
    "  - Slightly lower quality\n",
    "- Larger models (text-embedding-3-large: 3072 dims):\n",
    "  - Better semantic understanding\n",
    "  - More accurate retrieval\n",
    "  - Higher cost and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "### Advanced RAG Techniques\n",
    "\n",
    "Our implementation is a basic RAG pipeline. Production systems often use:\n",
    "\n",
    "**1. Hybrid Search**\n",
    "- Combine vector similarity with keyword search (BM25)\n",
    "- Leverages both semantic and lexical matching\n",
    "- More robust to different query types\n",
    "\n",
    "**2. Re-ranking**\n",
    "- Use a separate model to re-rank retrieved results\n",
    "- Can consider query-document interaction more deeply\n",
    "- Improves precision of top results\n",
    "\n",
    "**3. Query Expansion**\n",
    "- Generate multiple variations of user query\n",
    "- Search with all variations\n",
    "- Helps with vocabulary mismatch between query and documents\n",
    "\n",
    "**4. Hierarchical Retrieval**\n",
    "- First retrieve at document level\n",
    "- Then retrieve specific passages within those documents\n",
    "- Balances context and precision\n",
    "\n",
    "**5. Iterative RAG**\n",
    "- Let LLM decide what to retrieve next\n",
    "- Multiple retrieval rounds\n",
    "- Better for complex, multi-hop reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "### Connection to Course Themes\n",
    "\n",
    "RAG systems connect to several course concepts:\n",
    "\n",
    "**Networks and Information Flow**\n",
    "- RAG creates edges from LLM to knowledge sources\n",
    "- Quality of answer depends on network structure (which documents are connected via similarity)\n",
    "- Similar to how social network structure affects information diffusion\n",
    "\n",
    "**Game Theory and Mechanism Design**\n",
    "- In multi-agent RAG systems, agents must decide what information to share\n",
    "- Strategic information retrieval: costly to retrieve everything, must be selective\n",
    "- Related to costly network formation from our game theory module\n",
    "\n",
    "**Agent-Based Modeling**\n",
    "- Can model RAG systems as agents with retrieval actions\n",
    "- Environment is the knowledge base\n",
    "- Rules govern when to retrieve, what to retrieve, how to synthesize\n",
    "- Next lecture will explore multi-agent AI systems in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "## Exercise 2: Build Your Own RAG System\n",
    "\n",
    "Choose one of the following domains and build a RAG system:\n",
    "\n",
    "**Option A: Game Theory Assistant**\n",
    "- Create documents summarizing key game theory concepts from our Week 8-9 lectures\n",
    "- Include: Nash equilibrium, dominant strategies, mixed strategies, auctions\n",
    "- Test with questions like \"What is a Nash equilibrium?\" or \"Explain the winner's curse\"\n",
    "\n",
    "**Option B: Julia Programming Helper**\n",
    "- Create documents with Julia programming tips and common patterns\n",
    "- Include: array operations, data frames, plotting, packages we've used\n",
    "- Test with questions like \"How do I create a histogram?\" or \"What's the syntax for filtering a DataFrame?\"\n",
    "\n",
    "**Option C: Your Domain**\n",
    "- Choose a topic you're interested in\n",
    "- Create 5-10 documents with information\n",
    "- Build and test a RAG system\n",
    "\n",
    "For your chosen option:\n",
    "1. Create at least 5 documents (can be shorter than our examples)\n",
    "2. Build a vector store\n",
    "3. Test with at least 3 different questions\n",
    "4. Compare RAG answers vs vanilla LLM answers\n",
    "5. Experiment with different values of `top_k` (number of retrieved documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n",
    "\n",
    "# Step 1: Create your documents\n",
    "my_docs = [\n",
    "    # Your documents here\n",
    "]\n",
    "\n",
    "# Step 2: Build vector store\n",
    "my_store = VectorStore()\n",
    "# add_documents!(my_store, my_docs)\n",
    "\n",
    "# Step 3: Test with questions\n",
    "# test_q1 = \"...\"\n",
    "# result = rag_query(my_store, test_q1)\n",
    "# println(result.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {},
   "source": [
    "## Exercise 3: Evaluating Retrieval Quality\n",
    "\n",
    "An important aspect of RAG systems is **retrieval quality** - are we finding the right documents?\n",
    "\n",
    "Using our network_store:\n",
    "\n",
    "1. Create 3 test queries where you know which document(s) should be retrieved\n",
    "2. For each query, perform retrieval with different values of k (k=1, 3, 5)\n",
    "3. Examine the similarity scores - what do they tell you?\n",
    "4. Try a query that requires information from multiple documents\n",
    "5. Try a query that's completely off-topic (e.g., about cooking) - what gets retrieved?\n",
    "\n",
    "Questions to explore:\n",
    "- Does the highest similarity document always have the best answer?\n",
    "- How much does similarity score drop from rank 1 to rank 3?\n",
    "- What happens when the query uses different terminology than the documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your analysis here\n",
    "\n",
    "# Example structure:\n",
    "# test_queries = [\n",
    "#     \"query 1...\",\n",
    "#     \"query 2...\",\n",
    "#     \"query 3...\"\n",
    "# ]\n",
    "\n",
    "# for query in test_queries\n",
    "#     println(\"Query: \", query)\n",
    "#     results = search(network_store, query; top_k=5)\n",
    "#     # Analyze results\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "## Exercise 4: Design Decisions\n",
    "\n",
    "Consider the following scenarios and discuss what RAG design choices you would make:\n",
    "\n",
    "**Scenario 1: Legal Document Q&A**\n",
    "- Database: 10,000 legal contracts and case documents\n",
    "- Queries: Lawyers asking about specific clauses and precedents\n",
    "- Requirements: High accuracy, must cite sources, complex reasoning\n",
    "\n",
    "**Scenario 2: Customer Support Chatbot**\n",
    "- Database: 500 FAQ articles and product documentation\n",
    "- Queries: Customers asking how to use products, troubleshoot issues\n",
    "- Requirements: Fast responses, conversational, handles unclear questions\n",
    "\n",
    "**Scenario 3: Academic Research Assistant**\n",
    "- Database: 100,000 research papers\n",
    "- Queries: Researchers asking about literature, methodologies, findings\n",
    "- Requirements: Comprehensive answers, identifies connections between papers\n",
    "\n",
    "For each scenario, decide:\n",
    "1. Chunk size (small, medium, large)?\n",
    "2. Number of documents to retrieve (k=?)?\n",
    "3. Would you use any advanced techniques (hybrid search, re-ranking, etc.)?\n",
    "4. How would you handle queries that require information from multiple documents?\n",
    "5. How would you evaluate the system's performance?\n",
    "\n",
    "Write your analysis below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "**Your Analysis:**\n",
    "\n",
    "Scenario 1: Legal Document Q&A\n",
    "- Chunk size: ...\n",
    "- k: ...\n",
    "- Advanced techniques: ...\n",
    "- Multi-document handling: ...\n",
    "- Evaluation: ...\n",
    "\n",
    "*(Continue for other scenarios)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "## Looking Ahead: Multi-Agent Systems\n",
    "\n",
    "- In this lecture we built a single-agent RAG system\n",
    "- The LLM agent retrieves information and generates answers\n",
    "- But what if we had multiple AI agents working together?\n",
    "- Next lecture (A1.03) we'll explore **multi-agent AI systems**:\n",
    "  - Multiple LLM agents with specialized roles\n",
    "  - Agents that coordinate and communicate\n",
    "  - Connections to agent-based models and game theory\n",
    "  - Building complex workflows with agentic systems\n",
    "\n",
    "Think about:\n",
    "- How might multiple agents improve retrieval? (e.g., one agent generates queries, another evaluates relevance)\n",
    "- What if different agents had access to different knowledge bases?\n",
    "- How does this relate to our study of networks and strategic interaction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-53",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLMs have fundamental knowledge limitations**: cutoffs, hallucination, missing domain-specific info\n",
    "\n",
    "2. **RAG augments LLMs with external knowledge** through retrieval from vector databases\n",
    "\n",
    "3. **Embeddings represent text as vectors** capturing semantic meaning, enabling similarity search\n",
    "\n",
    "4. **A RAG pipeline has four stages**: chunking, embedding, retrieval, generation\n",
    "\n",
    "5. **Design trade-offs matter**: chunk size, number of retrieved docs, embedding model all affect quality and performance\n",
    "\n",
    "6. **RAG enables practical AI applications** with up-to-date, domain-specific, and proprietary knowledge\n",
    "\n",
    "7. **RAG connects to network concepts**: creating dynamic information flow from LLM to knowledge sources\n",
    "\n",
    "8. **Citations and provenance** are key benefits of RAG over pure parametric knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "**Foundational Papers:**\n",
    "- Lewis et al. (2020) \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"\n",
    "- Gao et al. (2023) \"Retrieval-Augmented Generation for Large Language Models: A Survey\"\n",
    "\n",
    "**Advanced RAG:**\n",
    "- Khattab et al. (2023) \"DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines\" [arXiv:2310.03714](https://arxiv.org/abs/2310.03714)\n",
    "- Shi et al. (2023) \"REPLUG: Retrieval-Augmented Black-Box Language Models\" [arXiv:2301.12652](https://arxiv.org/abs/2301.12652)\n",
    "\n",
    "**Vector Databases:**\n",
    "- Pinecone Documentation: [https://docs.pinecone.io/](https://docs.pinecone.io/)\n",
    "- Weaviate Concepts: [https://weaviate.io/developers/weaviate/concepts](https://weaviate.io/developers/weaviate/concepts)\n",
    "\n",
    "**Embeddings:**\n",
    "- OpenAI Embeddings Guide: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "- Neelakantan et al. (2022) \"Text and Code Embeddings by Contrastive Pre-Training\" [arXiv:2201.10005](https://arxiv.org/abs/2201.10005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
